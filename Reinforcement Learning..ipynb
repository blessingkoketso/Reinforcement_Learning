{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Cartpole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the cartpole under the environment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the environment and extract the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1 : Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "even the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "    24/100000: episode: 1, duration: 0.235s, episode steps: 24, steps per second: 102, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.102 [-1.878, 0.796], mean_best_reward: --\n",
      "    36/100000: episode: 2, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.152 [-1.135, 2.120], mean_best_reward: --\n",
      "    75/100000: episode: 3, duration: 0.248s, episode steps: 39, steps per second: 157, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.037 [-1.476, 1.596], mean_best_reward: --\n",
      "   107/100000: episode: 4, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.608, 1.249], mean_best_reward: --\n",
      "   152/100000: episode: 5, duration: 0.236s, episode steps: 45, steps per second: 190, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.378 [0.000, 1.000], mean observation: -0.071 [-2.263, 2.600], mean_best_reward: --\n",
      "   162/100000: episode: 6, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.106 [-1.934, 1.185], mean_best_reward: --\n",
      "   177/100000: episode: 7, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.946, 1.797], mean_best_reward: --\n",
      "   189/100000: episode: 8, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.106 [-0.795, 1.411], mean_best_reward: --\n",
      "   204/100000: episode: 9, duration: 0.111s, episode steps: 15, steps per second: 135, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.116 [-1.135, 2.010], mean_best_reward: --\n",
      "   214/100000: episode: 10, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.996, 1.937], mean_best_reward: --\n",
      "   223/100000: episode: 11, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.787, 2.754], mean_best_reward: --\n",
      "   239/100000: episode: 12, duration: 0.076s, episode steps: 16, steps per second: 210, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.074 [-2.014, 1.221], mean_best_reward: --\n",
      "   256/100000: episode: 13, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.090 [-1.718, 2.743], mean_best_reward: --\n",
      "   284/100000: episode: 14, duration: 0.132s, episode steps: 28, steps per second: 213, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.092 [-0.570, 0.915], mean_best_reward: --\n",
      "   313/100000: episode: 15, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.058 [-1.988, 1.174], mean_best_reward: --\n",
      "   324/100000: episode: 16, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.367, 2.319], mean_best_reward: --\n",
      "   334/100000: episode: 17, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.243, 1.389], mean_best_reward: --\n",
      "   347/100000: episode: 18, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.098 [-2.308, 1.365], mean_best_reward: --\n",
      "   404/100000: episode: 19, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.060 [-1.336, 0.891], mean_best_reward: --\n",
      "   420/100000: episode: 20, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.090 [-1.175, 2.018], mean_best_reward: --\n",
      "   442/100000: episode: 21, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.016 [-1.842, 1.196], mean_best_reward: --\n",
      "   469/100000: episode: 22, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.191, 0.574], mean_best_reward: --\n",
      "   483/100000: episode: 23, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.066 [-1.775, 1.209], mean_best_reward: --\n",
      "   497/100000: episode: 24, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.108 [-0.971, 1.697], mean_best_reward: --\n",
      "   521/100000: episode: 25, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.612, 1.145], mean_best_reward: --\n",
      "   536/100000: episode: 26, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.075 [-1.804, 1.189], mean_best_reward: --\n",
      "   551/100000: episode: 27, duration: 0.106s, episode steps: 15, steps per second: 141, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.064 [-1.572, 2.358], mean_best_reward: --\n",
      "   585/100000: episode: 28, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-1.111, 1.250], mean_best_reward: --\n",
      "   595/100000: episode: 29, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.910, 3.113], mean_best_reward: --\n",
      "   606/100000: episode: 30, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.133, 1.914], mean_best_reward: --\n",
      "   619/100000: episode: 31, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.091 [-1.403, 2.214], mean_best_reward: --\n",
      "   633/100000: episode: 32, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.085 [-1.228, 2.103], mean_best_reward: --\n",
      "   663/100000: episode: 33, duration: 0.180s, episode steps: 30, steps per second: 166, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.059 [-1.353, 2.291], mean_best_reward: --\n",
      "   687/100000: episode: 34, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.067 [-1.127, 1.946], mean_best_reward: --\n",
      "   702/100000: episode: 35, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.082 [-2.275, 1.536], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   722/100000: episode: 36, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.088 [-2.088, 1.162], mean_best_reward: --\n",
      "   759/100000: episode: 37, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: 0.033 [-1.544, 2.338], mean_best_reward: --\n",
      "   771/100000: episode: 38, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.121 [-3.049, 1.974], mean_best_reward: --\n",
      "   785/100000: episode: 39, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.102 [-1.734, 0.983], mean_best_reward: --\n",
      "   835/100000: episode: 40, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.188 [-1.159, 1.495], mean_best_reward: --\n",
      "   861/100000: episode: 41, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: 0.032 [-2.552, 1.937], mean_best_reward: --\n",
      "   875/100000: episode: 42, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.117 [-2.574, 1.521], mean_best_reward: --\n",
      "   915/100000: episode: 43, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.981, 1.074], mean_best_reward: --\n",
      "   930/100000: episode: 44, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.084 [-1.926, 1.217], mean_best_reward: --\n",
      "   945/100000: episode: 45, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.115 [-1.337, 2.376], mean_best_reward: --\n",
      "   981/100000: episode: 46, duration: 0.338s, episode steps: 36, steps per second: 107, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.010 [-2.111, 1.405], mean_best_reward: --\n",
      "  1022/100000: episode: 47, duration: 0.259s, episode steps: 41, steps per second: 158, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.020 [-1.615, 0.839], mean_best_reward: --\n",
      "  1032/100000: episode: 48, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.601, 2.549], mean_best_reward: --\n",
      "  1053/100000: episode: 49, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.074 [-0.802, 1.509], mean_best_reward: --\n",
      "  1063/100000: episode: 50, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.148 [-2.034, 1.159], mean_best_reward: --\n",
      "  1073/100000: episode: 51, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.533, 2.527], mean_best_reward: --\n",
      "  1101/100000: episode: 52, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.084 [-0.813, 1.442], mean_best_reward: --\n",
      "  1119/100000: episode: 53, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.101 [-1.360, 0.565], mean_best_reward: --\n",
      "  1134/100000: episode: 54, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.813, 0.984], mean_best_reward: --\n",
      "  1145/100000: episode: 55, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.104 [-0.962, 1.601], mean_best_reward: --\n",
      "  1158/100000: episode: 56, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.121 [-1.801, 0.960], mean_best_reward: --\n",
      "  1169/100000: episode: 57, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.115 [-1.775, 2.806], mean_best_reward: --\n",
      "  1183/100000: episode: 58, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.103 [-1.540, 2.577], mean_best_reward: --\n",
      "  1230/100000: episode: 59, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.007 [-1.152, 0.990], mean_best_reward: --\n",
      "  1252/100000: episode: 60, duration: 0.102s, episode steps: 22, steps per second: 216, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.076 [-2.124, 1.144], mean_best_reward: --\n",
      "  1292/100000: episode: 61, duration: 0.192s, episode steps: 40, steps per second: 208, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.019 [-0.787, 1.307], mean_best_reward: --\n",
      "  1356/100000: episode: 62, duration: 0.361s, episode steps: 64, steps per second: 177, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.011 [-0.962, 1.784], mean_best_reward: --\n",
      "  1367/100000: episode: 63, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.139 [-1.911, 2.935], mean_best_reward: --\n",
      "  1405/100000: episode: 64, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.033 [-1.306, 1.653], mean_best_reward: --\n",
      "  1429/100000: episode: 65, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.087 [-1.445, 0.778], mean_best_reward: --\n",
      "  1462/100000: episode: 66, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.092 [-0.568, 1.091], mean_best_reward: --\n",
      "  1472/100000: episode: 67, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.011, 1.176], mean_best_reward: --\n",
      "  1485/100000: episode: 68, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.111 [-1.158, 1.994], mean_best_reward: --\n",
      "  1506/100000: episode: 69, duration: 0.098s, episode steps: 21, steps per second: 215, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.086 [-2.408, 1.392], mean_best_reward: --\n",
      "  1539/100000: episode: 70, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.031 [-1.466, 0.808], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1552/100000: episode: 71, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.098 [-2.379, 1.517], mean_best_reward: --\n",
      "  1564/100000: episode: 72, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.957, 1.779], mean_best_reward: --\n",
      "  1576/100000: episode: 73, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.101 [-1.612, 2.570], mean_best_reward: --\n",
      "  1597/100000: episode: 74, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.030 [-1.829, 1.209], mean_best_reward: --\n",
      "  1636/100000: episode: 75, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.410 [0.000, 1.000], mean observation: -0.001 [-1.341, 2.008], mean_best_reward: --\n",
      "  1651/100000: episode: 76, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.118 [-0.940, 1.660], mean_best_reward: --\n",
      "  1681/100000: episode: 77, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.021 [-2.100, 2.732], mean_best_reward: --\n",
      "  1692/100000: episode: 78, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.118 [-2.436, 1.606], mean_best_reward: --\n",
      "  1705/100000: episode: 79, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.087 [-1.890, 1.211], mean_best_reward: --\n",
      "  1720/100000: episode: 80, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-1.686, 1.027], mean_best_reward: --\n",
      "  1735/100000: episode: 81, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.114 [-0.783, 1.553], mean_best_reward: --\n",
      "  1749/100000: episode: 82, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.329, 2.187], mean_best_reward: --\n",
      "  1766/100000: episode: 83, duration: 0.084s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.093 [-1.840, 0.968], mean_best_reward: --\n",
      "  1776/100000: episode: 84, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.543, 1.611], mean_best_reward: --\n",
      "  1793/100000: episode: 85, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.064 [-1.875, 1.165], mean_best_reward: --\n",
      "  1804/100000: episode: 86, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-0.975, 1.689], mean_best_reward: --\n",
      "  1820/100000: episode: 87, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.175, 0.644], mean_best_reward: --\n",
      "  1850/100000: episode: 88, duration: 0.135s, episode steps: 30, steps per second: 222, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.067 [-2.337, 2.650], mean_best_reward: --\n",
      "  1872/100000: episode: 89, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.067 [-2.069, 1.158], mean_best_reward: --\n",
      "  1883/100000: episode: 90, duration: 0.053s, episode steps: 11, steps per second: 208, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.122 [-1.982, 1.219], mean_best_reward: --\n",
      "  1895/100000: episode: 91, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.112 [-1.316, 2.007], mean_best_reward: --\n",
      "  1907/100000: episode: 92, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.109 [-2.067, 1.160], mean_best_reward: --\n",
      "  1919/100000: episode: 93, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.099 [-1.760, 2.676], mean_best_reward: --\n",
      "  1933/100000: episode: 94, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.102 [-2.568, 1.547], mean_best_reward: --\n",
      "  1953/100000: episode: 95, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.757, 1.519], mean_best_reward: --\n",
      "  1973/100000: episode: 96, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.087 [-1.216, 0.633], mean_best_reward: --\n",
      "  1992/100000: episode: 97, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.060 [-0.796, 1.211], mean_best_reward: --\n",
      "  2021/100000: episode: 98, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.006 [-2.079, 1.395], mean_best_reward: --\n",
      "  2034/100000: episode: 99, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.108 [-0.999, 1.819], mean_best_reward: --\n",
      "  2045/100000: episode: 100, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.143 [-1.795, 0.946], mean_best_reward: --\n",
      "  2061/100000: episode: 101, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.104 [-1.282, 0.792], mean_best_reward: 60.500000\n",
      "  2166/100000: episode: 102, duration: 0.523s, episode steps: 105, steps per second: 201, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.216 [-1.343, 0.745], mean_best_reward: --\n",
      "  2188/100000: episode: 103, duration: 0.101s, episode steps: 22, steps per second: 218, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.057 [-1.165, 1.819], mean_best_reward: --\n",
      "  2239/100000: episode: 104, duration: 0.248s, episode steps: 51, steps per second: 206, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.134 [-1.413, 0.975], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2298/100000: episode: 105, duration: 0.278s, episode steps: 59, steps per second: 212, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.091 [-0.682, 1.049], mean_best_reward: --\n",
      "  2335/100000: episode: 106, duration: 0.167s, episode steps: 37, steps per second: 222, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.118 [-0.899, 0.549], mean_best_reward: --\n",
      "  2374/100000: episode: 107, duration: 0.181s, episode steps: 39, steps per second: 216, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.006 [-1.354, 0.986], mean_best_reward: --\n",
      "  2394/100000: episode: 108, duration: 0.092s, episode steps: 20, steps per second: 217, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.929, 0.632], mean_best_reward: --\n",
      "  2406/100000: episode: 109, duration: 0.057s, episode steps: 12, steps per second: 209, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-0.762, 1.595], mean_best_reward: --\n",
      "  2471/100000: episode: 110, duration: 0.314s, episode steps: 65, steps per second: 207, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.179 [-1.291, 0.695], mean_best_reward: --\n",
      "  2526/100000: episode: 111, duration: 0.271s, episode steps: 55, steps per second: 203, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: 0.044 [-0.814, 1.627], mean_best_reward: --\n",
      "  2570/100000: episode: 112, duration: 0.200s, episode steps: 44, steps per second: 220, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.944, 1.177], mean_best_reward: --\n",
      "  2593/100000: episode: 113, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.043 [-1.795, 1.014], mean_best_reward: --\n",
      "  2612/100000: episode: 114, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.082 [-1.447, 0.942], mean_best_reward: --\n",
      "  2629/100000: episode: 115, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.074 [-0.821, 1.288], mean_best_reward: --\n",
      "  2656/100000: episode: 116, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.018 [-1.270, 0.938], mean_best_reward: --\n",
      "  2695/100000: episode: 117, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.180 [-1.106, 0.495], mean_best_reward: --\n",
      "  2743/100000: episode: 118, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.025 [-0.810, 1.270], mean_best_reward: --\n",
      "  2765/100000: episode: 119, duration: 0.102s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.051 [-0.590, 1.184], mean_best_reward: --\n",
      "  2804/100000: episode: 120, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.054 [-0.764, 1.780], mean_best_reward: --\n",
      "  2819/100000: episode: 121, duration: 0.071s, episode steps: 15, steps per second: 213, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.058 [-1.208, 1.766], mean_best_reward: --\n",
      "  2835/100000: episode: 122, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.100 [-1.196, 2.077], mean_best_reward: --\n",
      "  2906/100000: episode: 123, duration: 0.322s, episode steps: 71, steps per second: 220, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.047 [-1.179, 1.189], mean_best_reward: --\n",
      "  2923/100000: episode: 124, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.107 [-0.604, 1.363], mean_best_reward: --\n",
      "  2968/100000: episode: 125, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.067 [-1.089, 0.772], mean_best_reward: --\n",
      "  2995/100000: episode: 126, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.094 [-1.191, 0.443], mean_best_reward: --\n",
      "  3015/100000: episode: 127, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.301, 0.819], mean_best_reward: --\n",
      "  3032/100000: episode: 128, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.114 [-0.561, 1.406], mean_best_reward: --\n",
      "  3067/100000: episode: 129, duration: 0.169s, episode steps: 35, steps per second: 208, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.115 [-0.409, 1.096], mean_best_reward: --\n",
      "  3089/100000: episode: 130, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.431, 1.027], mean_best_reward: --\n",
      "  3115/100000: episode: 131, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.096, 0.567], mean_best_reward: --\n",
      "  3193/100000: episode: 132, duration: 0.349s, episode steps: 78, steps per second: 223, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.396, 0.816], mean_best_reward: --\n",
      "  3218/100000: episode: 133, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.107 [-0.558, 0.967], mean_best_reward: --\n",
      "  3245/100000: episode: 134, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.037 [-1.151, 1.850], mean_best_reward: --\n",
      "  3273/100000: episode: 135, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.062 [-1.730, 0.970], mean_best_reward: --\n",
      "  3350/100000: episode: 136, duration: 0.392s, episode steps: 77, steps per second: 196, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.041 [-1.195, 1.382], mean_best_reward: --\n",
      "  3387/100000: episode: 137, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.044 [-0.774, 1.405], mean_best_reward: --\n",
      "  3467/100000: episode: 138, duration: 0.380s, episode steps: 80, steps per second: 210, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.036 [-1.042, 1.119], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3533/100000: episode: 139, duration: 0.327s, episode steps: 66, steps per second: 202, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.119 [-1.145, 0.627], mean_best_reward: --\n",
      "  3556/100000: episode: 140, duration: 0.104s, episode steps: 23, steps per second: 221, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.050 [-0.776, 1.146], mean_best_reward: --\n",
      "  3575/100000: episode: 141, duration: 0.091s, episode steps: 19, steps per second: 208, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.086 [-1.298, 0.765], mean_best_reward: --\n",
      "  3595/100000: episode: 142, duration: 0.094s, episode steps: 20, steps per second: 212, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.471, 0.780], mean_best_reward: --\n",
      "  3617/100000: episode: 143, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.144 [-0.373, 1.068], mean_best_reward: --\n",
      "  3642/100000: episode: 144, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.064 [-0.614, 1.053], mean_best_reward: --\n",
      "  3669/100000: episode: 145, duration: 0.123s, episode steps: 27, steps per second: 220, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.024 [-1.590, 1.002], mean_best_reward: --\n",
      "  3691/100000: episode: 146, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.134 [-1.003, 0.568], mean_best_reward: --\n",
      "  3727/100000: episode: 147, duration: 0.171s, episode steps: 36, steps per second: 210, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.051 [-0.651, 1.576], mean_best_reward: --\n",
      "  3758/100000: episode: 148, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.025 [-2.156, 1.373], mean_best_reward: --\n",
      "  3829/100000: episode: 149, duration: 0.380s, episode steps: 71, steps per second: 187, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.079 [-1.158, 2.135], mean_best_reward: --\n",
      "  3848/100000: episode: 150, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.064 [-0.805, 1.492], mean_best_reward: --\n",
      "  3862/100000: episode: 151, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.096 [-0.787, 1.360], mean_best_reward: 77.500000\n",
      "  3924/100000: episode: 152, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.129 [-1.677, 0.822], mean_best_reward: --\n",
      "  3998/100000: episode: 153, duration: 0.406s, episode steps: 74, steps per second: 182, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.144 [-1.090, 0.703], mean_best_reward: --\n",
      "  4056/100000: episode: 154, duration: 0.309s, episode steps: 58, steps per second: 187, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.181 [-0.805, 1.183], mean_best_reward: --\n",
      "  4103/100000: episode: 155, duration: 0.287s, episode steps: 47, steps per second: 164, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.097 [-0.768, 1.455], mean_best_reward: --\n",
      "  4138/100000: episode: 156, duration: 0.191s, episode steps: 35, steps per second: 184, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.089 [-1.596, 0.774], mean_best_reward: --\n",
      "  4160/100000: episode: 157, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.431, 1.051], mean_best_reward: --\n",
      "  4220/100000: episode: 158, duration: 0.314s, episode steps: 60, steps per second: 191, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.085 [-0.589, 0.979], mean_best_reward: --\n",
      "  4233/100000: episode: 159, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.109 [-0.758, 1.344], mean_best_reward: --\n",
      "  4275/100000: episode: 160, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.143 [-1.154, 0.648], mean_best_reward: --\n",
      "  4325/100000: episode: 161, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.155 [-0.526, 1.253], mean_best_reward: --\n",
      "  4357/100000: episode: 162, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.050 [-1.060, 0.798], mean_best_reward: --\n",
      "  4390/100000: episode: 163, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.056 [-0.824, 1.272], mean_best_reward: --\n",
      "  4402/100000: episode: 164, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.125 [-0.761, 1.279], mean_best_reward: --\n",
      "  4422/100000: episode: 165, duration: 0.091s, episode steps: 20, steps per second: 221, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.068 [-0.806, 1.312], mean_best_reward: --\n",
      "  4487/100000: episode: 166, duration: 0.535s, episode steps: 65, steps per second: 122, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.096 [-0.964, 0.885], mean_best_reward: --\n",
      "  4536/100000: episode: 167, duration: 0.339s, episode steps: 49, steps per second: 144, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.031 [-1.145, 1.456], mean_best_reward: --\n",
      "  4561/100000: episode: 168, duration: 0.155s, episode steps: 25, steps per second: 161, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.064 [-1.151, 0.639], mean_best_reward: --\n",
      "  4584/100000: episode: 169, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.051 [-0.786, 1.180], mean_best_reward: --\n",
      "  4611/100000: episode: 170, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.096 [-1.736, 0.643], mean_best_reward: --\n",
      "  4642/100000: episode: 171, duration: 0.196s, episode steps: 31, steps per second: 158, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.060 [-0.627, 1.086], mean_best_reward: --\n",
      "  4677/100000: episode: 172, duration: 0.187s, episode steps: 35, steps per second: 188, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.102 [-0.450, 0.957], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4732/100000: episode: 173, duration: 0.275s, episode steps: 55, steps per second: 200, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.206 [-1.258, 0.661], mean_best_reward: --\n",
      "  4768/100000: episode: 174, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.446, 1.442], mean_best_reward: --\n",
      "  4789/100000: episode: 175, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.101 [-0.578, 1.304], mean_best_reward: --\n",
      "  4852/100000: episode: 176, duration: 0.310s, episode steps: 63, steps per second: 204, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.190 [-1.035, 0.455], mean_best_reward: --\n",
      "  4932/100000: episode: 177, duration: 0.399s, episode steps: 80, steps per second: 200, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.595, 1.181], mean_best_reward: --\n",
      "  4944/100000: episode: 178, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-1.001, 1.758], mean_best_reward: --\n",
      "  4958/100000: episode: 179, duration: 0.095s, episode steps: 14, steps per second: 147, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.105 [-0.597, 1.064], mean_best_reward: --\n",
      "  5037/100000: episode: 180, duration: 0.431s, episode steps: 79, steps per second: 183, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.087 [-1.120, 0.938], mean_best_reward: --\n",
      "  5077/100000: episode: 181, duration: 0.181s, episode steps: 40, steps per second: 220, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.076 [-0.876, 0.424], mean_best_reward: --\n",
      "  5101/100000: episode: 182, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.064 [-0.643, 1.393], mean_best_reward: --\n",
      "  5173/100000: episode: 183, duration: 0.345s, episode steps: 72, steps per second: 209, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.089 [-0.787, 0.928], mean_best_reward: --\n",
      "  5219/100000: episode: 184, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.005 [-1.038, 1.737], mean_best_reward: --\n",
      "  5259/100000: episode: 185, duration: 0.270s, episode steps: 40, steps per second: 148, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.106 [-0.627, 1.606], mean_best_reward: --\n",
      "  5314/100000: episode: 186, duration: 0.335s, episode steps: 55, steps per second: 164, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.135 [-0.764, 1.194], mean_best_reward: --\n",
      "  5324/100000: episode: 187, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.138 [-1.580, 0.987], mean_best_reward: --\n",
      "  5393/100000: episode: 188, duration: 0.338s, episode steps: 69, steps per second: 204, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.079 [-0.797, 1.336], mean_best_reward: --\n",
      "  5428/100000: episode: 189, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.002 [-1.142, 0.761], mean_best_reward: --\n",
      "  5535/100000: episode: 190, duration: 0.524s, episode steps: 107, steps per second: 204, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.084 [-0.626, 1.142], mean_best_reward: --\n",
      "  5574/100000: episode: 191, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.090 [-0.783, 1.260], mean_best_reward: --\n",
      "  5592/100000: episode: 192, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.231, 0.793], mean_best_reward: --\n",
      "  5613/100000: episode: 193, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.117 [-0.929, 0.570], mean_best_reward: --\n",
      "  5630/100000: episode: 194, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.074 [-0.628, 1.307], mean_best_reward: --\n",
      "  5678/100000: episode: 195, duration: 0.275s, episode steps: 48, steps per second: 174, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.737, 1.342], mean_best_reward: --\n",
      "  5704/100000: episode: 196, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.129 [-0.352, 0.986], mean_best_reward: --\n",
      "  5795/100000: episode: 197, duration: 0.480s, episode steps: 91, steps per second: 190, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.008 [-0.992, 1.327], mean_best_reward: --\n",
      "  5863/100000: episode: 198, duration: 0.353s, episode steps: 68, steps per second: 193, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.106 [-1.113, 0.750], mean_best_reward: --\n",
      "  5877/100000: episode: 199, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.124 [-0.772, 1.606], mean_best_reward: --\n",
      "  5918/100000: episode: 200, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.065 [-0.616, 0.900], mean_best_reward: --\n",
      "  5957/100000: episode: 201, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.144 [-0.561, 1.034], mean_best_reward: 106.000000\n",
      "  6021/100000: episode: 202, duration: 0.389s, episode steps: 64, steps per second: 165, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-0.638, 0.925], mean_best_reward: --\n",
      "  6056/100000: episode: 203, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.139 [-0.399, 0.916], mean_best_reward: --\n",
      "  6100/100000: episode: 204, duration: 0.244s, episode steps: 44, steps per second: 180, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.064 [-1.603, 0.802], mean_best_reward: --\n",
      "  6115/100000: episode: 205, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.813, 1.428], mean_best_reward: --\n",
      "  6151/100000: episode: 206, duration: 0.226s, episode steps: 36, steps per second: 159, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.088 [-0.757, 1.722], mean_best_reward: --\n",
      "  6168/100000: episode: 207, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.803, 1.222], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6185/100000: episode: 208, duration: 0.111s, episode steps: 17, steps per second: 153, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.075 [-0.981, 1.458], mean_best_reward: --\n",
      "  6246/100000: episode: 209, duration: 0.354s, episode steps: 61, steps per second: 172, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.083 [-0.636, 0.977], mean_best_reward: --\n",
      "  6279/100000: episode: 210, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.086 [-1.222, 0.732], mean_best_reward: --\n",
      "  6322/100000: episode: 211, duration: 0.276s, episode steps: 43, steps per second: 156, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.083 [-0.621, 0.862], mean_best_reward: --\n",
      "  6363/100000: episode: 212, duration: 0.186s, episode steps: 41, steps per second: 220, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.017 [-0.890, 0.727], mean_best_reward: --\n",
      "  6376/100000: episode: 213, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.090 [-1.672, 1.017], mean_best_reward: --\n",
      "  6423/100000: episode: 214, duration: 0.309s, episode steps: 47, steps per second: 152, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.020 [-1.147, 1.555], mean_best_reward: --\n",
      "  6453/100000: episode: 215, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.060 [-0.597, 1.281], mean_best_reward: --\n",
      "  6495/100000: episode: 216, duration: 0.197s, episode steps: 42, steps per second: 213, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.156 [-0.783, 1.255], mean_best_reward: --\n",
      "  6522/100000: episode: 217, duration: 0.125s, episode steps: 27, steps per second: 215, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.084 [-0.414, 0.921], mean_best_reward: --\n",
      "  6553/100000: episode: 218, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.096 [-1.215, 0.544], mean_best_reward: --\n",
      "  6565/100000: episode: 219, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.112 [-1.301, 0.801], mean_best_reward: --\n",
      "  6604/100000: episode: 220, duration: 0.188s, episode steps: 39, steps per second: 208, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.152 [-0.774, 2.175], mean_best_reward: --\n",
      "  6635/100000: episode: 221, duration: 0.144s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.107 [-0.627, 1.349], mean_best_reward: --\n",
      "  6671/100000: episode: 222, duration: 0.164s, episode steps: 36, steps per second: 220, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.120 [-0.952, 0.575], mean_best_reward: --\n",
      "  6685/100000: episode: 223, duration: 0.066s, episode steps: 14, steps per second: 214, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.103 [-1.132, 1.839], mean_best_reward: --\n",
      "  6699/100000: episode: 224, duration: 0.065s, episode steps: 14, steps per second: 214, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.084 [-0.809, 1.320], mean_best_reward: --\n",
      "  6737/100000: episode: 225, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.081 [-0.537, 1.081], mean_best_reward: --\n",
      "  6756/100000: episode: 226, duration: 0.131s, episode steps: 19, steps per second: 145, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.122 [-0.589, 1.025], mean_best_reward: --\n",
      "  6851/100000: episode: 227, duration: 0.503s, episode steps: 95, steps per second: 189, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.080 [-1.086, 0.745], mean_best_reward: --\n",
      "  6864/100000: episode: 228, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-1.188, 1.819], mean_best_reward: --\n",
      "  6908/100000: episode: 229, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.052 [-0.757, 1.097], mean_best_reward: --\n",
      "  6943/100000: episode: 230, duration: 0.219s, episode steps: 35, steps per second: 160, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.002 [-1.020, 1.591], mean_best_reward: --\n",
      "  6966/100000: episode: 231, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.129 [-0.792, 0.348], mean_best_reward: --\n",
      "  6991/100000: episode: 232, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.131 [-0.374, 1.033], mean_best_reward: --\n",
      "  7021/100000: episode: 233, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.819, 1.074], mean_best_reward: --\n",
      "  7041/100000: episode: 234, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.596, 1.028], mean_best_reward: --\n",
      "  7074/100000: episode: 235, duration: 0.228s, episode steps: 33, steps per second: 145, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.094 [-0.986, 2.135], mean_best_reward: --\n",
      "  7093/100000: episode: 236, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.058 [-1.030, 1.604], mean_best_reward: --\n",
      "  7108/100000: episode: 237, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.720, 1.012], mean_best_reward: --\n",
      "  7133/100000: episode: 238, duration: 0.175s, episode steps: 25, steps per second: 143, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.118 [-0.631, 0.943], mean_best_reward: --\n",
      "  7165/100000: episode: 239, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-0.616, 0.909], mean_best_reward: --\n",
      "  7216/100000: episode: 240, duration: 0.302s, episode steps: 51, steps per second: 169, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.052 [-0.959, 1.485], mean_best_reward: --\n",
      "  7249/100000: episode: 241, duration: 0.194s, episode steps: 33, steps per second: 171, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.082 [-1.260, 0.639], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7281/100000: episode: 242, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.826, 1.113], mean_best_reward: --\n",
      "  7307/100000: episode: 243, duration: 0.180s, episode steps: 26, steps per second: 145, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.032 [-1.751, 1.029], mean_best_reward: --\n",
      "  7344/100000: episode: 244, duration: 0.336s, episode steps: 37, steps per second: 110, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.002 [-1.166, 1.226], mean_best_reward: --\n",
      "  7376/100000: episode: 245, duration: 0.202s, episode steps: 32, steps per second: 159, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: 0.064 [-1.244, 2.287], mean_best_reward: --\n",
      "  7414/100000: episode: 246, duration: 0.247s, episode steps: 38, steps per second: 154, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.062 [-1.032, 1.861], mean_best_reward: --\n",
      "  7456/100000: episode: 247, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.109 [-0.773, 0.562], mean_best_reward: --\n",
      "  7485/100000: episode: 248, duration: 0.258s, episode steps: 29, steps per second: 112, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.096 [-0.633, 1.025], mean_best_reward: --\n",
      "  7506/100000: episode: 249, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.126 [-1.565, 0.576], mean_best_reward: --\n",
      "  7549/100000: episode: 250, duration: 0.211s, episode steps: 43, steps per second: 204, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.040 [-0.570, 1.081], mean_best_reward: --\n",
      "  7577/100000: episode: 251, duration: 0.133s, episode steps: 28, steps per second: 210, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.117 [-0.541, 1.444], mean_best_reward: 71.000000\n",
      "  7688/100000: episode: 252, duration: 0.609s, episode steps: 111, steps per second: 182, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.053 [-0.972, 1.254], mean_best_reward: --\n",
      "  7733/100000: episode: 253, duration: 0.286s, episode steps: 45, steps per second: 157, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.182 [-0.348, 0.964], mean_best_reward: --\n",
      "  7786/100000: episode: 254, duration: 0.275s, episode steps: 53, steps per second: 193, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: 0.115 [-1.206, 2.475], mean_best_reward: --\n",
      "  7807/100000: episode: 255, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.080 [-0.562, 1.100], mean_best_reward: --\n",
      "  7835/100000: episode: 256, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.051 [-1.726, 1.018], mean_best_reward: --\n",
      "  7866/100000: episode: 257, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.084 [-0.756, 1.462], mean_best_reward: --\n",
      "  7894/100000: episode: 258, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.096 [-0.570, 1.393], mean_best_reward: --\n",
      "  7991/100000: episode: 259, duration: 0.469s, episode steps: 97, steps per second: 207, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.132 [-1.337, 1.066], mean_best_reward: --\n",
      "  8016/100000: episode: 260, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.092 [-1.990, 0.981], mean_best_reward: --\n",
      "  8033/100000: episode: 261, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.095 [-0.615, 1.232], mean_best_reward: --\n",
      "  8065/100000: episode: 262, duration: 0.145s, episode steps: 32, steps per second: 220, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.746, 1.156], mean_best_reward: --\n",
      "  8105/100000: episode: 263, duration: 0.184s, episode steps: 40, steps per second: 217, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.648, 1.482], mean_best_reward: --\n",
      "  8165/100000: episode: 264, duration: 0.276s, episode steps: 60, steps per second: 217, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.239 [-1.485, 0.531], mean_best_reward: --\n",
      "  8187/100000: episode: 265, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.067 [-1.438, 0.925], mean_best_reward: --\n",
      "  8241/100000: episode: 266, duration: 0.265s, episode steps: 54, steps per second: 204, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.806, 1.219], mean_best_reward: --\n",
      "  8287/100000: episode: 267, duration: 0.267s, episode steps: 46, steps per second: 172, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.155 [-1.501, 0.370], mean_best_reward: --\n",
      "  8313/100000: episode: 268, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.597, 1.107], mean_best_reward: --\n",
      "  8347/100000: episode: 269, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.044 [-0.614, 1.035], mean_best_reward: --\n",
      "  8373/100000: episode: 270, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.069 [-1.177, 1.899], mean_best_reward: --\n",
      "  8403/100000: episode: 271, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.990, 0.583], mean_best_reward: --\n",
      "  8414/100000: episode: 272, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.109 [-0.962, 1.671], mean_best_reward: --\n",
      "  8437/100000: episode: 273, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.064 [-0.835, 1.568], mean_best_reward: --\n",
      "  8467/100000: episode: 274, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.043 [-2.029, 1.335], mean_best_reward: --\n",
      "  8510/100000: episode: 275, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.210 [-1.013, 1.388], mean_best_reward: --\n",
      "  8544/100000: episode: 276, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.194, 0.564], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8566/100000: episode: 277, duration: 0.106s, episode steps: 22, steps per second: 209, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.108 [-0.459, 1.295], mean_best_reward: --\n",
      "  8643/100000: episode: 278, duration: 0.461s, episode steps: 77, steps per second: 167, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.168 [-0.736, 1.167], mean_best_reward: --\n",
      "  8676/100000: episode: 279, duration: 0.154s, episode steps: 33, steps per second: 215, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.141 [-0.597, 1.152], mean_best_reward: --\n",
      "  8692/100000: episode: 280, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.103 [-1.168, 2.087], mean_best_reward: --\n",
      "  8711/100000: episode: 281, duration: 0.133s, episode steps: 19, steps per second: 143, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.097 [-1.106, 0.459], mean_best_reward: --\n",
      "  8777/100000: episode: 282, duration: 0.300s, episode steps: 66, steps per second: 220, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.155 [-0.658, 1.440], mean_best_reward: --\n",
      "  8848/100000: episode: 283, duration: 0.435s, episode steps: 71, steps per second: 163, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.010 [-1.990, 0.950], mean_best_reward: --\n",
      "  8907/100000: episode: 284, duration: 0.318s, episode steps: 59, steps per second: 185, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.060 [-1.364, 0.580], mean_best_reward: --\n",
      "  8957/100000: episode: 285, duration: 0.301s, episode steps: 50, steps per second: 166, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.182 [-1.289, 0.707], mean_best_reward: --\n",
      "  8983/100000: episode: 286, duration: 0.122s, episode steps: 26, steps per second: 213, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.115 [-0.819, 1.214], mean_best_reward: --\n",
      "  9010/100000: episode: 287, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.097 [-0.436, 0.931], mean_best_reward: --\n",
      "  9048/100000: episode: 288, duration: 0.253s, episode steps: 38, steps per second: 150, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.933, 0.627], mean_best_reward: --\n",
      "  9093/100000: episode: 289, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.085 [-0.862, 1.143], mean_best_reward: --\n",
      "  9123/100000: episode: 290, duration: 0.139s, episode steps: 30, steps per second: 216, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.090 [-0.947, 0.394], mean_best_reward: --\n",
      "  9149/100000: episode: 291, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.102 [-0.376, 1.243], mean_best_reward: --\n",
      "  9180/100000: episode: 292, duration: 0.138s, episode steps: 31, steps per second: 224, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.108 [-0.989, 0.569], mean_best_reward: --\n",
      "  9214/100000: episode: 293, duration: 0.153s, episode steps: 34, steps per second: 222, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.098 [-0.643, 1.139], mean_best_reward: --\n",
      "  9289/100000: episode: 294, duration: 0.374s, episode steps: 75, steps per second: 201, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.011 [-1.557, 0.961], mean_best_reward: --\n",
      "  9309/100000: episode: 295, duration: 0.092s, episode steps: 20, steps per second: 218, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.040, 0.544], mean_best_reward: --\n",
      "  9328/100000: episode: 296, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.095 [-1.190, 0.587], mean_best_reward: --\n",
      "  9369/100000: episode: 297, duration: 0.255s, episode steps: 41, steps per second: 161, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.025 [-1.268, 0.960], mean_best_reward: --\n",
      "  9386/100000: episode: 298, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.067 [-1.171, 0.822], mean_best_reward: --\n",
      "  9404/100000: episode: 299, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.055 [-1.477, 1.012], mean_best_reward: --\n",
      "  9474/100000: episode: 300, duration: 0.425s, episode steps: 70, steps per second: 165, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.154 [-1.017, 0.994], mean_best_reward: --\n",
      "  9509/100000: episode: 301, duration: 0.168s, episode steps: 35, steps per second: 209, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.056 [-2.050, 1.047], mean_best_reward: 109.000000\n",
      "  9567/100000: episode: 302, duration: 0.295s, episode steps: 58, steps per second: 196, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.160 [-0.601, 1.052], mean_best_reward: --\n",
      "  9611/100000: episode: 303, duration: 0.199s, episode steps: 44, steps per second: 221, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.176 [-1.153, 0.358], mean_best_reward: --\n",
      "  9629/100000: episode: 304, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.056 [-1.232, 0.824], mean_best_reward: --\n",
      "  9683/100000: episode: 305, duration: 0.254s, episode steps: 54, steps per second: 213, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.297, 0.601], mean_best_reward: --\n",
      "  9698/100000: episode: 306, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.068 [-1.432, 0.828], mean_best_reward: --\n",
      "  9719/100000: episode: 307, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.042 [-1.262, 0.827], mean_best_reward: --\n",
      "  9750/100000: episode: 308, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.032 [-1.001, 1.634], mean_best_reward: --\n",
      "  9761/100000: episode: 309, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.134 [-0.948, 1.561], mean_best_reward: --\n",
      "  9795/100000: episode: 310, duration: 0.172s, episode steps: 34, steps per second: 197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.099 [-0.877, 0.539], mean_best_reward: --\n",
      "  9812/100000: episode: 311, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.096 [-0.612, 1.398], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9855/100000: episode: 312, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.138 [-1.120, 0.634], mean_best_reward: --\n",
      "  9869/100000: episode: 313, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.094 [-1.674, 0.981], mean_best_reward: --\n",
      "  9945/100000: episode: 314, duration: 0.398s, episode steps: 76, steps per second: 191, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.133 [-1.254, 1.118], mean_best_reward: --\n",
      "  9985/100000: episode: 315, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.052 [-0.748, 1.098], mean_best_reward: --\n",
      " 10006/100000: episode: 316, duration: 0.098s, episode steps: 21, steps per second: 214, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.099 [-0.951, 1.895], mean_best_reward: --\n",
      " 10027/100000: episode: 317, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.123 [-1.038, 0.376], mean_best_reward: --\n",
      " 10066/100000: episode: 318, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.154 [-0.645, 1.028], mean_best_reward: --\n",
      " 10099/100000: episode: 319, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.076 [-1.323, 0.463], mean_best_reward: --\n",
      " 10113/100000: episode: 320, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.464, 0.799], mean_best_reward: --\n",
      " 10144/100000: episode: 321, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.076 [-1.209, 0.433], mean_best_reward: --\n",
      " 10182/100000: episode: 322, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.150 [-0.548, 1.006], mean_best_reward: --\n",
      " 10241/100000: episode: 323, duration: 0.303s, episode steps: 59, steps per second: 195, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.014 [-0.957, 1.170], mean_best_reward: --\n",
      " 10262/100000: episode: 324, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.074 [-2.376, 1.363], mean_best_reward: --\n",
      " 10333/100000: episode: 325, duration: 0.341s, episode steps: 71, steps per second: 208, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.104 [-1.534, 1.042], mean_best_reward: --\n",
      " 10363/100000: episode: 326, duration: 0.142s, episode steps: 30, steps per second: 212, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.040, 0.453], mean_best_reward: --\n",
      " 10387/100000: episode: 327, duration: 0.120s, episode steps: 24, steps per second: 199, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.024 [-1.140, 0.805], mean_best_reward: --\n",
      " 10417/100000: episode: 328, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.107 [-1.066, 0.618], mean_best_reward: --\n",
      " 10437/100000: episode: 329, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.088 [-0.596, 1.346], mean_best_reward: --\n",
      " 10463/100000: episode: 330, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.094 [-0.635, 0.966], mean_best_reward: --\n",
      " 10518/100000: episode: 331, duration: 0.253s, episode steps: 55, steps per second: 217, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.101 [-1.103, 0.584], mean_best_reward: --\n",
      " 10531/100000: episode: 332, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.101 [-1.323, 0.761], mean_best_reward: --\n",
      " 10568/100000: episode: 333, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.110 [-0.805, 1.620], mean_best_reward: --\n",
      " 10597/100000: episode: 334, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.065 [-1.222, 0.749], mean_best_reward: --\n",
      " 10651/100000: episode: 335, duration: 0.263s, episode steps: 54, steps per second: 205, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.087 [-1.565, 0.726], mean_best_reward: --\n",
      " 10670/100000: episode: 336, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.063 [-1.230, 2.021], mean_best_reward: --\n",
      " 10726/100000: episode: 337, duration: 0.293s, episode steps: 56, steps per second: 191, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.156 [-0.888, 0.984], mean_best_reward: --\n",
      " 10742/100000: episode: 338, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.790, 1.351], mean_best_reward: --\n",
      " 10768/100000: episode: 339, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.078 [-1.956, 0.998], mean_best_reward: --\n",
      " 10782/100000: episode: 340, duration: 0.112s, episode steps: 14, steps per second: 125, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.116 [-0.944, 1.650], mean_best_reward: --\n",
      " 10837/100000: episode: 341, duration: 0.265s, episode steps: 55, steps per second: 207, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.097 [-1.175, 0.788], mean_best_reward: --\n",
      " 10851/100000: episode: 342, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.085 [-1.846, 1.160], mean_best_reward: --\n",
      " 10911/100000: episode: 343, duration: 0.425s, episode steps: 60, steps per second: 141, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.050 [-1.209, 0.828], mean_best_reward: --\n",
      " 10947/100000: episode: 344, duration: 0.164s, episode steps: 36, steps per second: 219, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.294, 0.612], mean_best_reward: --\n",
      " 10967/100000: episode: 345, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.078 [-1.338, 0.794], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11006/100000: episode: 346, duration: 0.270s, episode steps: 39, steps per second: 144, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.072 [-1.598, 0.656], mean_best_reward: --\n",
      " 11076/100000: episode: 347, duration: 0.385s, episode steps: 70, steps per second: 182, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-0.738, 1.552], mean_best_reward: --\n",
      " 11119/100000: episode: 348, duration: 0.265s, episode steps: 43, steps per second: 163, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.139 [-0.938, 0.633], mean_best_reward: --\n",
      " 11156/100000: episode: 349, duration: 0.216s, episode steps: 37, steps per second: 172, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.088 [-0.550, 1.077], mean_best_reward: --\n",
      " 11181/100000: episode: 350, duration: 0.177s, episode steps: 25, steps per second: 141, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.107 [-1.023, 0.371], mean_best_reward: --\n",
      " 11196/100000: episode: 351, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.076 [-1.516, 0.968], mean_best_reward: 83.000000\n",
      " 11207/100000: episode: 352, duration: 0.083s, episode steps: 11, steps per second: 132, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.116 [-1.266, 0.782], mean_best_reward: --\n",
      " 11221/100000: episode: 353, duration: 0.096s, episode steps: 14, steps per second: 145, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.112 [-0.619, 1.104], mean_best_reward: --\n",
      " 11245/100000: episode: 354, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.109 [-0.738, 1.228], mean_best_reward: --\n",
      " 11263/100000: episode: 355, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.118, 0.572], mean_best_reward: --\n",
      " 11328/100000: episode: 356, duration: 0.345s, episode steps: 65, steps per second: 189, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.139 [-1.334, 0.920], mean_best_reward: --\n",
      " 11342/100000: episode: 357, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.094 [-0.746, 1.350], mean_best_reward: --\n",
      " 11395/100000: episode: 358, duration: 0.389s, episode steps: 53, steps per second: 136, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.063 [-1.061, 0.582], mean_best_reward: --\n",
      " 11441/100000: episode: 359, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.130 [-1.164, 0.523], mean_best_reward: --\n",
      " 11493/100000: episode: 360, duration: 0.321s, episode steps: 52, steps per second: 162, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.121, 0.767], mean_best_reward: --\n",
      " 11543/100000: episode: 361, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.055 [-0.625, 0.848], mean_best_reward: --\n",
      " 11563/100000: episode: 362, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.636, 1.196], mean_best_reward: --\n",
      " 11601/100000: episode: 363, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.119 [-0.490, 0.850], mean_best_reward: --\n",
      " 11637/100000: episode: 364, duration: 0.219s, episode steps: 36, steps per second: 165, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.548, 1.089], mean_best_reward: --\n",
      " 11675/100000: episode: 365, duration: 0.264s, episode steps: 38, steps per second: 144, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.935, 1.559], mean_best_reward: --\n",
      " 11689/100000: episode: 366, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.776, 1.333], mean_best_reward: --\n",
      " 11739/100000: episode: 367, duration: 0.295s, episode steps: 50, steps per second: 170, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.140 [-0.705, 1.399], mean_best_reward: --\n",
      " 11791/100000: episode: 368, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.007, 0.876], mean_best_reward: --\n",
      " 11827/100000: episode: 369, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.129 [-0.677, 1.663], mean_best_reward: --\n",
      " 11859/100000: episode: 370, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.516, 0.998], mean_best_reward: --\n",
      " 11881/100000: episode: 371, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.078 [-0.793, 1.735], mean_best_reward: --\n",
      " 11914/100000: episode: 372, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.077 [-1.602, 0.787], mean_best_reward: --\n",
      " 11945/100000: episode: 373, duration: 0.200s, episode steps: 31, steps per second: 155, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.116 [-0.543, 1.010], mean_best_reward: --\n",
      " 11972/100000: episode: 374, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.036 [-1.159, 1.816], mean_best_reward: --\n",
      " 12011/100000: episode: 375, duration: 0.250s, episode steps: 39, steps per second: 156, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.093 [-0.561, 1.055], mean_best_reward: --\n",
      " 12036/100000: episode: 376, duration: 0.125s, episode steps: 25, steps per second: 201, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.093 [-0.830, 1.264], mean_best_reward: --\n",
      " 12095/100000: episode: 377, duration: 0.321s, episode steps: 59, steps per second: 184, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.056 [-0.834, 0.545], mean_best_reward: --\n",
      " 12154/100000: episode: 378, duration: 0.321s, episode steps: 59, steps per second: 184, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.209 [-0.802, 1.539], mean_best_reward: --\n",
      " 12196/100000: episode: 379, duration: 0.223s, episode steps: 42, steps per second: 189, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.034 [-1.420, 0.794], mean_best_reward: --\n",
      " 12215/100000: episode: 380, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.093 [-0.800, 1.513], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12285/100000: episode: 381, duration: 0.351s, episode steps: 70, steps per second: 199, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.025 [-1.072, 0.725], mean_best_reward: --\n",
      " 12351/100000: episode: 382, duration: 0.306s, episode steps: 66, steps per second: 216, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.088 [-0.970, 1.281], mean_best_reward: --\n",
      " 12375/100000: episode: 383, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.035 [-0.978, 1.308], mean_best_reward: --\n",
      " 12433/100000: episode: 384, duration: 0.415s, episode steps: 58, steps per second: 140, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.133 [-0.568, 1.213], mean_best_reward: --\n",
      " 12520/100000: episode: 385, duration: 0.534s, episode steps: 87, steps per second: 163, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.028 [-0.860, 1.114], mean_best_reward: --\n",
      " 12535/100000: episode: 386, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.088 [-1.333, 0.827], mean_best_reward: --\n",
      " 12546/100000: episode: 387, duration: 0.079s, episode steps: 11, steps per second: 140, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-0.985, 1.592], mean_best_reward: --\n",
      " 12592/100000: episode: 388, duration: 0.327s, episode steps: 46, steps per second: 141, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.648, 1.195], mean_best_reward: --\n",
      " 12624/100000: episode: 389, duration: 0.266s, episode steps: 32, steps per second: 120, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.009 [-0.999, 1.429], mean_best_reward: --\n",
      " 12653/100000: episode: 390, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.051 [-0.812, 1.059], mean_best_reward: --\n",
      " 12676/100000: episode: 391, duration: 0.209s, episode steps: 23, steps per second: 110, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.091 [-0.585, 1.490], mean_best_reward: --\n",
      " 12710/100000: episode: 392, duration: 0.200s, episode steps: 34, steps per second: 170, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.091 [-0.562, 1.087], mean_best_reward: --\n",
      " 12721/100000: episode: 393, duration: 0.115s, episode steps: 11, steps per second: 95, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.124 [-0.766, 1.390], mean_best_reward: --\n",
      " 12732/100000: episode: 394, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.214, 1.966], mean_best_reward: --\n",
      " 12764/100000: episode: 395, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.603, 0.978], mean_best_reward: --\n",
      " 12839/100000: episode: 396, duration: 0.360s, episode steps: 75, steps per second: 208, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.073 [-1.564, 2.101], mean_best_reward: --\n",
      " 12859/100000: episode: 397, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.062 [-1.308, 0.823], mean_best_reward: --\n",
      " 12925/100000: episode: 398, duration: 0.398s, episode steps: 66, steps per second: 166, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.065 [-2.184, 1.566], mean_best_reward: --\n",
      " 12949/100000: episode: 399, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.432, 0.990], mean_best_reward: --\n",
      " 13044/100000: episode: 400, duration: 0.463s, episode steps: 95, steps per second: 205, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.026 [-0.934, 1.097], mean_best_reward: --\n",
      " 13065/100000: episode: 401, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.091 [-1.047, 2.119], mean_best_reward: 91.000000\n",
      " 13089/100000: episode: 402, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.066 [-0.836, 1.701], mean_best_reward: --\n",
      " 13100/100000: episode: 403, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.100 [-0.966, 1.537], mean_best_reward: --\n",
      " 13169/100000: episode: 404, duration: 0.342s, episode steps: 69, steps per second: 202, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.028 [-0.793, 0.903], mean_best_reward: --\n",
      " 13196/100000: episode: 405, duration: 0.125s, episode steps: 27, steps per second: 216, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.063 [-1.336, 0.618], mean_best_reward: --\n",
      " 13212/100000: episode: 406, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.075 [-0.977, 1.495], mean_best_reward: --\n",
      " 13237/100000: episode: 407, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.131 [-0.962, 0.555], mean_best_reward: --\n",
      " 13292/100000: episode: 408, duration: 0.286s, episode steps: 55, steps per second: 192, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.034 [-0.812, 1.116], mean_best_reward: --\n",
      " 13358/100000: episode: 409, duration: 0.330s, episode steps: 66, steps per second: 200, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.003 [-1.363, 0.763], mean_best_reward: --\n",
      " 13371/100000: episode: 410, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.130 [-1.543, 0.951], mean_best_reward: --\n",
      " 13395/100000: episode: 411, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.096 [-0.953, 0.631], mean_best_reward: --\n",
      " 13412/100000: episode: 412, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.106 [-0.568, 1.092], mean_best_reward: --\n",
      " 13434/100000: episode: 413, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.097 [-1.161, 2.172], mean_best_reward: --\n",
      " 13458/100000: episode: 414, duration: 0.111s, episode steps: 24, steps per second: 217, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.126 [-0.347, 0.908], mean_best_reward: --\n",
      " 13499/100000: episode: 415, duration: 0.193s, episode steps: 41, steps per second: 212, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.005 [-0.981, 1.290], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13556/100000: episode: 416, duration: 0.259s, episode steps: 57, steps per second: 220, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.036 [-0.652, 1.156], mean_best_reward: --\n",
      " 13577/100000: episode: 417, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.126 [-0.766, 1.279], mean_best_reward: --\n",
      " 13595/100000: episode: 418, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.390, 1.085], mean_best_reward: --\n",
      " 13613/100000: episode: 419, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.120 [-0.541, 1.064], mean_best_reward: --\n",
      " 13660/100000: episode: 420, duration: 0.213s, episode steps: 47, steps per second: 221, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.096 [-0.426, 1.392], mean_best_reward: --\n",
      " 13680/100000: episode: 421, duration: 0.093s, episode steps: 20, steps per second: 216, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.365, 0.942], mean_best_reward: --\n",
      " 13697/100000: episode: 422, duration: 0.087s, episode steps: 17, steps per second: 197, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.117 [-0.751, 1.478], mean_best_reward: --\n",
      " 13716/100000: episode: 423, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-1.011, 0.585], mean_best_reward: --\n",
      " 13774/100000: episode: 424, duration: 0.294s, episode steps: 58, steps per second: 198, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.041 [-1.103, 0.949], mean_best_reward: --\n",
      " 13784/100000: episode: 425, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.126 [-1.915, 1.174], mean_best_reward: --\n",
      " 13819/100000: episode: 426, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.075 [-0.601, 1.239], mean_best_reward: --\n",
      " 13883/100000: episode: 427, duration: 0.322s, episode steps: 64, steps per second: 198, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.046 [-1.021, 1.033], mean_best_reward: --\n",
      " 13917/100000: episode: 428, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-0.388, 1.331], mean_best_reward: --\n",
      " 13944/100000: episode: 429, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.131 [-0.399, 1.224], mean_best_reward: --\n",
      " 14048/100000: episode: 430, duration: 0.521s, episode steps: 104, steps per second: 200, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.034 [-0.773, 1.295], mean_best_reward: --\n",
      " 14093/100000: episode: 431, duration: 0.214s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.049 [-1.163, 0.771], mean_best_reward: --\n",
      " 14127/100000: episode: 432, duration: 0.157s, episode steps: 34, steps per second: 216, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.024 [-1.267, 0.642], mean_best_reward: --\n",
      " 14198/100000: episode: 433, duration: 0.328s, episode steps: 71, steps per second: 217, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.082 [-1.232, 0.957], mean_best_reward: --\n",
      " 14258/100000: episode: 434, duration: 0.292s, episode steps: 60, steps per second: 205, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.070 [-0.581, 0.867], mean_best_reward: --\n",
      " 14295/100000: episode: 435, duration: 0.262s, episode steps: 37, steps per second: 141, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.035 [-1.111, 1.259], mean_best_reward: --\n",
      " 14372/100000: episode: 436, duration: 0.395s, episode steps: 77, steps per second: 195, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.037 [-1.483, 1.344], mean_best_reward: --\n",
      " 14417/100000: episode: 437, duration: 0.203s, episode steps: 45, steps per second: 221, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.062 [-1.062, 0.776], mean_best_reward: --\n",
      " 14477/100000: episode: 438, duration: 0.293s, episode steps: 60, steps per second: 205, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.133 [-0.791, 1.342], mean_best_reward: --\n",
      " 14508/100000: episode: 439, duration: 0.227s, episode steps: 31, steps per second: 137, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.049 [-0.762, 1.512], mean_best_reward: --\n",
      " 14531/100000: episode: 440, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.109 [-1.009, 0.378], mean_best_reward: --\n",
      " 14547/100000: episode: 441, duration: 0.129s, episode steps: 16, steps per second: 124, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.114 [-0.746, 1.351], mean_best_reward: --\n",
      " 14557/100000: episode: 442, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.098 [-1.016, 1.596], mean_best_reward: --\n",
      " 14572/100000: episode: 443, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.114 [-1.142, 0.591], mean_best_reward: --\n",
      " 14599/100000: episode: 444, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.070 [-0.753, 1.250], mean_best_reward: --\n",
      " 14634/100000: episode: 445, duration: 0.228s, episode steps: 35, steps per second: 153, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.181 [-0.655, 1.384], mean_best_reward: --\n",
      " 14654/100000: episode: 446, duration: 0.150s, episode steps: 20, steps per second: 133, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.070 [-0.762, 1.550], mean_best_reward: --\n",
      " 14681/100000: episode: 447, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.085 [-1.586, 0.820], mean_best_reward: --\n",
      " 14691/100000: episode: 448, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.112 [-1.003, 1.576], mean_best_reward: --\n",
      " 14727/100000: episode: 449, duration: 0.456s, episode steps: 36, steps per second: 79, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.127 [-0.542, 0.919], mean_best_reward: --\n",
      " 14745/100000: episode: 450, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.066 [-1.757, 1.014], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14778/100000: episode: 451, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.064 [-1.150, 0.621], mean_best_reward: 88.500000\n",
      " 14804/100000: episode: 452, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.623, 1.176], mean_best_reward: --\n",
      " 14824/100000: episode: 453, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.203, 0.631], mean_best_reward: --\n",
      " 14877/100000: episode: 454, duration: 0.251s, episode steps: 53, steps per second: 211, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.011 [-1.279, 1.104], mean_best_reward: --\n",
      " 14908/100000: episode: 455, duration: 0.184s, episode steps: 31, steps per second: 168, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.133 [-0.587, 0.929], mean_best_reward: --\n",
      " 14949/100000: episode: 456, duration: 0.329s, episode steps: 41, steps per second: 125, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.137 [-1.297, 0.685], mean_best_reward: --\n",
      " 14980/100000: episode: 457, duration: 0.214s, episode steps: 31, steps per second: 145, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.067 [-1.892, 0.962], mean_best_reward: --\n",
      " 15017/100000: episode: 458, duration: 0.227s, episode steps: 37, steps per second: 163, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: 0.038 [-1.959, 1.689], mean_best_reward: --\n",
      " 15037/100000: episode: 459, duration: 0.139s, episode steps: 20, steps per second: 143, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.058 [-1.002, 1.455], mean_best_reward: --\n",
      " 15075/100000: episode: 460, duration: 0.269s, episode steps: 38, steps per second: 141, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.015 [-0.999, 1.232], mean_best_reward: --\n",
      " 15132/100000: episode: 461, duration: 0.265s, episode steps: 57, steps per second: 215, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.223 [-1.014, 0.825], mean_best_reward: --\n",
      " 15160/100000: episode: 462, duration: 0.136s, episode steps: 28, steps per second: 207, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.154, 0.616], mean_best_reward: --\n",
      " 15217/100000: episode: 463, duration: 0.273s, episode steps: 57, steps per second: 209, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.026 [-0.990, 0.585], mean_best_reward: --\n",
      " 15266/100000: episode: 464, duration: 0.225s, episode steps: 49, steps per second: 217, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.126 [-0.469, 1.151], mean_best_reward: --\n",
      " 15292/100000: episode: 465, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-1.061, 0.618], mean_best_reward: --\n",
      " 15322/100000: episode: 466, duration: 0.145s, episode steps: 30, steps per second: 206, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.132, 0.624], mean_best_reward: --\n",
      " 15389/100000: episode: 467, duration: 0.354s, episode steps: 67, steps per second: 189, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.065 [-0.883, 1.495], mean_best_reward: --\n",
      " 15403/100000: episode: 468, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.088 [-0.961, 1.444], mean_best_reward: --\n",
      " 15426/100000: episode: 469, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.062 [-1.454, 0.803], mean_best_reward: --\n",
      " 15454/100000: episode: 470, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.437, 0.806], mean_best_reward: --\n",
      " 15490/100000: episode: 471, duration: 0.182s, episode steps: 36, steps per second: 197, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.122 [-0.472, 0.921], mean_best_reward: --\n",
      " 15522/100000: episode: 472, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-1.379, 0.650], mean_best_reward: --\n",
      " 15567/100000: episode: 473, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.096 [-0.395, 1.079], mean_best_reward: --\n",
      " 15585/100000: episode: 474, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.090 [-0.969, 1.683], mean_best_reward: --\n",
      " 15621/100000: episode: 475, duration: 0.214s, episode steps: 36, steps per second: 168, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.089 [-0.795, 0.453], mean_best_reward: --\n",
      " 15658/100000: episode: 476, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.091 [-0.613, 1.205], mean_best_reward: --\n",
      " 15697/100000: episode: 477, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.070 [-0.774, 1.411], mean_best_reward: --\n",
      " 15724/100000: episode: 478, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.079 [-0.596, 1.216], mean_best_reward: --\n",
      " 15779/100000: episode: 479, duration: 0.273s, episode steps: 55, steps per second: 201, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.135 [-1.214, 0.807], mean_best_reward: --\n",
      " 15816/100000: episode: 480, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: 0.056 [-1.921, 1.590], mean_best_reward: --\n",
      " 15838/100000: episode: 481, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.090 [-0.595, 0.871], mean_best_reward: --\n",
      " 15849/100000: episode: 482, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.016, 1.754], mean_best_reward: --\n",
      " 15887/100000: episode: 483, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.083, 0.507], mean_best_reward: --\n",
      " 15915/100000: episode: 484, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.044 [-1.293, 0.651], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15963/100000: episode: 485, duration: 0.295s, episode steps: 48, steps per second: 162, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.027 [-1.160, 0.953], mean_best_reward: --\n",
      " 16019/100000: episode: 486, duration: 0.285s, episode steps: 56, steps per second: 197, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.053 [-0.603, 1.023], mean_best_reward: --\n",
      " 16052/100000: episode: 487, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.087 [-0.585, 1.296], mean_best_reward: --\n",
      " 16090/100000: episode: 488, duration: 0.181s, episode steps: 38, steps per second: 210, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.006 [-1.511, 0.993], mean_best_reward: --\n",
      " 16131/100000: episode: 489, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.152 [-1.112, 1.562], mean_best_reward: --\n",
      " 16181/100000: episode: 490, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.080 [-0.798, 1.140], mean_best_reward: --\n",
      " 16205/100000: episode: 491, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.425, 0.747], mean_best_reward: --\n",
      " 16235/100000: episode: 492, duration: 0.178s, episode steps: 30, steps per second: 169, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.128 [-0.794, 1.243], mean_best_reward: --\n",
      " 16286/100000: episode: 493, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.076 [-0.483, 1.399], mean_best_reward: --\n",
      " 16328/100000: episode: 494, duration: 0.217s, episode steps: 42, steps per second: 193, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.752, 1.321], mean_best_reward: --\n",
      " 16368/100000: episode: 495, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.060 [-0.853, 1.832], mean_best_reward: --\n",
      " 16397/100000: episode: 496, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.111 [-0.357, 0.885], mean_best_reward: --\n",
      " 16416/100000: episode: 497, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.083 [-1.523, 0.941], mean_best_reward: --\n",
      " 16441/100000: episode: 498, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.077 [-0.620, 1.113], mean_best_reward: --\n",
      " 16476/100000: episode: 499, duration: 0.161s, episode steps: 35, steps per second: 218, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.140 [-0.596, 1.248], mean_best_reward: --\n",
      " 16500/100000: episode: 500, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.597, 1.218], mean_best_reward: --\n",
      " 16517/100000: episode: 501, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.085 [-1.497, 1.005], mean_best_reward: 91.500000\n",
      " 16546/100000: episode: 502, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.098 [-0.579, 1.533], mean_best_reward: --\n",
      " 16608/100000: episode: 503, duration: 0.301s, episode steps: 62, steps per second: 206, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.643, 1.536], mean_best_reward: --\n",
      " 16662/100000: episode: 504, duration: 0.262s, episode steps: 54, steps per second: 206, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.060 [-1.763, 0.806], mean_best_reward: --\n",
      " 16709/100000: episode: 505, duration: 0.247s, episode steps: 47, steps per second: 191, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.113 [-0.590, 1.406], mean_best_reward: --\n",
      " 16734/100000: episode: 506, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.076 [-0.753, 1.246], mean_best_reward: --\n",
      " 16767/100000: episode: 507, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.157 [-0.388, 0.984], mean_best_reward: --\n",
      " 16791/100000: episode: 508, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.124 [-0.634, 0.980], mean_best_reward: --\n",
      " 16815/100000: episode: 509, duration: 0.113s, episode steps: 24, steps per second: 213, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.022, 0.623], mean_best_reward: --\n",
      " 16847/100000: episode: 510, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.137 [-1.465, 0.556], mean_best_reward: --\n",
      " 16881/100000: episode: 511, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.090 [-1.189, 1.733], mean_best_reward: --\n",
      " 16945/100000: episode: 512, duration: 0.303s, episode steps: 64, steps per second: 211, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.131 [-0.451, 0.919], mean_best_reward: --\n",
      " 16978/100000: episode: 513, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.079 [-0.568, 1.122], mean_best_reward: --\n",
      " 17007/100000: episode: 514, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.116 [-0.439, 0.850], mean_best_reward: --\n",
      " 17070/100000: episode: 515, duration: 0.315s, episode steps: 63, steps per second: 200, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.173 [-1.113, 0.741], mean_best_reward: --\n",
      " 17113/100000: episode: 516, duration: 0.198s, episode steps: 43, steps per second: 218, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.053 [-0.592, 0.879], mean_best_reward: --\n",
      " 17154/100000: episode: 517, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.185 [-1.249, 0.540], mean_best_reward: --\n",
      " 17168/100000: episode: 518, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.100 [-1.550, 0.956], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17205/100000: episode: 519, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.091 [-1.455, 0.569], mean_best_reward: --\n",
      " 17242/100000: episode: 520, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.131 [-1.813, 0.579], mean_best_reward: --\n",
      " 17264/100000: episode: 521, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.102, 0.591], mean_best_reward: --\n",
      " 17290/100000: episode: 522, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.020 [-1.178, 1.787], mean_best_reward: --\n",
      " 17324/100000: episode: 523, duration: 0.157s, episode steps: 34, steps per second: 217, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.051 [-1.685, 1.018], mean_best_reward: --\n",
      " 17374/100000: episode: 524, duration: 0.252s, episode steps: 50, steps per second: 199, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.170 [-0.646, 1.139], mean_best_reward: --\n",
      " 17420/100000: episode: 525, duration: 0.221s, episode steps: 46, steps per second: 208, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.135, 0.388], mean_best_reward: --\n",
      " 17463/100000: episode: 526, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.056 [-0.845, 1.212], mean_best_reward: --\n",
      " 17506/100000: episode: 527, duration: 0.204s, episode steps: 43, steps per second: 211, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.067 [-0.843, 1.814], mean_best_reward: --\n",
      " 17527/100000: episode: 528, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.069 [-0.647, 1.368], mean_best_reward: --\n",
      " 17567/100000: episode: 529, duration: 0.264s, episode steps: 40, steps per second: 152, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.157 [-1.097, 0.594], mean_best_reward: --\n",
      " 17598/100000: episode: 530, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.102 [-0.597, 1.079], mean_best_reward: --\n",
      " 17611/100000: episode: 531, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-1.813, 1.002], mean_best_reward: --\n",
      " 17628/100000: episode: 532, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.095 [-1.299, 0.739], mean_best_reward: --\n",
      " 17689/100000: episode: 533, duration: 0.290s, episode steps: 61, steps per second: 211, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.239 [-1.334, 0.548], mean_best_reward: --\n",
      " 17754/100000: episode: 534, duration: 0.324s, episode steps: 65, steps per second: 200, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.205 [-1.149, 0.629], mean_best_reward: --\n",
      " 17807/100000: episode: 535, duration: 0.252s, episode steps: 53, steps per second: 210, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.147 [-1.369, 0.816], mean_best_reward: --\n",
      " 17834/100000: episode: 536, duration: 0.127s, episode steps: 27, steps per second: 213, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.093 [-1.022, 0.553], mean_best_reward: --\n",
      " 17859/100000: episode: 537, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.095 [-0.898, 0.578], mean_best_reward: --\n",
      " 17901/100000: episode: 538, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.113 [-1.297, 0.507], mean_best_reward: --\n",
      " 17934/100000: episode: 539, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.051 [-1.472, 0.950], mean_best_reward: --\n",
      " 17949/100000: episode: 540, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.105 [-1.462, 0.927], mean_best_reward: --\n",
      " 17961/100000: episode: 541, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.151, 1.995], mean_best_reward: --\n",
      " 17998/100000: episode: 542, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.142 [-0.953, 0.496], mean_best_reward: --\n",
      " 18079/100000: episode: 543, duration: 0.482s, episode steps: 81, steps per second: 168, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.007 [-0.801, 1.111], mean_best_reward: --\n",
      " 18112/100000: episode: 544, duration: 0.331s, episode steps: 33, steps per second: 100, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.040 [-0.759, 1.199], mean_best_reward: --\n",
      " 18135/100000: episode: 545, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.076 [-1.488, 0.953], mean_best_reward: --\n",
      " 18175/100000: episode: 546, duration: 0.261s, episode steps: 40, steps per second: 153, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.149 [-0.703, 1.091], mean_best_reward: --\n",
      " 18225/100000: episode: 547, duration: 0.243s, episode steps: 50, steps per second: 206, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.049 [-0.919, 1.309], mean_best_reward: --\n",
      " 18339/100000: episode: 548, duration: 0.520s, episode steps: 114, steps per second: 219, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.023 [-1.097, 0.964], mean_best_reward: --\n",
      " 18383/100000: episode: 549, duration: 0.212s, episode steps: 44, steps per second: 208, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.115 [-0.629, 0.961], mean_best_reward: --\n",
      " 18467/100000: episode: 550, duration: 0.429s, episode steps: 84, steps per second: 196, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.085 [-0.848, 1.001], mean_best_reward: --\n",
      " 18491/100000: episode: 551, duration: 0.165s, episode steps: 24, steps per second: 146, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.096 [-0.937, 0.606], mean_best_reward: 86.500000\n",
      " 18519/100000: episode: 552, duration: 0.188s, episode steps: 28, steps per second: 149, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.040 [-1.050, 0.743], mean_best_reward: --\n",
      " 18541/100000: episode: 553, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.066 [-0.642, 1.342], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18569/100000: episode: 554, duration: 0.186s, episode steps: 28, steps per second: 151, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.547, 0.944], mean_best_reward: --\n",
      " 18597/100000: episode: 555, duration: 0.190s, episode steps: 28, steps per second: 148, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.041 [-0.774, 1.212], mean_best_reward: --\n",
      " 18630/100000: episode: 556, duration: 0.197s, episode steps: 33, steps per second: 167, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.161 [-1.193, 0.739], mean_best_reward: --\n",
      " 18686/100000: episode: 557, duration: 0.362s, episode steps: 56, steps per second: 155, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.241 [-0.941, 1.508], mean_best_reward: --\n",
      " 18705/100000: episode: 558, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.112 [-1.379, 0.748], mean_best_reward: --\n",
      " 18726/100000: episode: 559, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.079 [-1.016, 0.596], mean_best_reward: --\n",
      " 18757/100000: episode: 560, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.021 [-1.130, 1.600], mean_best_reward: --\n",
      " 18789/100000: episode: 561, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.070 [-0.775, 1.413], mean_best_reward: --\n",
      " 18814/100000: episode: 562, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.097 [-1.220, 0.804], mean_best_reward: --\n",
      " 18826/100000: episode: 563, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.095 [-2.231, 1.419], mean_best_reward: --\n",
      " 18877/100000: episode: 564, duration: 0.241s, episode steps: 51, steps per second: 212, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.164 [-1.180, 0.735], mean_best_reward: --\n",
      " 18947/100000: episode: 565, duration: 0.472s, episode steps: 70, steps per second: 148, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.142 [-1.253, 0.694], mean_best_reward: --\n",
      " 18971/100000: episode: 566, duration: 0.170s, episode steps: 24, steps per second: 141, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.123, 0.739], mean_best_reward: --\n",
      " 18985/100000: episode: 567, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.787, 1.328], mean_best_reward: --\n",
      " 19022/100000: episode: 568, duration: 0.224s, episode steps: 37, steps per second: 165, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.031 [-1.333, 0.833], mean_best_reward: --\n",
      " 19072/100000: episode: 569, duration: 0.283s, episode steps: 50, steps per second: 177, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.089 [-1.556, 0.868], mean_best_reward: --\n",
      " 19116/100000: episode: 570, duration: 0.359s, episode steps: 44, steps per second: 123, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.133 [-0.490, 0.944], mean_best_reward: --\n",
      " 19158/100000: episode: 571, duration: 0.301s, episode steps: 42, steps per second: 139, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-1.234, 0.595], mean_best_reward: --\n",
      " 19170/100000: episode: 572, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.091 [-1.006, 1.683], mean_best_reward: --\n",
      " 19188/100000: episode: 573, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.100 [-0.588, 1.254], mean_best_reward: --\n",
      " 19217/100000: episode: 574, duration: 0.177s, episode steps: 29, steps per second: 163, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.042 [-1.223, 0.833], mean_best_reward: --\n",
      " 19242/100000: episode: 575, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.051 [-0.741, 1.121], mean_best_reward: --\n",
      " 19256/100000: episode: 576, duration: 0.094s, episode steps: 14, steps per second: 150, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.087 [-1.367, 0.821], mean_best_reward: --\n",
      " 19304/100000: episode: 577, duration: 0.268s, episode steps: 48, steps per second: 179, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.145 [-1.073, 0.677], mean_best_reward: --\n",
      " 19342/100000: episode: 578, duration: 0.187s, episode steps: 38, steps per second: 204, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.131 [-0.873, 0.535], mean_best_reward: --\n",
      " 19373/100000: episode: 579, duration: 0.150s, episode steps: 31, steps per second: 207, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.121 [-0.902, 0.601], mean_best_reward: --\n",
      " 19442/100000: episode: 580, duration: 0.401s, episode steps: 69, steps per second: 172, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.152 [-0.543, 1.115], mean_best_reward: --\n",
      " 19470/100000: episode: 581, duration: 0.192s, episode steps: 28, steps per second: 146, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: -0.018 [-1.916, 2.358], mean_best_reward: --\n",
      " 19526/100000: episode: 582, duration: 0.294s, episode steps: 56, steps per second: 191, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.756, 1.059], mean_best_reward: --\n",
      " 19568/100000: episode: 583, duration: 0.290s, episode steps: 42, steps per second: 145, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.157 [-1.017, 0.561], mean_best_reward: --\n",
      " 19585/100000: episode: 584, duration: 0.084s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.099 [-0.762, 1.205], mean_best_reward: --\n",
      " 19606/100000: episode: 585, duration: 0.145s, episode steps: 21, steps per second: 145, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.045 [-1.549, 1.026], mean_best_reward: --\n",
      " 19625/100000: episode: 586, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.085 [-0.954, 1.632], mean_best_reward: --\n",
      " 19659/100000: episode: 587, duration: 0.254s, episode steps: 34, steps per second: 134, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.807, 1.152], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19710/100000: episode: 588, duration: 0.386s, episode steps: 51, steps per second: 132, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.090 [-0.519, 1.057], mean_best_reward: --\n",
      " 19748/100000: episode: 589, duration: 0.259s, episode steps: 38, steps per second: 147, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.057 [-1.420, 0.743], mean_best_reward: --\n",
      " 19767/100000: episode: 590, duration: 0.140s, episode steps: 19, steps per second: 135, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.085 [-1.318, 0.642], mean_best_reward: --\n",
      " 19891/100000: episode: 591, duration: 0.714s, episode steps: 124, steps per second: 174, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.003 [-0.990, 1.086], mean_best_reward: --\n",
      " 19915/100000: episode: 592, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.781, 1.075], mean_best_reward: --\n",
      " 20019/100000: episode: 593, duration: 0.685s, episode steps: 104, steps per second: 152, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.036 [-1.110, 1.489], mean_best_reward: --\n",
      " 20045/100000: episode: 594, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.448, 0.936], mean_best_reward: --\n",
      " 20077/100000: episode: 595, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.161 [-0.945, 0.544], mean_best_reward: --\n",
      " 20091/100000: episode: 596, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.013, 0.641], mean_best_reward: --\n",
      " 20113/100000: episode: 597, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.827, 1.271], mean_best_reward: --\n",
      " 20173/100000: episode: 598, duration: 0.339s, episode steps: 60, steps per second: 177, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.543, 1.341], mean_best_reward: --\n",
      " 20240/100000: episode: 599, duration: 0.331s, episode steps: 67, steps per second: 202, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.083 [-0.990, 0.775], mean_best_reward: --\n",
      " 20283/100000: episode: 600, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.022 [-1.153, 0.971], mean_best_reward: --\n",
      " 20349/100000: episode: 601, duration: 0.348s, episode steps: 66, steps per second: 190, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-0.629, 1.128], mean_best_reward: 115.500000\n",
      " 20389/100000: episode: 602, duration: 0.185s, episode steps: 40, steps per second: 216, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.110 [-1.137, 0.586], mean_best_reward: --\n",
      " 20410/100000: episode: 603, duration: 0.136s, episode steps: 21, steps per second: 155, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.049 [-1.431, 0.810], mean_best_reward: --\n",
      " 20449/100000: episode: 604, duration: 0.177s, episode steps: 39, steps per second: 220, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.069 [-1.190, 0.580], mean_best_reward: --\n",
      " 20466/100000: episode: 605, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.091 [-1.064, 0.623], mean_best_reward: --\n",
      " 20495/100000: episode: 606, duration: 0.133s, episode steps: 29, steps per second: 218, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.035 [-1.595, 1.150], mean_best_reward: --\n",
      " 20540/100000: episode: 607, duration: 0.208s, episode steps: 45, steps per second: 216, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.058 [-1.183, 0.586], mean_best_reward: --\n",
      " 20590/100000: episode: 608, duration: 0.240s, episode steps: 50, steps per second: 208, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.486, 1.299], mean_best_reward: --\n",
      " 20627/100000: episode: 609, duration: 0.170s, episode steps: 37, steps per second: 217, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.023 [-1.561, 0.930], mean_best_reward: --\n",
      " 20686/100000: episode: 610, duration: 0.276s, episode steps: 59, steps per second: 213, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.048 [-0.839, 1.047], mean_best_reward: --\n",
      " 20729/100000: episode: 611, duration: 0.199s, episode steps: 43, steps per second: 216, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.126 [-0.734, 1.096], mean_best_reward: --\n",
      " 20826/100000: episode: 612, duration: 0.436s, episode steps: 97, steps per second: 222, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.132 [-0.919, 0.847], mean_best_reward: --\n",
      " 20840/100000: episode: 613, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.106 [-1.794, 1.123], mean_best_reward: --\n",
      " 20963/100000: episode: 614, duration: 0.558s, episode steps: 123, steps per second: 220, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.110 [-0.823, 1.308], mean_best_reward: --\n",
      " 20981/100000: episode: 615, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.115 [-0.761, 1.596], mean_best_reward: --\n",
      " 21014/100000: episode: 616, duration: 0.156s, episode steps: 33, steps per second: 211, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.045 [-1.045, 0.618], mean_best_reward: --\n",
      " 21035/100000: episode: 617, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.089 [-1.069, 0.618], mean_best_reward: --\n",
      " 21111/100000: episode: 618, duration: 0.353s, episode steps: 76, steps per second: 215, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.077, 0.855], mean_best_reward: --\n",
      " 21138/100000: episode: 619, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.088 [-1.427, 0.959], mean_best_reward: --\n",
      " 21176/100000: episode: 620, duration: 0.335s, episode steps: 38, steps per second: 113, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.777, 1.427], mean_best_reward: --\n",
      " 21208/100000: episode: 621, duration: 0.255s, episode steps: 32, steps per second: 125, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.077 [-1.128, 0.729], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21249/100000: episode: 622, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.149 [-0.727, 1.547], mean_best_reward: --\n",
      " 21265/100000: episode: 623, duration: 0.102s, episode steps: 16, steps per second: 156, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.069, 0.616], mean_best_reward: --\n",
      " 21282/100000: episode: 624, duration: 0.147s, episode steps: 17, steps per second: 116, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.087 [-0.764, 1.288], mean_best_reward: --\n",
      " 21344/100000: episode: 625, duration: 0.382s, episode steps: 62, steps per second: 162, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.204, 0.457], mean_best_reward: --\n",
      " 21361/100000: episode: 626, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.099 [-1.883, 0.968], mean_best_reward: --\n",
      " 21432/100000: episode: 627, duration: 0.358s, episode steps: 71, steps per second: 198, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.071 [-1.218, 0.677], mean_best_reward: --\n",
      " 21463/100000: episode: 628, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.094 [-1.549, 0.598], mean_best_reward: --\n",
      " 21515/100000: episode: 629, duration: 0.256s, episode steps: 52, steps per second: 203, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.025 [-1.888, 1.509], mean_best_reward: --\n",
      " 21585/100000: episode: 630, duration: 0.319s, episode steps: 70, steps per second: 220, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.118 [-1.000, 0.669], mean_best_reward: --\n",
      " 21607/100000: episode: 631, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.128, 0.810], mean_best_reward: --\n",
      " 21667/100000: episode: 632, duration: 0.286s, episode steps: 60, steps per second: 209, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.032 [-0.796, 0.859], mean_best_reward: --\n",
      " 21689/100000: episode: 633, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.016, 0.569], mean_best_reward: --\n",
      " 21724/100000: episode: 634, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.093 [-1.395, 0.605], mean_best_reward: --\n",
      " 21739/100000: episode: 635, duration: 0.070s, episode steps: 15, steps per second: 213, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.090 [-1.294, 0.809], mean_best_reward: --\n",
      " 21754/100000: episode: 636, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.104 [-1.208, 0.765], mean_best_reward: --\n",
      " 21792/100000: episode: 637, duration: 0.175s, episode steps: 38, steps per second: 217, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.017 [-1.332, 0.955], mean_best_reward: --\n",
      " 21808/100000: episode: 638, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.216, 0.751], mean_best_reward: --\n",
      " 21840/100000: episode: 639, duration: 0.148s, episode steps: 32, steps per second: 216, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.045, 0.450], mean_best_reward: --\n",
      " 21870/100000: episode: 640, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.065 [-0.748, 1.110], mean_best_reward: --\n",
      " 21893/100000: episode: 641, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.094 [-0.619, 1.110], mean_best_reward: --\n",
      " 21959/100000: episode: 642, duration: 0.302s, episode steps: 66, steps per second: 218, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.165 [-1.265, 0.526], mean_best_reward: --\n",
      " 21994/100000: episode: 643, duration: 0.160s, episode steps: 35, steps per second: 219, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.101 [-0.917, 0.417], mean_best_reward: --\n",
      " 22040/100000: episode: 644, duration: 0.209s, episode steps: 46, steps per second: 220, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.115 [-1.281, 0.571], mean_best_reward: --\n",
      " 22084/100000: episode: 645, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.067 [-1.359, 1.369], mean_best_reward: --\n",
      " 22131/100000: episode: 646, duration: 0.218s, episode steps: 47, steps per second: 215, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.059 [-0.607, 1.385], mean_best_reward: --\n",
      " 22165/100000: episode: 647, duration: 0.155s, episode steps: 34, steps per second: 219, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.118 [-0.812, 1.387], mean_best_reward: --\n",
      " 22212/100000: episode: 648, duration: 0.212s, episode steps: 47, steps per second: 221, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.176 [-1.067, 0.922], mean_best_reward: --\n",
      " 22243/100000: episode: 649, duration: 0.141s, episode steps: 31, steps per second: 220, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.082 [-0.614, 1.491], mean_best_reward: --\n",
      " 22269/100000: episode: 650, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.032 [-1.443, 1.019], mean_best_reward: --\n",
      " 22289/100000: episode: 651, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.077 [-1.517, 0.751], mean_best_reward: 101.000000\n",
      " 22325/100000: episode: 652, duration: 0.169s, episode steps: 36, steps per second: 213, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.412, 1.144], mean_best_reward: --\n",
      " 22338/100000: episode: 653, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.095 [-0.969, 1.710], mean_best_reward: --\n",
      " 22383/100000: episode: 654, duration: 0.207s, episode steps: 45, steps per second: 217, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.120 [-0.835, 1.203], mean_best_reward: --\n",
      " 22405/100000: episode: 655, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.112 [-0.767, 1.262], mean_best_reward: --\n",
      " 22424/100000: episode: 656, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.086 [-1.121, 0.601], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22446/100000: episode: 657, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.431, 1.051], mean_best_reward: --\n",
      " 22466/100000: episode: 658, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.038 [-1.614, 1.195], mean_best_reward: --\n",
      " 22492/100000: episode: 659, duration: 0.120s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.302, 0.600], mean_best_reward: --\n",
      " 22529/100000: episode: 660, duration: 0.170s, episode steps: 37, steps per second: 217, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.077 [-0.827, 0.384], mean_best_reward: --\n",
      " 22543/100000: episode: 661, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.103 [-1.800, 1.010], mean_best_reward: --\n",
      " 22624/100000: episode: 662, duration: 0.385s, episode steps: 81, steps per second: 210, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.091 [-1.246, 0.597], mean_best_reward: --\n",
      " 22702/100000: episode: 663, duration: 0.351s, episode steps: 78, steps per second: 222, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.114 [-1.086, 1.565], mean_best_reward: --\n",
      " 22739/100000: episode: 664, duration: 0.174s, episode steps: 37, steps per second: 213, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.068 [-0.995, 0.577], mean_best_reward: --\n",
      " 22775/100000: episode: 665, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.079 [-2.232, 1.158], mean_best_reward: --\n",
      " 22809/100000: episode: 666, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.340, 1.000], mean_best_reward: --\n",
      " 22864/100000: episode: 667, duration: 0.253s, episode steps: 55, steps per second: 217, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.066 [-0.959, 1.292], mean_best_reward: --\n",
      " 22948/100000: episode: 668, duration: 0.382s, episode steps: 84, steps per second: 220, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.843, 1.073], mean_best_reward: --\n",
      " 22975/100000: episode: 669, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.043 [-1.568, 0.993], mean_best_reward: --\n",
      " 22990/100000: episode: 670, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.087 [-1.715, 0.987], mean_best_reward: --\n",
      " 23020/100000: episode: 671, duration: 0.138s, episode steps: 30, steps per second: 217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.102 [-0.609, 1.393], mean_best_reward: --\n",
      " 23057/100000: episode: 672, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.077 [-0.864, 0.526], mean_best_reward: --\n",
      " 23075/100000: episode: 673, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.068 [-1.002, 1.604], mean_best_reward: --\n",
      " 23164/100000: episode: 674, duration: 0.437s, episode steps: 89, steps per second: 203, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.080 [-0.747, 1.002], mean_best_reward: --\n",
      " 23225/100000: episode: 675, duration: 0.308s, episode steps: 61, steps per second: 198, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.119 [-1.136, 0.719], mean_best_reward: --\n",
      " 23281/100000: episode: 676, duration: 0.279s, episode steps: 56, steps per second: 201, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.022 [-1.138, 0.935], mean_best_reward: --\n",
      " 23296/100000: episode: 677, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.094 [-0.825, 1.414], mean_best_reward: --\n",
      " 23325/100000: episode: 678, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.075 [-0.488, 1.120], mean_best_reward: --\n",
      " 23378/100000: episode: 679, duration: 0.278s, episode steps: 53, steps per second: 191, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.130 [-0.613, 0.973], mean_best_reward: --\n",
      " 23395/100000: episode: 680, duration: 0.078s, episode steps: 17, steps per second: 217, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.088 [-1.581, 0.813], mean_best_reward: --\n",
      " 23415/100000: episode: 681, duration: 0.092s, episode steps: 20, steps per second: 217, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-0.359, 0.970], mean_best_reward: --\n",
      " 23463/100000: episode: 682, duration: 0.216s, episode steps: 48, steps per second: 223, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.073 [-0.951, 1.642], mean_best_reward: --\n",
      " 23492/100000: episode: 683, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.035 [-0.947, 1.585], mean_best_reward: --\n",
      " 23519/100000: episode: 684, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.082 [-0.949, 1.501], mean_best_reward: --\n",
      " 23565/100000: episode: 685, duration: 0.211s, episode steps: 46, steps per second: 218, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.056, 0.377], mean_best_reward: --\n",
      " 23641/100000: episode: 686, duration: 0.344s, episode steps: 76, steps per second: 221, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.232 [-1.174, 0.907], mean_best_reward: --\n",
      " 23668/100000: episode: 687, duration: 0.123s, episode steps: 27, steps per second: 220, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.033 [-1.801, 1.019], mean_best_reward: --\n",
      " 23688/100000: episode: 688, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-0.953, 0.445], mean_best_reward: --\n",
      " 23702/100000: episode: 689, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.117 [-1.279, 0.612], mean_best_reward: --\n",
      " 23770/100000: episode: 690, duration: 0.330s, episode steps: 68, steps per second: 206, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.454, 0.813], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23817/100000: episode: 691, duration: 0.232s, episode steps: 47, steps per second: 203, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.066 [-0.783, 1.055], mean_best_reward: --\n",
      " 23869/100000: episode: 692, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.473, 1.210], mean_best_reward: --\n",
      " 23949/100000: episode: 693, duration: 0.396s, episode steps: 80, steps per second: 202, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.478, 0.774], mean_best_reward: --\n",
      " 23961/100000: episode: 694, duration: 0.057s, episode steps: 12, steps per second: 210, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.103 [-1.423, 2.246], mean_best_reward: --\n",
      " 23990/100000: episode: 695, duration: 0.137s, episode steps: 29, steps per second: 212, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.129 [-0.538, 0.808], mean_best_reward: --\n",
      " 24033/100000: episode: 696, duration: 0.211s, episode steps: 43, steps per second: 204, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.156 [-0.824, 0.450], mean_best_reward: --\n",
      " 24063/100000: episode: 697, duration: 0.136s, episode steps: 30, steps per second: 220, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.610, 1.047], mean_best_reward: --\n",
      " 24103/100000: episode: 698, duration: 0.182s, episode steps: 40, steps per second: 220, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.121 [-1.003, 0.630], mean_best_reward: --\n",
      " 24166/100000: episode: 699, duration: 0.297s, episode steps: 63, steps per second: 212, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.064 [-0.937, 1.485], mean_best_reward: --\n",
      " 24199/100000: episode: 700, duration: 0.153s, episode steps: 33, steps per second: 216, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.035 [-1.336, 0.988], mean_best_reward: --\n",
      " 24221/100000: episode: 701, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.209, 0.598], mean_best_reward: 104.000000\n",
      " 24235/100000: episode: 702, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.773, 1.326], mean_best_reward: --\n",
      " 24247/100000: episode: 703, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.128 [-1.366, 0.748], mean_best_reward: --\n",
      " 24288/100000: episode: 704, duration: 0.187s, episode steps: 41, steps per second: 219, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.089 [-1.376, 0.541], mean_best_reward: --\n",
      " 24306/100000: episode: 705, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.567, 1.061], mean_best_reward: --\n",
      " 24332/100000: episode: 706, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.772, 1.165], mean_best_reward: --\n",
      " 24368/100000: episode: 707, duration: 0.165s, episode steps: 36, steps per second: 219, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.111 [-0.619, 1.082], mean_best_reward: --\n",
      " 24422/100000: episode: 708, duration: 0.286s, episode steps: 54, steps per second: 189, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.317, 0.776], mean_best_reward: --\n",
      " 24520/100000: episode: 709, duration: 0.446s, episode steps: 98, steps per second: 220, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-1.735, 1.173], mean_best_reward: --\n",
      " 24533/100000: episode: 710, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.101 [-1.031, 1.688], mean_best_reward: --\n",
      " 24576/100000: episode: 711, duration: 0.199s, episode steps: 43, steps per second: 216, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.117 [-0.928, 0.542], mean_best_reward: --\n",
      " 24605/100000: episode: 712, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.056 [-1.139, 0.773], mean_best_reward: --\n",
      " 24631/100000: episode: 713, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.081 [-0.971, 1.868], mean_best_reward: --\n",
      " 24663/100000: episode: 714, duration: 0.147s, episode steps: 32, steps per second: 218, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.126 [-1.460, 1.007], mean_best_reward: --\n",
      " 24702/100000: episode: 715, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.099 [-1.238, 0.577], mean_best_reward: --\n",
      " 24756/100000: episode: 716, duration: 0.244s, episode steps: 54, steps per second: 221, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.189 [-0.745, 1.189], mean_best_reward: --\n",
      " 24784/100000: episode: 717, duration: 0.131s, episode steps: 28, steps per second: 214, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.159, 0.611], mean_best_reward: --\n",
      " 24823/100000: episode: 718, duration: 0.185s, episode steps: 39, steps per second: 211, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.039 [-1.530, 0.958], mean_best_reward: --\n",
      " 24862/100000: episode: 719, duration: 0.186s, episode steps: 39, steps per second: 210, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.107 [-0.724, 1.119], mean_best_reward: --\n",
      " 24888/100000: episode: 720, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.075 [-0.954, 1.478], mean_best_reward: --\n",
      " 24924/100000: episode: 721, duration: 0.234s, episode steps: 36, steps per second: 154, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.062 [-1.451, 0.612], mean_best_reward: --\n",
      " 24945/100000: episode: 722, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.098 [-0.838, 0.441], mean_best_reward: --\n",
      " 24995/100000: episode: 723, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.181 [-1.244, 0.588], mean_best_reward: --\n",
      " 25029/100000: episode: 724, duration: 0.253s, episode steps: 34, steps per second: 134, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.138 [-1.135, 0.550], mean_best_reward: --\n",
      " 25056/100000: episode: 725, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.093 [-0.928, 0.607], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25083/100000: episode: 726, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.092 [-0.758, 1.675], mean_best_reward: --\n",
      " 25109/100000: episode: 727, duration: 0.127s, episode steps: 26, steps per second: 206, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.088 [-0.965, 0.396], mean_best_reward: --\n",
      " 25155/100000: episode: 728, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.112 [-0.654, 1.172], mean_best_reward: --\n",
      " 25189/100000: episode: 729, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.391, 1.000], mean_best_reward: --\n",
      " 25217/100000: episode: 730, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.062 [-0.461, 1.117], mean_best_reward: --\n",
      " 25243/100000: episode: 731, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.101 [-1.021, 0.623], mean_best_reward: --\n",
      " 25294/100000: episode: 732, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.217 [-0.873, 1.845], mean_best_reward: --\n",
      " 25324/100000: episode: 733, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.169, 0.564], mean_best_reward: --\n",
      " 25353/100000: episode: 734, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.075 [-1.068, 0.550], mean_best_reward: --\n",
      " 25376/100000: episode: 735, duration: 0.158s, episode steps: 23, steps per second: 146, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.056 [-1.950, 1.169], mean_best_reward: --\n",
      " 25410/100000: episode: 736, duration: 0.213s, episode steps: 34, steps per second: 160, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.093 [-1.502, 0.439], mean_best_reward: --\n",
      " 25448/100000: episode: 737, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.081 [-1.005, 0.598], mean_best_reward: --\n",
      " 25502/100000: episode: 738, duration: 0.272s, episode steps: 54, steps per second: 199, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.290, 0.388], mean_best_reward: --\n",
      " 25589/100000: episode: 739, duration: 0.485s, episode steps: 87, steps per second: 180, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.161 [-1.053, 1.381], mean_best_reward: --\n",
      " 25606/100000: episode: 740, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.106 [-1.640, 0.935], mean_best_reward: --\n",
      " 25622/100000: episode: 741, duration: 0.112s, episode steps: 16, steps per second: 143, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.102 [-1.318, 0.787], mean_best_reward: --\n",
      " 25674/100000: episode: 742, duration: 0.254s, episode steps: 52, steps per second: 205, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.499, 1.116], mean_best_reward: --\n",
      " 25707/100000: episode: 743, duration: 0.197s, episode steps: 33, steps per second: 168, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.089 [-1.204, 0.818], mean_best_reward: --\n",
      " 25721/100000: episode: 744, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.089 [-1.605, 0.997], mean_best_reward: --\n",
      " 25800/100000: episode: 745, duration: 0.436s, episode steps: 79, steps per second: 181, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.296 [-2.013, 1.124], mean_best_reward: --\n",
      " 25812/100000: episode: 746, duration: 0.091s, episode steps: 12, steps per second: 133, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.139 [-0.766, 1.561], mean_best_reward: --\n",
      " 25898/100000: episode: 747, duration: 0.465s, episode steps: 86, steps per second: 185, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.107 [-0.681, 0.875], mean_best_reward: --\n",
      " 25949/100000: episode: 748, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.047 [-1.195, 0.731], mean_best_reward: --\n",
      " 25988/100000: episode: 749, duration: 0.193s, episode steps: 39, steps per second: 202, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.001 [-1.425, 0.996], mean_best_reward: --\n",
      " 26030/100000: episode: 750, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.073, 0.808], mean_best_reward: --\n",
      " 26051/100000: episode: 751, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.074 [-0.965, 1.871], mean_best_reward: 100.500000\n",
      " 26063/100000: episode: 752, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.281, 0.761], mean_best_reward: --\n",
      " 26082/100000: episode: 753, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.988, 0.629], mean_best_reward: --\n",
      " 26145/100000: episode: 754, duration: 0.313s, episode steps: 63, steps per second: 201, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.023 [-1.313, 0.612], mean_best_reward: --\n",
      " 26167/100000: episode: 755, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.048 [-1.880, 1.224], mean_best_reward: --\n",
      " 26217/100000: episode: 756, duration: 0.281s, episode steps: 50, steps per second: 178, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.311, 1.135], mean_best_reward: --\n",
      " 26238/100000: episode: 757, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.054 [-1.474, 0.998], mean_best_reward: --\n",
      " 26302/100000: episode: 758, duration: 0.317s, episode steps: 64, steps per second: 202, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.088 [-1.101, 0.737], mean_best_reward: --\n",
      " 26332/100000: episode: 759, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.123, 0.550], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26370/100000: episode: 760, duration: 0.219s, episode steps: 38, steps per second: 174, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.006, 0.563], mean_best_reward: --\n",
      " 26415/100000: episode: 761, duration: 0.209s, episode steps: 45, steps per second: 216, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: 0.032 [-1.749, 2.489], mean_best_reward: --\n",
      " 26435/100000: episode: 762, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.083 [-1.387, 0.772], mean_best_reward: --\n",
      " 26475/100000: episode: 763, duration: 0.257s, episode steps: 40, steps per second: 155, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.027 [-1.200, 1.529], mean_best_reward: --\n",
      " 26490/100000: episode: 764, duration: 0.071s, episode steps: 15, steps per second: 212, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.084 [-1.337, 2.171], mean_best_reward: --\n",
      " 26513/100000: episode: 765, duration: 0.105s, episode steps: 23, steps per second: 219, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.083 [-1.157, 0.572], mean_best_reward: --\n",
      " 26555/100000: episode: 766, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.069 [-0.737, 1.246], mean_best_reward: --\n",
      " 26585/100000: episode: 767, duration: 0.136s, episode steps: 30, steps per second: 221, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.427, 0.611], mean_best_reward: --\n",
      " 26613/100000: episode: 768, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.054 [-1.642, 0.850], mean_best_reward: --\n",
      " 26656/100000: episode: 769, duration: 0.200s, episode steps: 43, steps per second: 215, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.011 [-1.528, 1.170], mean_best_reward: --\n",
      " 26713/100000: episode: 770, duration: 0.265s, episode steps: 57, steps per second: 215, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.110 [-1.099, 0.656], mean_best_reward: --\n",
      " 26737/100000: episode: 771, duration: 0.110s, episode steps: 24, steps per second: 218, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.192, 0.869], mean_best_reward: --\n",
      " 26783/100000: episode: 772, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.767, 1.217], mean_best_reward: --\n",
      " 26792/100000: episode: 773, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.133 [-2.116, 1.357], mean_best_reward: --\n",
      " 26859/100000: episode: 774, duration: 0.304s, episode steps: 67, steps per second: 220, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.157 [-0.820, 1.401], mean_best_reward: --\n",
      " 26899/100000: episode: 775, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.055 [-0.913, 0.845], mean_best_reward: --\n",
      " 26917/100000: episode: 776, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.296, 0.785], mean_best_reward: --\n",
      " 26940/100000: episode: 777, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.058 [-0.999, 1.729], mean_best_reward: --\n",
      " 26983/100000: episode: 778, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.132 [-0.951, 0.431], mean_best_reward: --\n",
      " 27010/100000: episode: 779, duration: 0.125s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.131 [-0.776, 1.280], mean_best_reward: --\n",
      " 27035/100000: episode: 780, duration: 0.119s, episode steps: 25, steps per second: 211, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.074 [-1.458, 0.748], mean_best_reward: --\n",
      " 27100/100000: episode: 781, duration: 0.368s, episode steps: 65, steps per second: 176, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.013 [-1.271, 0.813], mean_best_reward: --\n",
      " 27164/100000: episode: 782, duration: 0.335s, episode steps: 64, steps per second: 191, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.024 [-1.593, 1.163], mean_best_reward: --\n",
      " 27202/100000: episode: 783, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.000 [-1.295, 0.925], mean_best_reward: --\n",
      " 27266/100000: episode: 784, duration: 0.319s, episode steps: 64, steps per second: 201, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.147, 0.812], mean_best_reward: --\n",
      " 27303/100000: episode: 785, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.038 [-1.452, 0.941], mean_best_reward: --\n",
      " 27317/100000: episode: 786, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.116 [-1.759, 0.968], mean_best_reward: --\n",
      " 27338/100000: episode: 787, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.108 [-1.341, 0.576], mean_best_reward: --\n",
      " 27369/100000: episode: 788, duration: 0.229s, episode steps: 31, steps per second: 135, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.115 [-1.123, 0.558], mean_best_reward: --\n",
      " 27442/100000: episode: 789, duration: 0.379s, episode steps: 73, steps per second: 193, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.103 [-1.189, 1.111], mean_best_reward: --\n",
      " 27502/100000: episode: 790, duration: 0.295s, episode steps: 60, steps per second: 203, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.168, 1.115], mean_best_reward: --\n",
      " 27524/100000: episode: 791, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.585, 0.983], mean_best_reward: --\n",
      " 27561/100000: episode: 792, duration: 0.221s, episode steps: 37, steps per second: 168, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.040 [-1.354, 0.579], mean_best_reward: --\n",
      " 27614/100000: episode: 793, duration: 0.308s, episode steps: 53, steps per second: 172, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.061 [-0.553, 1.026], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27680/100000: episode: 794, duration: 0.301s, episode steps: 66, steps per second: 219, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.168 [-0.805, 1.115], mean_best_reward: --\n",
      " 27708/100000: episode: 795, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.584, 1.168], mean_best_reward: --\n",
      " 27783/100000: episode: 796, duration: 0.415s, episode steps: 75, steps per second: 181, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.142 [-1.420, 1.165], mean_best_reward: --\n",
      " 27800/100000: episode: 797, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.089 [-0.807, 1.218], mean_best_reward: --\n",
      " 27829/100000: episode: 798, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.045 [-0.612, 1.097], mean_best_reward: --\n",
      " 27870/100000: episode: 799, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.085 [-0.599, 0.994], mean_best_reward: --\n",
      " 27954/100000: episode: 800, duration: 0.430s, episode steps: 84, steps per second: 195, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.077 [-1.357, 1.120], mean_best_reward: --\n",
      " 28002/100000: episode: 801, duration: 0.221s, episode steps: 48, steps per second: 217, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.884, 0.580], mean_best_reward: 87.500000\n",
      " 28020/100000: episode: 802, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.109, 0.378], mean_best_reward: --\n",
      " 28059/100000: episode: 803, duration: 0.231s, episode steps: 39, steps per second: 169, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.100 [-0.529, 0.988], mean_best_reward: --\n",
      " 28079/100000: episode: 804, duration: 0.129s, episode steps: 20, steps per second: 155, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-0.833, 1.644], mean_best_reward: --\n",
      " 28099/100000: episode: 805, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.202, 0.749], mean_best_reward: --\n",
      " 28165/100000: episode: 806, duration: 0.371s, episode steps: 66, steps per second: 178, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.223 [-0.870, 1.337], mean_best_reward: --\n",
      " 28255/100000: episode: 807, duration: 0.495s, episode steps: 90, steps per second: 182, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.122 [-1.529, 1.003], mean_best_reward: --\n",
      " 28282/100000: episode: 808, duration: 0.126s, episode steps: 27, steps per second: 214, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.068 [-1.332, 0.803], mean_best_reward: --\n",
      " 28311/100000: episode: 809, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.065 [-1.132, 1.963], mean_best_reward: --\n",
      " 28354/100000: episode: 810, duration: 0.256s, episode steps: 43, steps per second: 168, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.039 [-1.547, 0.652], mean_best_reward: --\n",
      " 28380/100000: episode: 811, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.147, 0.765], mean_best_reward: --\n",
      " 28452/100000: episode: 812, duration: 0.341s, episode steps: 72, steps per second: 211, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-1.051, 1.433], mean_best_reward: --\n",
      " 28501/100000: episode: 813, duration: 0.249s, episode steps: 49, steps per second: 197, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.172 [-0.583, 1.122], mean_best_reward: --\n",
      " 28545/100000: episode: 814, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.129 [-0.642, 0.707], mean_best_reward: --\n",
      " 28574/100000: episode: 815, duration: 0.186s, episode steps: 29, steps per second: 156, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.066 [-1.012, 0.576], mean_best_reward: --\n",
      " 28630/100000: episode: 816, duration: 0.277s, episode steps: 56, steps per second: 202, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.111 [-0.605, 1.212], mean_best_reward: --\n",
      " 28672/100000: episode: 817, duration: 0.193s, episode steps: 42, steps per second: 217, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.118 [-0.581, 1.202], mean_best_reward: --\n",
      " 28717/100000: episode: 818, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.094 [-1.279, 0.558], mean_best_reward: --\n",
      " 28773/100000: episode: 819, duration: 0.272s, episode steps: 56, steps per second: 206, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.080 [-0.611, 1.089], mean_best_reward: --\n",
      " 28813/100000: episode: 820, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.147 [-1.220, 0.822], mean_best_reward: --\n",
      " 28846/100000: episode: 821, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.140 [-0.359, 0.894], mean_best_reward: --\n",
      " 28919/100000: episode: 822, duration: 0.412s, episode steps: 73, steps per second: 177, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.239 [-0.751, 1.504], mean_best_reward: --\n",
      " 28963/100000: episode: 823, duration: 0.203s, episode steps: 44, steps per second: 216, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.109 [-0.934, 0.607], mean_best_reward: --\n",
      " 29012/100000: episode: 824, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.109 [-0.640, 0.997], mean_best_reward: --\n",
      " 29033/100000: episode: 825, duration: 0.139s, episode steps: 21, steps per second: 151, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.104 [-1.056, 0.399], mean_best_reward: --\n",
      " 29051/100000: episode: 826, duration: 0.122s, episode steps: 18, steps per second: 147, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.241, 0.824], mean_best_reward: --\n",
      " 29138/100000: episode: 827, duration: 0.463s, episode steps: 87, steps per second: 188, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.087 [-1.386, 0.724], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29190/100000: episode: 828, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.104 [-1.127, 0.526], mean_best_reward: --\n",
      " 29223/100000: episode: 829, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.093 [-0.927, 0.428], mean_best_reward: --\n",
      " 29247/100000: episode: 830, duration: 0.166s, episode steps: 24, steps per second: 144, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.128 [-1.039, 0.567], mean_best_reward: --\n",
      " 29256/100000: episode: 831, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.125 [-1.410, 2.178], mean_best_reward: --\n",
      " 29284/100000: episode: 832, duration: 0.162s, episode steps: 28, steps per second: 172, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.109 [-0.384, 1.083], mean_best_reward: --\n",
      " 29319/100000: episode: 833, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.133 [-0.248, 1.006], mean_best_reward: --\n",
      " 29340/100000: episode: 834, duration: 0.155s, episode steps: 21, steps per second: 135, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.109 [-1.185, 0.368], mean_best_reward: --\n",
      " 29405/100000: episode: 835, duration: 0.306s, episode steps: 65, steps per second: 212, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.063 [-0.822, 0.956], mean_best_reward: --\n",
      " 29454/100000: episode: 836, duration: 0.232s, episode steps: 49, steps per second: 211, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.111 [-1.802, 1.095], mean_best_reward: --\n",
      " 29471/100000: episode: 837, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.073 [-1.631, 1.014], mean_best_reward: --\n",
      " 29486/100000: episode: 838, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.650, 1.291], mean_best_reward: --\n",
      " 29544/100000: episode: 839, duration: 0.292s, episode steps: 58, steps per second: 199, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.184 [-0.620, 2.006], mean_best_reward: --\n",
      " 29597/100000: episode: 840, duration: 0.304s, episode steps: 53, steps per second: 174, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.089 [-1.132, 0.570], mean_best_reward: --\n",
      " 29627/100000: episode: 841, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.292, 0.612], mean_best_reward: --\n",
      " 29650/100000: episode: 842, duration: 0.146s, episode steps: 23, steps per second: 158, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.063 [-0.798, 1.294], mean_best_reward: --\n",
      " 29688/100000: episode: 843, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.151 [-0.744, 1.165], mean_best_reward: --\n",
      " 29701/100000: episode: 844, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.093 [-0.993, 1.553], mean_best_reward: --\n",
      " 29764/100000: episode: 845, duration: 0.322s, episode steps: 63, steps per second: 196, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.082 [-0.702, 0.962], mean_best_reward: --\n",
      " 29805/100000: episode: 846, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.081 [-0.781, 1.014], mean_best_reward: --\n",
      " 29886/100000: episode: 847, duration: 0.414s, episode steps: 81, steps per second: 196, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.061 [-0.892, 0.736], mean_best_reward: --\n",
      " 29975/100000: episode: 848, duration: 0.437s, episode steps: 89, steps per second: 204, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.068 [-0.830, 1.240], mean_best_reward: --\n",
      " 30030/100000: episode: 849, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.078 [-1.669, 0.615], mean_best_reward: --\n",
      " 30064/100000: episode: 850, duration: 0.155s, episode steps: 34, steps per second: 219, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.190, 0.447], mean_best_reward: --\n",
      " 30085/100000: episode: 851, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.103 [-0.776, 1.240], mean_best_reward: 76.500000\n",
      " 30129/100000: episode: 852, duration: 0.213s, episode steps: 44, steps per second: 207, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.120 [-0.553, 1.085], mean_best_reward: --\n",
      " 30211/100000: episode: 853, duration: 0.388s, episode steps: 82, steps per second: 211, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.113 [-0.994, 0.557], mean_best_reward: --\n",
      " 30223/100000: episode: 854, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.606, 1.015], mean_best_reward: --\n",
      " 30233/100000: episode: 855, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.119 [-1.182, 1.796], mean_best_reward: --\n",
      " 30268/100000: episode: 856, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.131 [-0.841, 1.265], mean_best_reward: --\n",
      " 30286/100000: episode: 857, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.812, 1.306], mean_best_reward: --\n",
      " 30323/100000: episode: 858, duration: 0.228s, episode steps: 37, steps per second: 162, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.086 [-0.626, 1.514], mean_best_reward: --\n",
      " 30348/100000: episode: 859, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.119 [-0.942, 0.377], mean_best_reward: --\n",
      " 30359/100000: episode: 860, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.099 [-0.973, 1.694], mean_best_reward: --\n",
      " 30375/100000: episode: 861, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.615, 1.130], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30434/100000: episode: 862, duration: 0.323s, episode steps: 59, steps per second: 183, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.030 [-1.185, 0.800], mean_best_reward: --\n",
      " 30500/100000: episode: 863, duration: 0.380s, episode steps: 66, steps per second: 174, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.183 [-1.166, 1.030], mean_best_reward: --\n",
      " 30550/100000: episode: 864, duration: 0.232s, episode steps: 50, steps per second: 215, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.049 [-1.117, 1.111], mean_best_reward: --\n",
      " 30567/100000: episode: 865, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.106 [-1.822, 0.974], mean_best_reward: --\n",
      " 30582/100000: episode: 866, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.089 [-1.431, 0.970], mean_best_reward: --\n",
      " 30611/100000: episode: 867, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.121 [-0.406, 1.334], mean_best_reward: --\n",
      " 30645/100000: episode: 868, duration: 0.208s, episode steps: 34, steps per second: 164, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.110 [-1.004, 0.594], mean_best_reward: --\n",
      " 30685/100000: episode: 869, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.033 [-1.389, 0.754], mean_best_reward: --\n",
      " 30716/100000: episode: 870, duration: 0.146s, episode steps: 31, steps per second: 212, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.109 [-0.379, 1.259], mean_best_reward: --\n",
      " 30761/100000: episode: 871, duration: 0.277s, episode steps: 45, steps per second: 162, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.132 [-0.653, 0.953], mean_best_reward: --\n",
      " 30812/100000: episode: 872, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.073 [-1.324, 0.378], mean_best_reward: --\n",
      " 30845/100000: episode: 873, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.135 [-1.212, 0.576], mean_best_reward: --\n",
      " 30890/100000: episode: 874, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.069 [-0.889, 1.389], mean_best_reward: --\n",
      " 30953/100000: episode: 875, duration: 0.340s, episode steps: 63, steps per second: 185, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.196 [-1.112, 0.583], mean_best_reward: --\n",
      " 30986/100000: episode: 876, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.007 [-1.691, 1.032], mean_best_reward: --\n",
      " 31024/100000: episode: 877, duration: 0.242s, episode steps: 38, steps per second: 157, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-1.655, 0.706], mean_best_reward: --\n",
      " 31075/100000: episode: 878, duration: 0.317s, episode steps: 51, steps per second: 161, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.090 [-0.926, 1.064], mean_best_reward: --\n",
      " 31123/100000: episode: 879, duration: 0.227s, episode steps: 48, steps per second: 211, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.027 [-1.185, 0.817], mean_best_reward: --\n",
      " 31134/100000: episode: 880, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.127 [-0.799, 1.466], mean_best_reward: --\n",
      " 31149/100000: episode: 881, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-0.976, 1.762], mean_best_reward: --\n",
      " 31166/100000: episode: 882, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.075 [-1.916, 1.137], mean_best_reward: --\n",
      " 31214/100000: episode: 883, duration: 0.235s, episode steps: 48, steps per second: 204, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.079 [-0.567, 1.249], mean_best_reward: --\n",
      " 31241/100000: episode: 884, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.043 [-0.900, 1.182], mean_best_reward: --\n",
      " 31298/100000: episode: 885, duration: 0.303s, episode steps: 57, steps per second: 188, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.044 [-0.626, 1.203], mean_best_reward: --\n",
      " 31367/100000: episode: 886, duration: 0.376s, episode steps: 69, steps per second: 184, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.099 [-1.235, 0.910], mean_best_reward: --\n",
      " 31429/100000: episode: 887, duration: 0.338s, episode steps: 62, steps per second: 183, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.791, 1.081], mean_best_reward: --\n",
      " 31469/100000: episode: 888, duration: 0.244s, episode steps: 40, steps per second: 164, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.191 [-1.103, 0.380], mean_best_reward: --\n",
      " 31530/100000: episode: 889, duration: 0.374s, episode steps: 61, steps per second: 163, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.169 [-1.374, 0.611], mean_best_reward: --\n",
      " 31581/100000: episode: 890, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.175 [-0.602, 1.206], mean_best_reward: --\n",
      " 31629/100000: episode: 891, duration: 0.302s, episode steps: 48, steps per second: 159, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.869, 1.069], mean_best_reward: --\n",
      " 31661/100000: episode: 892, duration: 0.152s, episode steps: 32, steps per second: 210, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.118 [-1.143, 0.748], mean_best_reward: --\n",
      " 31677/100000: episode: 893, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.070 [-0.782, 1.283], mean_best_reward: --\n",
      " 31718/100000: episode: 894, duration: 0.192s, episode steps: 41, steps per second: 214, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.077 [-0.613, 0.997], mean_best_reward: --\n",
      " 31733/100000: episode: 895, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.074 [-1.507, 0.991], mean_best_reward: --\n",
      " 31757/100000: episode: 896, duration: 0.110s, episode steps: 24, steps per second: 219, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.120 [-0.390, 0.891], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31786/100000: episode: 897, duration: 0.132s, episode steps: 29, steps per second: 220, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.070 [-1.216, 0.600], mean_best_reward: --\n",
      " 31856/100000: episode: 898, duration: 0.316s, episode steps: 70, steps per second: 221, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.069 [-0.883, 1.196], mean_best_reward: --\n",
      " 31888/100000: episode: 899, duration: 0.149s, episode steps: 32, steps per second: 214, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.096 [-1.938, 0.945], mean_best_reward: --\n",
      " 31929/100000: episode: 900, duration: 0.193s, episode steps: 41, steps per second: 212, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.022 [-0.767, 1.314], mean_best_reward: --\n",
      " 31946/100000: episode: 901, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.106 [-1.368, 0.592], mean_best_reward: 101.000000\n",
      " 31979/100000: episode: 902, duration: 0.151s, episode steps: 33, steps per second: 218, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.046 [-1.399, 0.755], mean_best_reward: --\n",
      " 32015/100000: episode: 903, duration: 0.164s, episode steps: 36, steps per second: 219, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.078 [-0.417, 1.039], mean_best_reward: --\n",
      " 32083/100000: episode: 904, duration: 0.316s, episode steps: 68, steps per second: 215, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-1.335, 1.020], mean_best_reward: --\n",
      " 32109/100000: episode: 905, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.486, 0.935], mean_best_reward: --\n",
      " 32129/100000: episode: 906, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.099 [-0.864, 0.410], mean_best_reward: --\n",
      " 32147/100000: episode: 907, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.009, 0.614], mean_best_reward: --\n",
      " 32170/100000: episode: 908, duration: 0.162s, episode steps: 23, steps per second: 142, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.093 [-0.556, 1.197], mean_best_reward: --\n",
      " 32196/100000: episode: 909, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.043, 0.564], mean_best_reward: --\n",
      " 32235/100000: episode: 910, duration: 0.190s, episode steps: 39, steps per second: 205, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.060 [-1.565, 0.796], mean_best_reward: --\n",
      " 32276/100000: episode: 911, duration: 0.198s, episode steps: 41, steps per second: 208, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.074 [-1.026, 0.452], mean_best_reward: --\n",
      " 32330/100000: episode: 912, duration: 0.254s, episode steps: 54, steps per second: 213, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.034 [-1.246, 0.810], mean_best_reward: --\n",
      " 32376/100000: episode: 913, duration: 0.214s, episode steps: 46, steps per second: 215, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.137 [-1.182, 0.580], mean_best_reward: --\n",
      " 32401/100000: episode: 914, duration: 0.115s, episode steps: 25, steps per second: 217, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.052 [-1.008, 1.486], mean_best_reward: --\n",
      " 32454/100000: episode: 915, duration: 0.241s, episode steps: 53, steps per second: 220, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.018 [-1.071, 1.138], mean_best_reward: --\n",
      " 32472/100000: episode: 916, duration: 0.085s, episode steps: 18, steps per second: 213, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-1.138, 0.548], mean_best_reward: --\n",
      " 32524/100000: episode: 917, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.220 [-1.284, 0.815], mean_best_reward: --\n",
      " 32545/100000: episode: 918, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.077 [-0.796, 1.440], mean_best_reward: --\n",
      " 32570/100000: episode: 919, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.056 [-1.872, 1.029], mean_best_reward: --\n",
      " 32588/100000: episode: 920, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.062 [-2.125, 1.369], mean_best_reward: --\n",
      " 32632/100000: episode: 921, duration: 0.203s, episode steps: 44, steps per second: 217, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.024 [-1.139, 1.620], mean_best_reward: --\n",
      " 32650/100000: episode: 922, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.101 [-1.302, 0.629], mean_best_reward: --\n",
      " 32699/100000: episode: 923, duration: 0.228s, episode steps: 49, steps per second: 215, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.156 [-0.947, 1.005], mean_best_reward: --\n",
      " 32730/100000: episode: 924, duration: 0.145s, episode steps: 31, steps per second: 214, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.050 [-1.389, 0.604], mean_best_reward: --\n",
      " 32775/100000: episode: 925, duration: 0.213s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.080 [-0.612, 1.304], mean_best_reward: --\n",
      " 32790/100000: episode: 926, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.085 [-1.536, 0.986], mean_best_reward: --\n",
      " 32863/100000: episode: 927, duration: 0.335s, episode steps: 73, steps per second: 218, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.099 [-0.839, 1.322], mean_best_reward: --\n",
      " 32893/100000: episode: 928, duration: 0.138s, episode steps: 30, steps per second: 217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.106 [-2.039, 0.852], mean_best_reward: --\n",
      " 32919/100000: episode: 929, duration: 0.122s, episode steps: 26, steps per second: 213, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.005 [-1.585, 2.180], mean_best_reward: --\n",
      " 32942/100000: episode: 930, duration: 0.106s, episode steps: 23, steps per second: 217, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.102 [-1.506, 0.611], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32975/100000: episode: 931, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.071 [-1.057, 0.782], mean_best_reward: --\n",
      " 32991/100000: episode: 932, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.118 [-1.784, 0.943], mean_best_reward: --\n",
      " 33036/100000: episode: 933, duration: 0.213s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.099 [-1.185, 0.828], mean_best_reward: --\n",
      " 33065/100000: episode: 934, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.095 [-0.381, 0.840], mean_best_reward: --\n",
      " 33086/100000: episode: 935, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.066 [-0.978, 0.609], mean_best_reward: --\n",
      " 33111/100000: episode: 936, duration: 0.116s, episode steps: 25, steps per second: 215, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.084 [-1.872, 1.001], mean_best_reward: --\n",
      " 33125/100000: episode: 937, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.056 [-2.001, 1.414], mean_best_reward: --\n",
      " 33184/100000: episode: 938, duration: 0.276s, episode steps: 59, steps per second: 214, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.006 [-0.851, 0.425], mean_best_reward: --\n",
      " 33225/100000: episode: 939, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.070 [-0.587, 1.119], mean_best_reward: --\n",
      " 33250/100000: episode: 940, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.027 [-1.637, 0.986], mean_best_reward: --\n",
      " 33286/100000: episode: 941, duration: 0.233s, episode steps: 36, steps per second: 154, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.158 [-0.364, 0.962], mean_best_reward: --\n",
      " 33306/100000: episode: 942, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.056 [-1.537, 0.821], mean_best_reward: --\n",
      " 33323/100000: episode: 943, duration: 0.143s, episode steps: 17, steps per second: 119, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.079 [-1.354, 2.195], mean_best_reward: --\n",
      " 33348/100000: episode: 944, duration: 0.164s, episode steps: 25, steps per second: 153, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.425, 0.907], mean_best_reward: --\n",
      " 33375/100000: episode: 945, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.094 [-0.753, 1.314], mean_best_reward: --\n",
      " 33441/100000: episode: 946, duration: 0.395s, episode steps: 66, steps per second: 167, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.186 [-1.250, 0.774], mean_best_reward: --\n",
      " 33490/100000: episode: 947, duration: 0.305s, episode steps: 49, steps per second: 161, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.015 [-0.901, 0.613], mean_best_reward: --\n",
      " 33558/100000: episode: 948, duration: 0.460s, episode steps: 68, steps per second: 148, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.035 [-0.842, 1.487], mean_best_reward: --\n",
      " 33603/100000: episode: 949, duration: 0.550s, episode steps: 45, steps per second: 82, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.093 [-1.167, 0.459], mean_best_reward: --\n",
      " 33646/100000: episode: 950, duration: 0.466s, episode steps: 43, steps per second: 92, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.105 [-1.397, 0.695], mean_best_reward: --\n",
      " 33722/100000: episode: 951, duration: 0.588s, episode steps: 76, steps per second: 129, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.041 [-2.351, 0.917], mean_best_reward: 80.500000\n",
      " 33755/100000: episode: 952, duration: 0.372s, episode steps: 33, steps per second: 89, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.093 [-0.546, 0.984], mean_best_reward: --\n",
      " 33780/100000: episode: 953, duration: 0.309s, episode steps: 25, steps per second: 81, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.084 [-1.837, 1.010], mean_best_reward: --\n",
      " 33837/100000: episode: 954, duration: 0.345s, episode steps: 57, steps per second: 165, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.102 [-0.620, 0.997], mean_best_reward: --\n",
      " 33886/100000: episode: 955, duration: 0.350s, episode steps: 49, steps per second: 140, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.054 [-0.872, 0.584], mean_best_reward: --\n",
      " 33901/100000: episode: 956, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.109 [-1.343, 0.585], mean_best_reward: --\n",
      " 33924/100000: episode: 957, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.091 [-0.635, 1.377], mean_best_reward: --\n",
      " 33982/100000: episode: 958, duration: 0.288s, episode steps: 58, steps per second: 201, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.156 [-1.347, 0.599], mean_best_reward: --\n",
      " 34020/100000: episode: 959, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.058 [-0.666, 1.581], mean_best_reward: --\n",
      " 34033/100000: episode: 960, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.110 [-0.751, 1.376], mean_best_reward: --\n",
      " 34053/100000: episode: 961, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.100 [-0.449, 1.217], mean_best_reward: --\n",
      " 34081/100000: episode: 962, duration: 0.186s, episode steps: 28, steps per second: 151, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-1.367, 1.009], mean_best_reward: --\n",
      " 34119/100000: episode: 963, duration: 0.219s, episode steps: 38, steps per second: 174, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.127 [-0.796, 0.525], mean_best_reward: --\n",
      " 34173/100000: episode: 964, duration: 0.309s, episode steps: 54, steps per second: 175, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.240 [-0.999, 1.687], mean_best_reward: --\n",
      " 34198/100000: episode: 965, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.099 [-0.574, 0.910], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34281/100000: episode: 966, duration: 0.401s, episode steps: 83, steps per second: 207, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.022 [-1.379, 0.978], mean_best_reward: --\n",
      " 34338/100000: episode: 967, duration: 0.306s, episode steps: 57, steps per second: 186, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.232 [-1.398, 0.575], mean_best_reward: --\n",
      " 34364/100000: episode: 968, duration: 0.153s, episode steps: 26, steps per second: 169, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.782, 1.258], mean_best_reward: --\n",
      " 34380/100000: episode: 969, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.075 [-1.875, 1.216], mean_best_reward: --\n",
      " 34486/100000: episode: 970, duration: 0.516s, episode steps: 106, steps per second: 205, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.098 [-0.775, 0.845], mean_best_reward: --\n",
      " 34525/100000: episode: 971, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.023 [-0.610, 1.090], mean_best_reward: --\n",
      " 34544/100000: episode: 972, duration: 0.172s, episode steps: 19, steps per second: 110, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.097 [-1.122, 0.557], mean_best_reward: --\n",
      " 34639/100000: episode: 973, duration: 0.583s, episode steps: 95, steps per second: 163, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.126 [-1.265, 0.591], mean_best_reward: --\n",
      " 34723/100000: episode: 974, duration: 0.473s, episode steps: 84, steps per second: 178, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-1.173, 1.453], mean_best_reward: --\n",
      " 34740/100000: episode: 975, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.108 [-1.217, 0.746], mean_best_reward: --\n",
      " 34783/100000: episode: 976, duration: 0.374s, episode steps: 43, steps per second: 115, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.087 [-1.674, 0.649], mean_best_reward: --\n",
      " 34869/100000: episode: 977, duration: 0.661s, episode steps: 86, steps per second: 130, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.107 [-0.656, 1.222], mean_best_reward: --\n",
      " 34925/100000: episode: 978, duration: 0.295s, episode steps: 56, steps per second: 190, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.161 [-1.917, 0.844], mean_best_reward: --\n",
      " 34946/100000: episode: 979, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.078 [-0.822, 1.323], mean_best_reward: --\n",
      " 35001/100000: episode: 980, duration: 0.330s, episode steps: 55, steps per second: 167, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.105 [-1.012, 0.536], mean_best_reward: --\n",
      " 35030/100000: episode: 981, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.141 [-0.537, 1.183], mean_best_reward: --\n",
      " 35098/100000: episode: 982, duration: 0.515s, episode steps: 68, steps per second: 132, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.107 [-0.617, 1.056], mean_best_reward: --\n",
      " 35139/100000: episode: 983, duration: 0.322s, episode steps: 41, steps per second: 127, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.199 [-1.246, 0.740], mean_best_reward: --\n",
      " 35202/100000: episode: 984, duration: 0.301s, episode steps: 63, steps per second: 209, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.120 [-0.654, 0.873], mean_best_reward: --\n",
      " 35216/100000: episode: 985, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.105 [-2.066, 1.176], mean_best_reward: --\n",
      " 35240/100000: episode: 986, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.496, 1.001], mean_best_reward: --\n",
      " 35262/100000: episode: 987, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.242, 0.577], mean_best_reward: --\n",
      " 35310/100000: episode: 988, duration: 0.224s, episode steps: 48, steps per second: 215, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.003 [-0.774, 1.139], mean_best_reward: --\n",
      " 35412/100000: episode: 989, duration: 0.474s, episode steps: 102, steps per second: 215, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.092 [-0.912, 1.075], mean_best_reward: --\n",
      " 35456/100000: episode: 990, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.092 [-0.384, 0.830], mean_best_reward: --\n",
      " 35496/100000: episode: 991, duration: 0.232s, episode steps: 40, steps per second: 172, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.128 [-1.398, 0.654], mean_best_reward: --\n",
      " 35536/100000: episode: 992, duration: 0.244s, episode steps: 40, steps per second: 164, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.110 [-1.215, 0.776], mean_best_reward: --\n",
      " 35585/100000: episode: 993, duration: 0.450s, episode steps: 49, steps per second: 109, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.201 [-0.602, 1.344], mean_best_reward: --\n",
      " 35610/100000: episode: 994, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.085 [-0.596, 1.545], mean_best_reward: --\n",
      " 35627/100000: episode: 995, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.075 [-1.224, 0.616], mean_best_reward: --\n",
      " 35639/100000: episode: 996, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-0.950, 1.711], mean_best_reward: --\n",
      " 35678/100000: episode: 997, duration: 0.226s, episode steps: 39, steps per second: 173, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.171 [-0.454, 0.964], mean_best_reward: --\n",
      " 35698/100000: episode: 998, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.039, 0.579], mean_best_reward: --\n",
      " 35740/100000: episode: 999, duration: 0.257s, episode steps: 42, steps per second: 164, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.184 [-0.489, 1.149], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35788/100000: episode: 1000, duration: 0.229s, episode steps: 48, steps per second: 209, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-1.195, 0.558], mean_best_reward: --\n",
      " 35808/100000: episode: 1001, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.081 [-1.136, 0.627], mean_best_reward: 87.000000\n",
      " 35838/100000: episode: 1002, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.103 [-1.889, 0.822], mean_best_reward: --\n",
      " 35882/100000: episode: 1003, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.048, 0.636], mean_best_reward: --\n",
      " 35956/100000: episode: 1004, duration: 0.373s, episode steps: 74, steps per second: 198, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.108 [-0.539, 1.124], mean_best_reward: --\n",
      " 36003/100000: episode: 1005, duration: 0.217s, episode steps: 47, steps per second: 217, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.207 [-0.861, 1.119], mean_best_reward: --\n",
      " 36050/100000: episode: 1006, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.084 [-1.002, 0.900], mean_best_reward: --\n",
      " 36108/100000: episode: 1007, duration: 0.416s, episode steps: 58, steps per second: 140, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.305, 0.797], mean_best_reward: --\n",
      " 36159/100000: episode: 1008, duration: 0.358s, episode steps: 51, steps per second: 142, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.045 [-0.957, 1.307], mean_best_reward: --\n",
      " 36183/100000: episode: 1009, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-0.862, 0.560], mean_best_reward: --\n",
      " 36244/100000: episode: 1010, duration: 0.331s, episode steps: 61, steps per second: 184, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.088 [-0.985, 1.283], mean_best_reward: --\n",
      " 36263/100000: episode: 1011, duration: 0.147s, episode steps: 19, steps per second: 129, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.081 [-0.752, 1.345], mean_best_reward: --\n",
      " 36277/100000: episode: 1012, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.122 [-0.744, 1.400], mean_best_reward: --\n",
      " 36320/100000: episode: 1013, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.077 [-0.809, 1.240], mean_best_reward: --\n",
      " 36333/100000: episode: 1014, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.106 [-0.935, 1.580], mean_best_reward: --\n",
      " 36356/100000: episode: 1015, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.107 [-1.979, 1.018], mean_best_reward: --\n",
      " 36385/100000: episode: 1016, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.081 [-0.807, 1.207], mean_best_reward: --\n",
      " 36458/100000: episode: 1017, duration: 0.396s, episode steps: 73, steps per second: 185, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.165 [-1.304, 0.851], mean_best_reward: --\n",
      " 36483/100000: episode: 1018, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.075 [-0.635, 0.930], mean_best_reward: --\n",
      " 36579/100000: episode: 1019, duration: 0.560s, episode steps: 96, steps per second: 171, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.101 [-1.146, 1.017], mean_best_reward: --\n",
      " 36598/100000: episode: 1020, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.077 [-1.580, 0.955], mean_best_reward: --\n",
      " 36611/100000: episode: 1021, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.098 [-0.807, 1.479], mean_best_reward: --\n",
      " 36646/100000: episode: 1022, duration: 0.222s, episode steps: 35, steps per second: 157, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.012 [-1.581, 2.115], mean_best_reward: --\n",
      " 36662/100000: episode: 1023, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.103 [-0.783, 1.447], mean_best_reward: --\n",
      " 36684/100000: episode: 1024, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.056, 0.431], mean_best_reward: --\n",
      " 36749/100000: episode: 1025, duration: 0.330s, episode steps: 65, steps per second: 197, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.113 [-0.950, 1.256], mean_best_reward: --\n",
      " 36765/100000: episode: 1026, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.059 [-2.102, 1.381], mean_best_reward: --\n",
      " 36790/100000: episode: 1027, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.320 [0.000, 1.000], mean observation: 0.006 [-1.774, 2.518], mean_best_reward: --\n",
      " 36821/100000: episode: 1028, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.049 [-0.615, 1.057], mean_best_reward: --\n",
      " 36874/100000: episode: 1029, duration: 0.244s, episode steps: 53, steps per second: 217, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.024 [-1.313, 0.802], mean_best_reward: --\n",
      " 36898/100000: episode: 1030, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.063 [-0.849, 1.636], mean_best_reward: --\n",
      " 36916/100000: episode: 1031, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.076 [-0.814, 1.478], mean_best_reward: --\n",
      " 36958/100000: episode: 1032, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.033 [-0.758, 1.154], mean_best_reward: --\n",
      " 37023/100000: episode: 1033, duration: 0.454s, episode steps: 65, steps per second: 143, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.002 [-1.325, 1.348], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37060/100000: episode: 1034, duration: 0.305s, episode steps: 37, steps per second: 121, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.036 [-1.267, 0.649], mean_best_reward: --\n",
      " 37089/100000: episode: 1035, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.063 [-0.597, 1.032], mean_best_reward: --\n",
      " 37141/100000: episode: 1036, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.120 [-0.506, 1.341], mean_best_reward: --\n",
      " 37191/100000: episode: 1037, duration: 0.229s, episode steps: 50, steps per second: 218, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.029 [-0.775, 1.340], mean_best_reward: --\n",
      " 37227/100000: episode: 1038, duration: 0.171s, episode steps: 36, steps per second: 210, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.489, 1.004], mean_best_reward: --\n",
      " 37312/100000: episode: 1039, duration: 0.409s, episode steps: 85, steps per second: 208, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.135 [-1.258, 0.897], mean_best_reward: --\n",
      " 37345/100000: episode: 1040, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.026 [-1.178, 1.855], mean_best_reward: --\n",
      " 37363/100000: episode: 1041, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.073 [-0.819, 1.575], mean_best_reward: --\n",
      " 37388/100000: episode: 1042, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.078 [-0.743, 1.141], mean_best_reward: --\n",
      " 37453/100000: episode: 1043, duration: 0.363s, episode steps: 65, steps per second: 179, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.079 [-1.467, 0.669], mean_best_reward: --\n",
      " 37474/100000: episode: 1044, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.067 [-1.177, 1.886], mean_best_reward: --\n",
      " 37494/100000: episode: 1045, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.102 [-1.194, 0.609], mean_best_reward: --\n",
      " 37566/100000: episode: 1046, duration: 0.404s, episode steps: 72, steps per second: 178, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.192 [-1.232, 1.020], mean_best_reward: --\n",
      " 37622/100000: episode: 1047, duration: 0.286s, episode steps: 56, steps per second: 196, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.116 [-1.282, 0.951], mean_best_reward: --\n",
      " 37640/100000: episode: 1048, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.054 [-0.775, 1.322], mean_best_reward: --\n",
      " 37737/100000: episode: 1049, duration: 0.948s, episode steps: 97, steps per second: 102, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.003 [-0.821, 1.205], mean_best_reward: --\n",
      " 37806/100000: episode: 1050, duration: 0.457s, episode steps: 69, steps per second: 151, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.270 [-1.493, 0.658], mean_best_reward: --\n",
      " 37817/100000: episode: 1051, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.122 [-1.441, 0.779], mean_best_reward: 70.000000\n",
      " 37831/100000: episode: 1052, duration: 0.066s, episode steps: 14, steps per second: 212, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.263, 0.832], mean_best_reward: --\n",
      " 37864/100000: episode: 1053, duration: 0.157s, episode steps: 33, steps per second: 211, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.129 [-0.954, 0.590], mean_best_reward: --\n",
      " 37916/100000: episode: 1054, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-1.172, 0.957], mean_best_reward: --\n",
      " 37938/100000: episode: 1055, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.093 [-1.574, 0.792], mean_best_reward: --\n",
      " 37970/100000: episode: 1056, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.125 [-1.212, 0.556], mean_best_reward: --\n",
      " 38011/100000: episode: 1057, duration: 0.191s, episode steps: 41, steps per second: 214, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: -0.055 [-1.606, 0.939], mean_best_reward: --\n",
      " 38061/100000: episode: 1058, duration: 0.224s, episode steps: 50, steps per second: 223, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.091 [-1.056, 0.585], mean_best_reward: --\n",
      " 38104/100000: episode: 1059, duration: 0.191s, episode steps: 43, steps per second: 225, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.055 [-1.463, 1.186], mean_best_reward: --\n",
      " 38146/100000: episode: 1060, duration: 0.190s, episode steps: 42, steps per second: 221, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.091 [-1.774, 0.645], mean_best_reward: --\n",
      " 38190/100000: episode: 1061, duration: 0.198s, episode steps: 44, steps per second: 222, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.129 [-0.647, 1.685], mean_best_reward: --\n",
      " 38220/100000: episode: 1062, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-0.718, 0.441], mean_best_reward: --\n",
      " 38241/100000: episode: 1063, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.081 [-0.837, 1.653], mean_best_reward: --\n",
      " 38294/100000: episode: 1064, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.137 [-1.118, 0.768], mean_best_reward: --\n",
      " 38354/100000: episode: 1065, duration: 0.352s, episode steps: 60, steps per second: 171, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.046, 0.741], mean_best_reward: --\n",
      " 38420/100000: episode: 1066, duration: 0.332s, episode steps: 66, steps per second: 199, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.054 [-1.437, 1.300], mean_best_reward: --\n",
      " 38457/100000: episode: 1067, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.028 [-1.203, 0.810], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38494/100000: episode: 1068, duration: 0.174s, episode steps: 37, steps per second: 213, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.105 [-0.396, 1.077], mean_best_reward: --\n",
      " 38518/100000: episode: 1069, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.069 [-1.615, 0.825], mean_best_reward: --\n",
      " 38571/100000: episode: 1070, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.054 [-0.970, 1.722], mean_best_reward: --\n",
      " 38592/100000: episode: 1071, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.043 [-1.436, 1.004], mean_best_reward: --\n",
      " 38658/100000: episode: 1072, duration: 0.309s, episode steps: 66, steps per second: 213, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.205, 0.672], mean_best_reward: --\n",
      " 38704/100000: episode: 1073, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.048 [-1.139, 0.591], mean_best_reward: --\n",
      " 38786/100000: episode: 1074, duration: 0.587s, episode steps: 82, steps per second: 140, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.176 [-1.412, 0.771], mean_best_reward: --\n",
      " 38825/100000: episode: 1075, duration: 0.236s, episode steps: 39, steps per second: 165, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.046 [-1.626, 0.806], mean_best_reward: --\n",
      " 38849/100000: episode: 1076, duration: 0.162s, episode steps: 24, steps per second: 148, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.096, 0.633], mean_best_reward: --\n",
      " 38878/100000: episode: 1077, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.121 [-1.220, 0.396], mean_best_reward: --\n",
      " 38901/100000: episode: 1078, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.108 [-1.032, 0.638], mean_best_reward: --\n",
      " 38915/100000: episode: 1079, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.096 [-0.639, 1.277], mean_best_reward: --\n",
      " 38926/100000: episode: 1080, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-1.592, 0.999], mean_best_reward: --\n",
      " 38956/100000: episode: 1081, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-1.068, 0.401], mean_best_reward: --\n",
      " 38969/100000: episode: 1082, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.936, 1.537], mean_best_reward: --\n",
      " 39071/100000: episode: 1083, duration: 0.589s, episode steps: 102, steps per second: 173, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.159 [-0.880, 1.471], mean_best_reward: --\n",
      " 39125/100000: episode: 1084, duration: 0.312s, episode steps: 54, steps per second: 173, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.090 [-0.789, 0.792], mean_best_reward: --\n",
      " 39149/100000: episode: 1085, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.069 [-1.200, 0.804], mean_best_reward: --\n",
      " 39160/100000: episode: 1086, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.143 [-1.817, 0.976], mean_best_reward: --\n",
      " 39176/100000: episode: 1087, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.110 [-1.676, 0.806], mean_best_reward: --\n",
      " 39235/100000: episode: 1088, duration: 0.294s, episode steps: 59, steps per second: 201, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.138 [-1.197, 1.663], mean_best_reward: --\n",
      " 39281/100000: episode: 1089, duration: 0.278s, episode steps: 46, steps per second: 165, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.097 [-1.007, 0.677], mean_best_reward: --\n",
      " 39312/100000: episode: 1090, duration: 0.140s, episode steps: 31, steps per second: 222, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.030 [-1.189, 0.642], mean_best_reward: --\n",
      " 39351/100000: episode: 1091, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.114 [-0.443, 1.410], mean_best_reward: --\n",
      " 39364/100000: episode: 1092, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.094 [-1.361, 0.810], mean_best_reward: --\n",
      " 39433/100000: episode: 1093, duration: 0.353s, episode steps: 69, steps per second: 195, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.115 [-1.006, 1.329], mean_best_reward: --\n",
      " 39446/100000: episode: 1094, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.110 [-1.352, 0.741], mean_best_reward: --\n",
      " 39510/100000: episode: 1095, duration: 0.300s, episode steps: 64, steps per second: 213, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.785, 1.144], mean_best_reward: --\n",
      " 39555/100000: episode: 1096, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.033 [-1.495, 0.971], mean_best_reward: --\n",
      " 39583/100000: episode: 1097, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.124 [-1.050, 0.381], mean_best_reward: --\n",
      " 39607/100000: episode: 1098, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.400, 0.824], mean_best_reward: --\n",
      " 39623/100000: episode: 1099, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.074 [-1.951, 1.215], mean_best_reward: --\n",
      " 39642/100000: episode: 1100, duration: 0.238s, episode steps: 19, steps per second: 80, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.098 [-1.221, 0.615], mean_best_reward: --\n",
      " 39688/100000: episode: 1101, duration: 0.216s, episode steps: 46, steps per second: 213, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.002 [-1.333, 0.816], mean_best_reward: 98.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39744/100000: episode: 1102, duration: 0.341s, episode steps: 56, steps per second: 164, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.071 [-1.068, 0.819], mean_best_reward: --\n",
      " 39778/100000: episode: 1103, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.138 [-1.161, 0.703], mean_best_reward: --\n",
      " 39944/100000: episode: 1104, duration: 0.792s, episode steps: 166, steps per second: 210, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.117 [-1.401, 1.009], mean_best_reward: --\n",
      " 39959/100000: episode: 1105, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.084 [-1.787, 1.153], mean_best_reward: --\n",
      " 39979/100000: episode: 1106, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.083 [-1.491, 0.976], mean_best_reward: --\n",
      " 40014/100000: episode: 1107, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.163 [-1.334, 0.604], mean_best_reward: --\n",
      " 40071/100000: episode: 1108, duration: 0.315s, episode steps: 57, steps per second: 181, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.024 [-0.879, 0.964], mean_best_reward: --\n",
      " 40092/100000: episode: 1109, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.074 [-1.622, 0.950], mean_best_reward: --\n",
      " 40143/100000: episode: 1110, duration: 0.230s, episode steps: 51, steps per second: 222, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.092 [-1.192, 0.755], mean_best_reward: --\n",
      " 40174/100000: episode: 1111, duration: 0.145s, episode steps: 31, steps per second: 214, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.105 [-0.974, 0.390], mean_best_reward: --\n",
      " 40191/100000: episode: 1112, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.087 [-1.181, 0.624], mean_best_reward: --\n",
      " 40243/100000: episode: 1113, duration: 0.248s, episode steps: 52, steps per second: 210, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.051 [-0.751, 1.114], mean_best_reward: --\n",
      " 40264/100000: episode: 1114, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.102 [-0.554, 1.135], mean_best_reward: --\n",
      " 40281/100000: episode: 1115, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.094 [-0.799, 1.201], mean_best_reward: --\n",
      " 40339/100000: episode: 1116, duration: 0.272s, episode steps: 58, steps per second: 214, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.075 [-0.881, 1.080], mean_best_reward: --\n",
      " 40390/100000: episode: 1117, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.027 [-0.588, 0.999], mean_best_reward: --\n",
      " 40431/100000: episode: 1118, duration: 0.197s, episode steps: 41, steps per second: 208, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.035 [-1.815, 0.967], mean_best_reward: --\n",
      " 40447/100000: episode: 1119, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.113 [-0.737, 1.265], mean_best_reward: --\n",
      " 40475/100000: episode: 1120, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-1.019, 1.518], mean_best_reward: --\n",
      " 40500/100000: episode: 1121, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.112 [-0.583, 1.462], mean_best_reward: --\n",
      " 40535/100000: episode: 1122, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.156 [-1.466, 0.948], mean_best_reward: --\n",
      " 40574/100000: episode: 1123, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.123 [-0.657, 1.136], mean_best_reward: --\n",
      " 40613/100000: episode: 1124, duration: 0.182s, episode steps: 39, steps per second: 214, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.067 [-1.516, 0.644], mean_best_reward: --\n",
      " 40679/100000: episode: 1125, duration: 0.347s, episode steps: 66, steps per second: 190, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.084 [-1.557, 1.195], mean_best_reward: --\n",
      " 40705/100000: episode: 1126, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.273, 0.642], mean_best_reward: --\n",
      " 40745/100000: episode: 1127, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.005 [-0.786, 1.264], mean_best_reward: --\n",
      " 40768/100000: episode: 1128, duration: 0.150s, episode steps: 23, steps per second: 153, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.073 [-0.419, 0.877], mean_best_reward: --\n",
      " 40785/100000: episode: 1129, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.060 [-1.329, 0.814], mean_best_reward: --\n",
      " 40860/100000: episode: 1130, duration: 0.403s, episode steps: 75, steps per second: 186, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.164 [-0.769, 1.071], mean_best_reward: --\n",
      " 40891/100000: episode: 1131, duration: 0.203s, episode steps: 31, steps per second: 152, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.066 [-0.596, 1.003], mean_best_reward: --\n",
      " 40916/100000: episode: 1132, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.063 [-1.137, 0.620], mean_best_reward: --\n",
      " 40966/100000: episode: 1133, duration: 0.308s, episode steps: 50, steps per second: 162, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.112 [-0.586, 1.040], mean_best_reward: --\n",
      " 41031/100000: episode: 1134, duration: 0.424s, episode steps: 65, steps per second: 153, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.018 [-0.794, 1.124], mean_best_reward: --\n",
      " 41070/100000: episode: 1135, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.071 [-0.768, 1.228], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41153/100000: episode: 1136, duration: 0.413s, episode steps: 83, steps per second: 201, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.112 [-0.792, 1.078], mean_best_reward: --\n",
      " 41180/100000: episode: 1137, duration: 0.168s, episode steps: 27, steps per second: 160, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.080 [-1.235, 0.811], mean_best_reward: --\n",
      " 41219/100000: episode: 1138, duration: 0.247s, episode steps: 39, steps per second: 158, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.116 [-1.072, 0.713], mean_best_reward: --\n",
      " 41267/100000: episode: 1139, duration: 0.293s, episode steps: 48, steps per second: 164, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.107, 0.895], mean_best_reward: --\n",
      " 41327/100000: episode: 1140, duration: 0.404s, episode steps: 60, steps per second: 148, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.201 [-1.834, 1.000], mean_best_reward: --\n",
      " 41380/100000: episode: 1141, duration: 0.311s, episode steps: 53, steps per second: 171, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.031 [-0.812, 1.207], mean_best_reward: --\n",
      " 41432/100000: episode: 1142, duration: 0.299s, episode steps: 52, steps per second: 174, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.063 [-0.745, 1.107], mean_best_reward: --\n",
      " 41458/100000: episode: 1143, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.099 [-1.387, 0.634], mean_best_reward: --\n",
      " 41485/100000: episode: 1144, duration: 0.187s, episode steps: 27, steps per second: 144, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.091 [-0.730, 1.146], mean_best_reward: --\n",
      " 41519/100000: episode: 1145, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.599, 1.472], mean_best_reward: --\n",
      " 41562/100000: episode: 1146, duration: 0.328s, episode steps: 43, steps per second: 131, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.092 [-0.895, 0.586], mean_best_reward: --\n",
      " 41597/100000: episode: 1147, duration: 0.389s, episode steps: 35, steps per second: 90, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.056 [-0.607, 1.287], mean_best_reward: --\n",
      " 41646/100000: episode: 1148, duration: 0.346s, episode steps: 49, steps per second: 141, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.106 [-0.865, 0.490], mean_best_reward: --\n",
      " 41732/100000: episode: 1149, duration: 0.452s, episode steps: 86, steps per second: 190, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.085 [-1.151, 0.950], mean_best_reward: --\n",
      " 41771/100000: episode: 1150, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.101 [-0.563, 1.334], mean_best_reward: --\n",
      " 41834/100000: episode: 1151, duration: 0.381s, episode steps: 63, steps per second: 166, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.173 [-1.574, 1.080], mean_best_reward: 94.500000\n",
      " 41905/100000: episode: 1152, duration: 0.365s, episode steps: 71, steps per second: 195, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.202 [-0.740, 1.802], mean_best_reward: --\n",
      " 41923/100000: episode: 1153, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.074 [-0.958, 1.607], mean_best_reward: --\n",
      " 41972/100000: episode: 1154, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.127 [-0.576, 1.109], mean_best_reward: --\n",
      " 42018/100000: episode: 1155, duration: 0.255s, episode steps: 46, steps per second: 180, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.408, 0.680], mean_best_reward: --\n",
      " 42062/100000: episode: 1156, duration: 0.370s, episode steps: 44, steps per second: 119, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.613, 0.971], mean_best_reward: --\n",
      " 42086/100000: episode: 1157, duration: 0.178s, episode steps: 24, steps per second: 135, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.044 [-1.570, 0.838], mean_best_reward: --\n",
      " 42111/100000: episode: 1158, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.066 [-0.990, 1.955], mean_best_reward: --\n",
      " 42146/100000: episode: 1159, duration: 0.227s, episode steps: 35, steps per second: 154, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.158 [-0.544, 1.236], mean_best_reward: --\n",
      " 42174/100000: episode: 1160, duration: 0.202s, episode steps: 28, steps per second: 139, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.056 [-1.985, 1.209], mean_best_reward: --\n",
      " 42195/100000: episode: 1161, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.060 [-0.957, 1.705], mean_best_reward: --\n",
      " 42229/100000: episode: 1162, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.139 [-1.068, 0.547], mean_best_reward: --\n",
      " 42285/100000: episode: 1163, duration: 0.314s, episode steps: 56, steps per second: 178, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.055 [-0.591, 0.945], mean_best_reward: --\n",
      " 42325/100000: episode: 1164, duration: 0.235s, episode steps: 40, steps per second: 170, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.117 [-0.446, 0.901], mean_best_reward: --\n",
      " 42425/100000: episode: 1165, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.055 [-1.206, 1.153], mean_best_reward: --\n",
      " 42443/100000: episode: 1166, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.574, 1.066], mean_best_reward: --\n",
      " 42464/100000: episode: 1167, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.061 [-0.812, 1.440], mean_best_reward: --\n",
      " 42486/100000: episode: 1168, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.066 [-0.993, 1.508], mean_best_reward: --\n",
      " 42515/100000: episode: 1169, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.068 [-1.211, 0.464], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42567/100000: episode: 1170, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.068 [-0.938, 0.585], mean_best_reward: --\n",
      " 42619/100000: episode: 1171, duration: 0.344s, episode steps: 52, steps per second: 151, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.957, 1.160], mean_best_reward: --\n",
      " 42639/100000: episode: 1172, duration: 0.141s, episode steps: 20, steps per second: 142, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.068 [-0.981, 1.698], mean_best_reward: --\n",
      " 42656/100000: episode: 1173, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.055 [-0.996, 1.554], mean_best_reward: --\n",
      " 42681/100000: episode: 1174, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.095 [-0.957, 0.367], mean_best_reward: --\n",
      " 42729/100000: episode: 1175, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.491, 1.169], mean_best_reward: --\n",
      " 42760/100000: episode: 1176, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.060 [-0.778, 1.484], mean_best_reward: --\n",
      " 42826/100000: episode: 1177, duration: 0.405s, episode steps: 66, steps per second: 163, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-1.158, 1.783], mean_best_reward: --\n",
      " 42879/100000: episode: 1178, duration: 0.264s, episode steps: 53, steps per second: 201, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.083 [-1.073, 0.711], mean_best_reward: --\n",
      " 43023/100000: episode: 1179, duration: 0.938s, episode steps: 144, steps per second: 154, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.109 [-1.317, 1.115], mean_best_reward: --\n",
      " 43061/100000: episode: 1180, duration: 0.181s, episode steps: 38, steps per second: 210, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.090 [-0.931, 0.585], mean_best_reward: --\n",
      " 43189/100000: episode: 1181, duration: 0.589s, episode steps: 128, steps per second: 217, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.193, 1.013], mean_best_reward: --\n",
      " 43273/100000: episode: 1182, duration: 0.390s, episode steps: 84, steps per second: 215, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.209 [-0.729, 1.330], mean_best_reward: --\n",
      " 43408/100000: episode: 1183, duration: 0.691s, episode steps: 135, steps per second: 195, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.044 [-1.288, 0.953], mean_best_reward: --\n",
      " 43450/100000: episode: 1184, duration: 0.198s, episode steps: 42, steps per second: 213, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.068 [-0.565, 1.341], mean_best_reward: --\n",
      " 43467/100000: episode: 1185, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.083 [-1.387, 0.764], mean_best_reward: --\n",
      " 43479/100000: episode: 1186, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.789, 1.138], mean_best_reward: --\n",
      " 43524/100000: episode: 1187, duration: 0.211s, episode steps: 45, steps per second: 214, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.108 [-0.583, 1.307], mean_best_reward: --\n",
      " 43585/100000: episode: 1188, duration: 0.294s, episode steps: 61, steps per second: 208, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.147 [-0.584, 1.465], mean_best_reward: --\n",
      " 43595/100000: episode: 1189, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.112 [-1.573, 0.980], mean_best_reward: --\n",
      " 43625/100000: episode: 1190, duration: 0.140s, episode steps: 30, steps per second: 214, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.007, 0.540], mean_best_reward: --\n",
      " 43657/100000: episode: 1191, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.053, 0.606], mean_best_reward: --\n",
      " 43680/100000: episode: 1192, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.105 [-1.243, 0.553], mean_best_reward: --\n",
      " 43699/100000: episode: 1193, duration: 0.133s, episode steps: 19, steps per second: 142, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.123 [-0.571, 1.135], mean_best_reward: --\n",
      " 43760/100000: episode: 1194, duration: 0.397s, episode steps: 61, steps per second: 154, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.128 [-0.884, 1.219], mean_best_reward: --\n",
      " 43787/100000: episode: 1195, duration: 0.183s, episode steps: 27, steps per second: 148, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.061 [-2.010, 1.177], mean_best_reward: --\n",
      " 43835/100000: episode: 1196, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.212, 0.711], mean_best_reward: --\n",
      " 43875/100000: episode: 1197, duration: 0.233s, episode steps: 40, steps per second: 172, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.683, 1.280], mean_best_reward: --\n",
      " 43892/100000: episode: 1198, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.079 [-0.588, 1.166], mean_best_reward: --\n",
      " 43925/100000: episode: 1199, duration: 0.195s, episode steps: 33, steps per second: 170, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.074 [-0.929, 0.518], mean_best_reward: --\n",
      " 43956/100000: episode: 1200, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.064 [-0.617, 1.322], mean_best_reward: --\n",
      " 44019/100000: episode: 1201, duration: 0.323s, episode steps: 63, steps per second: 195, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.140 [-1.298, 0.965], mean_best_reward: 114.000000\n",
      " 44043/100000: episode: 1202, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.111 [-1.018, 0.622], mean_best_reward: --\n",
      " 44063/100000: episode: 1203, duration: 0.138s, episode steps: 20, steps per second: 145, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.733, 1.183], mean_best_reward: --\n",
      " 44076/100000: episode: 1204, duration: 0.095s, episode steps: 13, steps per second: 137, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.316, 0.738], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44108/100000: episode: 1205, duration: 0.264s, episode steps: 32, steps per second: 121, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.301, 0.506], mean_best_reward: --\n",
      " 44135/100000: episode: 1206, duration: 0.207s, episode steps: 27, steps per second: 130, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.068 [-1.282, 0.632], mean_best_reward: --\n",
      " 44166/100000: episode: 1207, duration: 0.147s, episode steps: 31, steps per second: 211, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.085 [-0.338, 0.988], mean_best_reward: --\n",
      " 44232/100000: episode: 1208, duration: 0.310s, episode steps: 66, steps per second: 213, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.169 [-0.667, 0.969], mean_best_reward: --\n",
      " 44270/100000: episode: 1209, duration: 0.178s, episode steps: 38, steps per second: 213, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.144, 0.399], mean_best_reward: --\n",
      " 44334/100000: episode: 1210, duration: 0.299s, episode steps: 64, steps per second: 214, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-1.140, 1.694], mean_best_reward: --\n",
      " 44366/100000: episode: 1211, duration: 0.148s, episode steps: 32, steps per second: 217, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.156, 0.799], mean_best_reward: --\n",
      " 44387/100000: episode: 1212, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.086 [-0.621, 0.967], mean_best_reward: --\n",
      " 44423/100000: episode: 1213, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.137 [-1.116, 0.704], mean_best_reward: --\n",
      " 44504/100000: episode: 1214, duration: 0.394s, episode steps: 81, steps per second: 206, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.116 [-1.645, 1.199], mean_best_reward: --\n",
      " 44546/100000: episode: 1215, duration: 0.208s, episode steps: 42, steps per second: 202, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.571, 1.136], mean_best_reward: --\n",
      " 44566/100000: episode: 1216, duration: 0.097s, episode steps: 20, steps per second: 205, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-0.985, 0.549], mean_best_reward: --\n",
      " 44594/100000: episode: 1217, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.050 [-1.286, 0.811], mean_best_reward: --\n",
      " 44650/100000: episode: 1218, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.105 [-0.751, 1.598], mean_best_reward: --\n",
      " 44693/100000: episode: 1219, duration: 0.205s, episode steps: 43, steps per second: 210, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.095 [-1.662, 0.778], mean_best_reward: --\n",
      " 44740/100000: episode: 1220, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.100 [-0.838, 0.884], mean_best_reward: --\n",
      " 44791/100000: episode: 1221, duration: 0.231s, episode steps: 51, steps per second: 221, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.043 [-1.436, 0.614], mean_best_reward: --\n",
      " 44852/100000: episode: 1222, duration: 0.286s, episode steps: 61, steps per second: 214, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.062 [-1.279, 0.758], mean_best_reward: --\n",
      " 44882/100000: episode: 1223, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.639, 1.227], mean_best_reward: --\n",
      " 44931/100000: episode: 1224, duration: 0.270s, episode steps: 49, steps per second: 181, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.089 [-1.219, 0.628], mean_best_reward: --\n",
      " 44984/100000: episode: 1225, duration: 0.295s, episode steps: 53, steps per second: 180, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.055 [-1.506, 0.933], mean_best_reward: --\n",
      " 45029/100000: episode: 1226, duration: 0.284s, episode steps: 45, steps per second: 158, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.143 [-0.746, 1.291], mean_best_reward: --\n",
      " 45055/100000: episode: 1227, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.113 [-0.841, 0.385], mean_best_reward: --\n",
      " 45081/100000: episode: 1228, duration: 0.171s, episode steps: 26, steps per second: 152, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-0.850, 0.420], mean_best_reward: --\n",
      " 45127/100000: episode: 1229, duration: 0.319s, episode steps: 46, steps per second: 144, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.201 [-0.613, 1.270], mean_best_reward: --\n",
      " 45268/100000: episode: 1230, duration: 0.812s, episode steps: 141, steps per second: 174, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.245 [-0.917, 1.292], mean_best_reward: --\n",
      " 45339/100000: episode: 1231, duration: 0.372s, episode steps: 71, steps per second: 191, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.139 [-0.823, 1.476], mean_best_reward: --\n",
      " 45402/100000: episode: 1232, duration: 0.303s, episode steps: 63, steps per second: 208, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.107 [-0.970, 1.264], mean_best_reward: --\n",
      " 45486/100000: episode: 1233, duration: 0.460s, episode steps: 84, steps per second: 183, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.142 [-0.741, 1.127], mean_best_reward: --\n",
      " 45535/100000: episode: 1234, duration: 0.298s, episode steps: 49, steps per second: 165, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.168 [-0.936, 0.624], mean_best_reward: --\n",
      " 45558/100000: episode: 1235, duration: 0.143s, episode steps: 23, steps per second: 160, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.062 [-1.532, 0.793], mean_best_reward: --\n",
      " 45603/100000: episode: 1236, duration: 0.329s, episode steps: 45, steps per second: 137, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.064 [-1.080, 0.596], mean_best_reward: --\n",
      " 45632/100000: episode: 1237, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.100 [-1.477, 0.604], mean_best_reward: --\n",
      " 45662/100000: episode: 1238, duration: 0.145s, episode steps: 30, steps per second: 208, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.140 [-0.601, 1.005], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45707/100000: episode: 1239, duration: 0.265s, episode steps: 45, steps per second: 170, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.088 [-1.182, 0.569], mean_best_reward: --\n",
      " 45764/100000: episode: 1240, duration: 0.274s, episode steps: 57, steps per second: 208, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.028 [-0.963, 1.040], mean_best_reward: --\n",
      " 45851/100000: episode: 1241, duration: 0.411s, episode steps: 87, steps per second: 211, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.080 [-1.524, 0.783], mean_best_reward: --\n",
      " 45863/100000: episode: 1242, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.115 [-1.608, 0.961], mean_best_reward: --\n",
      " 45887/100000: episode: 1243, duration: 0.191s, episode steps: 24, steps per second: 126, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.083 [-0.838, 1.606], mean_best_reward: --\n",
      " 45911/100000: episode: 1244, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.068 [-1.175, 0.589], mean_best_reward: --\n",
      " 45951/100000: episode: 1245, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.002 [-1.022, 1.636], mean_best_reward: --\n",
      " 45983/100000: episode: 1246, duration: 0.150s, episode steps: 32, steps per second: 214, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.207, 0.800], mean_best_reward: --\n",
      " 46010/100000: episode: 1247, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.083 [-0.755, 1.460], mean_best_reward: --\n",
      " 46075/100000: episode: 1248, duration: 0.294s, episode steps: 65, steps per second: 221, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.080 [-1.555, 0.593], mean_best_reward: --\n",
      " 46093/100000: episode: 1249, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.062 [-0.644, 1.280], mean_best_reward: --\n",
      " 46115/100000: episode: 1250, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.082 [-0.949, 0.604], mean_best_reward: --\n",
      " 46147/100000: episode: 1251, duration: 0.151s, episode steps: 32, steps per second: 211, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.107 [-0.997, 0.409], mean_best_reward: 74.500000\n",
      " 46179/100000: episode: 1252, duration: 0.147s, episode steps: 32, steps per second: 217, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.124 [-0.929, 0.603], mean_best_reward: --\n",
      " 46229/100000: episode: 1253, duration: 0.231s, episode steps: 50, steps per second: 216, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.190 [-0.803, 1.282], mean_best_reward: --\n",
      " 46257/100000: episode: 1254, duration: 0.130s, episode steps: 28, steps per second: 215, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.057 [-1.681, 0.824], mean_best_reward: --\n",
      " 46287/100000: episode: 1255, duration: 0.140s, episode steps: 30, steps per second: 215, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.107 [-1.513, 0.442], mean_best_reward: --\n",
      " 46306/100000: episode: 1256, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.102 [-0.570, 1.068], mean_best_reward: --\n",
      " 46342/100000: episode: 1257, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.109 [-0.614, 0.958], mean_best_reward: --\n",
      " 46386/100000: episode: 1258, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.090, 0.567], mean_best_reward: --\n",
      " 46426/100000: episode: 1259, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.472, 1.235], mean_best_reward: --\n",
      " 46492/100000: episode: 1260, duration: 0.339s, episode steps: 66, steps per second: 195, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.129 [-0.873, 1.307], mean_best_reward: --\n",
      " 46566/100000: episode: 1261, duration: 0.366s, episode steps: 74, steps per second: 202, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.659, 1.431], mean_best_reward: --\n",
      " 46613/100000: episode: 1262, duration: 0.217s, episode steps: 47, steps per second: 216, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.023 [-0.810, 1.161], mean_best_reward: --\n",
      " 46630/100000: episode: 1263, duration: 0.080s, episode steps: 17, steps per second: 211, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.101 [-2.421, 1.347], mean_best_reward: --\n",
      " 46651/100000: episode: 1264, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.102 [-0.610, 1.425], mean_best_reward: --\n",
      " 46684/100000: episode: 1265, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.071 [-1.338, 0.594], mean_best_reward: --\n",
      " 46710/100000: episode: 1266, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.103 [-1.530, 0.619], mean_best_reward: --\n",
      " 46729/100000: episode: 1267, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.113 [-0.995, 0.541], mean_best_reward: --\n",
      " 46741/100000: episode: 1268, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.125 [-1.626, 0.952], mean_best_reward: --\n",
      " 46767/100000: episode: 1269, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.008, 0.374], mean_best_reward: --\n",
      " 46805/100000: episode: 1270, duration: 0.176s, episode steps: 38, steps per second: 216, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.021 [-1.016, 1.472], mean_best_reward: --\n",
      " 46823/100000: episode: 1271, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.078 [-0.819, 1.241], mean_best_reward: --\n",
      " 46842/100000: episode: 1272, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.108 [-0.917, 0.555], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46857/100000: episode: 1273, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.077 [-0.834, 1.376], mean_best_reward: --\n",
      " 46886/100000: episode: 1274, duration: 0.134s, episode steps: 29, steps per second: 217, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.094 [-0.595, 1.159], mean_best_reward: --\n",
      " 46919/100000: episode: 1275, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.025 [-1.280, 0.941], mean_best_reward: --\n",
      " 46958/100000: episode: 1276, duration: 0.181s, episode steps: 39, steps per second: 215, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.081 [-1.029, 0.713], mean_best_reward: --\n",
      " 47011/100000: episode: 1277, duration: 0.246s, episode steps: 53, steps per second: 216, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.073 [-1.459, 0.721], mean_best_reward: --\n",
      " 47055/100000: episode: 1278, duration: 0.210s, episode steps: 44, steps per second: 209, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.124 [-0.592, 1.106], mean_best_reward: --\n",
      " 47117/100000: episode: 1279, duration: 0.293s, episode steps: 62, steps per second: 212, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.080 [-1.110, 0.620], mean_best_reward: --\n",
      " 47154/100000: episode: 1280, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.150 [-1.220, 0.354], mean_best_reward: --\n",
      " 47242/100000: episode: 1281, duration: 0.419s, episode steps: 88, steps per second: 210, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-1.316, 0.937], mean_best_reward: --\n",
      " 47263/100000: episode: 1282, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.104 [-1.716, 0.825], mean_best_reward: --\n",
      " 47294/100000: episode: 1283, duration: 0.146s, episode steps: 31, steps per second: 212, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.072 [-1.032, 0.584], mean_best_reward: --\n",
      " 47305/100000: episode: 1284, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.127 [-0.982, 1.689], mean_best_reward: --\n",
      " 47367/100000: episode: 1285, duration: 0.370s, episode steps: 62, steps per second: 168, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.072 [-1.367, 1.290], mean_best_reward: --\n",
      " 47405/100000: episode: 1286, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.066 [-0.562, 1.029], mean_best_reward: --\n",
      " 47458/100000: episode: 1287, duration: 0.312s, episode steps: 53, steps per second: 170, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.028 [-1.087, 0.763], mean_best_reward: --\n",
      " 47505/100000: episode: 1288, duration: 0.295s, episode steps: 47, steps per second: 160, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.039 [-1.331, 0.619], mean_best_reward: --\n",
      " 47573/100000: episode: 1289, duration: 0.345s, episode steps: 68, steps per second: 197, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.224 [-0.809, 1.399], mean_best_reward: --\n",
      " 47626/100000: episode: 1290, duration: 0.259s, episode steps: 53, steps per second: 205, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.063 [-0.645, 1.201], mean_best_reward: --\n",
      " 47666/100000: episode: 1291, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.001, 0.927], mean_best_reward: --\n",
      " 47705/100000: episode: 1292, duration: 0.193s, episode steps: 39, steps per second: 202, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.023 [-1.420, 0.835], mean_best_reward: --\n",
      " 47717/100000: episode: 1293, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.127 [-0.749, 1.374], mean_best_reward: --\n",
      " 47752/100000: episode: 1294, duration: 0.165s, episode steps: 35, steps per second: 212, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.082 [-1.064, 0.367], mean_best_reward: --\n",
      " 47779/100000: episode: 1295, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.035 [-1.142, 1.794], mean_best_reward: --\n",
      " 47809/100000: episode: 1296, duration: 0.139s, episode steps: 30, steps per second: 216, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.001, 0.575], mean_best_reward: --\n",
      " 47831/100000: episode: 1297, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.111, 0.544], mean_best_reward: --\n",
      " 47852/100000: episode: 1298, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.075 [-1.192, 0.786], mean_best_reward: --\n",
      " 47879/100000: episode: 1299, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: -0.067 [-2.534, 1.526], mean_best_reward: --\n",
      " 47901/100000: episode: 1300, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.070 [-0.961, 1.607], mean_best_reward: --\n",
      " 47941/100000: episode: 1301, duration: 0.250s, episode steps: 40, steps per second: 160, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-0.918, 0.594], mean_best_reward: 86.500000\n",
      " 48000/100000: episode: 1302, duration: 0.276s, episode steps: 59, steps per second: 214, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.123 [-0.862, 1.347], mean_best_reward: --\n",
      " 48037/100000: episode: 1303, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.025 [-0.645, 1.070], mean_best_reward: --\n",
      " 48079/100000: episode: 1304, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.589, 0.965], mean_best_reward: --\n",
      " 48153/100000: episode: 1305, duration: 0.368s, episode steps: 74, steps per second: 201, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.062 [-1.215, 0.760], mean_best_reward: --\n",
      " 48186/100000: episode: 1306, duration: 0.238s, episode steps: 33, steps per second: 139, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.047 [-0.884, 0.614], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48246/100000: episode: 1307, duration: 0.320s, episode steps: 60, steps per second: 187, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.128 [-0.735, 1.353], mean_best_reward: --\n",
      " 48287/100000: episode: 1308, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.101 [-0.347, 0.746], mean_best_reward: --\n",
      " 48303/100000: episode: 1309, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.778, 1.296], mean_best_reward: --\n",
      " 48362/100000: episode: 1310, duration: 0.338s, episode steps: 59, steps per second: 175, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.080 [-1.163, 0.612], mean_best_reward: --\n",
      " 48417/100000: episode: 1311, duration: 0.331s, episode steps: 55, steps per second: 166, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.041 [-1.488, 1.551], mean_best_reward: --\n",
      " 48461/100000: episode: 1312, duration: 0.222s, episode steps: 44, steps per second: 198, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.015 [-0.927, 1.424], mean_best_reward: --\n",
      " 48508/100000: episode: 1313, duration: 0.271s, episode steps: 47, steps per second: 174, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.059 [-0.412, 0.960], mean_best_reward: --\n",
      " 48563/100000: episode: 1314, duration: 0.329s, episode steps: 55, steps per second: 167, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.106 [-1.348, 0.671], mean_best_reward: --\n",
      " 48633/100000: episode: 1315, duration: 0.341s, episode steps: 70, steps per second: 205, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.017 [-2.124, 1.173], mean_best_reward: --\n",
      " 48647/100000: episode: 1316, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.110 [-1.606, 0.771], mean_best_reward: --\n",
      " 48665/100000: episode: 1317, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.042, 0.568], mean_best_reward: --\n",
      " 48677/100000: episode: 1318, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.103 [-1.430, 0.831], mean_best_reward: --\n",
      " 48759/100000: episode: 1319, duration: 0.425s, episode steps: 82, steps per second: 193, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.029 [-1.158, 1.104], mean_best_reward: --\n",
      " 48818/100000: episode: 1320, duration: 0.345s, episode steps: 59, steps per second: 171, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.100 [-1.509, 1.552], mean_best_reward: --\n",
      " 48914/100000: episode: 1321, duration: 0.538s, episode steps: 96, steps per second: 178, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.120 [-1.623, 1.418], mean_best_reward: --\n",
      " 48959/100000: episode: 1322, duration: 0.209s, episode steps: 45, steps per second: 215, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.144 [-1.217, 0.692], mean_best_reward: --\n",
      " 49002/100000: episode: 1323, duration: 0.207s, episode steps: 43, steps per second: 207, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.128 [-0.692, 0.775], mean_best_reward: --\n",
      " 49035/100000: episode: 1324, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.078 [-1.047, 0.554], mean_best_reward: --\n",
      " 49066/100000: episode: 1325, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.095 [-1.725, 0.891], mean_best_reward: --\n",
      " 49087/100000: episode: 1326, duration: 0.098s, episode steps: 21, steps per second: 215, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.088 [-0.594, 0.918], mean_best_reward: --\n",
      " 49127/100000: episode: 1327, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.022 [-1.420, 1.116], mean_best_reward: --\n",
      " 49170/100000: episode: 1328, duration: 0.198s, episode steps: 43, steps per second: 217, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.085 [-0.757, 1.075], mean_best_reward: --\n",
      " 49192/100000: episode: 1329, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.586, 1.191], mean_best_reward: --\n",
      " 49242/100000: episode: 1330, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.151 [-1.112, 0.586], mean_best_reward: --\n",
      " 49264/100000: episode: 1331, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.076, 0.579], mean_best_reward: --\n",
      " 49331/100000: episode: 1332, duration: 0.380s, episode steps: 67, steps per second: 176, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.081 [-0.800, 0.877], mean_best_reward: --\n",
      " 49371/100000: episode: 1333, duration: 0.227s, episode steps: 40, steps per second: 177, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.017 [-1.241, 0.933], mean_best_reward: --\n",
      " 49399/100000: episode: 1334, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.100 [-0.432, 0.788], mean_best_reward: --\n",
      " 49415/100000: episode: 1335, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.117 [-0.788, 1.628], mean_best_reward: --\n",
      " 49441/100000: episode: 1336, duration: 0.186s, episode steps: 26, steps per second: 140, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.081 [-1.391, 0.620], mean_best_reward: --\n",
      " 49477/100000: episode: 1337, duration: 0.259s, episode steps: 36, steps per second: 139, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.042 [-1.707, 0.816], mean_best_reward: --\n",
      " 49506/100000: episode: 1338, duration: 0.182s, episode steps: 29, steps per second: 159, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.070 [-1.004, 0.621], mean_best_reward: --\n",
      " 49540/100000: episode: 1339, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.165 [-0.456, 1.128], mean_best_reward: --\n",
      " 49593/100000: episode: 1340, duration: 0.294s, episode steps: 53, steps per second: 180, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.113 [-0.358, 1.116], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49653/100000: episode: 1341, duration: 0.327s, episode steps: 60, steps per second: 183, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-1.250, 0.931], mean_best_reward: --\n",
      " 49701/100000: episode: 1342, duration: 0.225s, episode steps: 48, steps per second: 213, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.081 [-1.545, 0.421], mean_best_reward: --\n",
      " 49740/100000: episode: 1343, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.196 [-0.594, 1.155], mean_best_reward: --\n",
      " 49786/100000: episode: 1344, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.909, 1.244], mean_best_reward: --\n",
      " 49816/100000: episode: 1345, duration: 0.138s, episode steps: 30, steps per second: 218, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.744, 1.112], mean_best_reward: --\n",
      " 49889/100000: episode: 1346, duration: 0.336s, episode steps: 73, steps per second: 217, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.030 [-1.385, 0.815], mean_best_reward: --\n",
      " 49951/100000: episode: 1347, duration: 0.288s, episode steps: 62, steps per second: 215, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.913, 0.985], mean_best_reward: --\n",
      " 50018/100000: episode: 1348, duration: 0.326s, episode steps: 67, steps per second: 206, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.196 [-0.930, 1.678], mean_best_reward: --\n",
      " 50058/100000: episode: 1349, duration: 0.185s, episode steps: 40, steps per second: 217, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.142 [-0.435, 1.354], mean_best_reward: --\n",
      " 50086/100000: episode: 1350, duration: 0.131s, episode steps: 28, steps per second: 213, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.100 [-1.572, 0.776], mean_best_reward: --\n",
      " 50126/100000: episode: 1351, duration: 0.185s, episode steps: 40, steps per second: 217, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-1.138, 0.635], mean_best_reward: 95.500000\n",
      " 50153/100000: episode: 1352, duration: 0.135s, episode steps: 27, steps per second: 201, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.135 [-0.777, 1.269], mean_best_reward: --\n",
      " 50228/100000: episode: 1353, duration: 0.394s, episode steps: 75, steps per second: 190, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.096 [-1.255, 0.775], mean_best_reward: --\n",
      " 50251/100000: episode: 1354, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.053 [-1.208, 0.782], mean_best_reward: --\n",
      " 50269/100000: episode: 1355, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.068 [-1.762, 1.138], mean_best_reward: --\n",
      " 50302/100000: episode: 1356, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.130 [-0.863, 0.364], mean_best_reward: --\n",
      " 50334/100000: episode: 1357, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.308, 0.574], mean_best_reward: --\n",
      " 50392/100000: episode: 1358, duration: 0.293s, episode steps: 58, steps per second: 198, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.162 [-0.607, 1.073], mean_best_reward: --\n",
      " 50418/100000: episode: 1359, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.124 [-0.377, 0.913], mean_best_reward: --\n",
      " 50448/100000: episode: 1360, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.082 [-0.604, 1.299], mean_best_reward: --\n",
      " 50476/100000: episode: 1361, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.127 [-0.370, 0.812], mean_best_reward: --\n",
      " 50513/100000: episode: 1362, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.089 [-0.865, 1.161], mean_best_reward: --\n",
      " 50557/100000: episode: 1363, duration: 0.236s, episode steps: 44, steps per second: 187, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.067 [-1.333, 0.581], mean_best_reward: --\n",
      " 50573/100000: episode: 1364, duration: 0.127s, episode steps: 16, steps per second: 126, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.100 [-1.942, 1.150], mean_best_reward: --\n",
      " 50609/100000: episode: 1365, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.065 [-0.628, 1.311], mean_best_reward: --\n",
      " 50640/100000: episode: 1366, duration: 0.185s, episode steps: 31, steps per second: 167, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.041 [-1.132, 0.643], mean_best_reward: --\n",
      " 50661/100000: episode: 1367, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.095 [-0.563, 1.058], mean_best_reward: --\n",
      " 50798/100000: episode: 1368, duration: 0.669s, episode steps: 137, steps per second: 205, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.081 [-1.378, 1.661], mean_best_reward: --\n",
      " 50831/100000: episode: 1369, duration: 0.153s, episode steps: 33, steps per second: 215, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.092 [-0.378, 1.286], mean_best_reward: --\n",
      " 50905/100000: episode: 1370, duration: 0.395s, episode steps: 74, steps per second: 188, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.110 [-1.281, 1.083], mean_best_reward: --\n",
      " 50946/100000: episode: 1371, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.130 [-0.461, 1.032], mean_best_reward: --\n",
      " 51012/100000: episode: 1372, duration: 0.330s, episode steps: 66, steps per second: 200, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.065 [-0.742, 0.969], mean_best_reward: --\n",
      " 51083/100000: episode: 1373, duration: 0.358s, episode steps: 71, steps per second: 198, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.262 [-0.830, 1.627], mean_best_reward: --\n",
      " 51106/100000: episode: 1374, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.054 [-0.783, 1.210], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51159/100000: episode: 1375, duration: 0.266s, episode steps: 53, steps per second: 199, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.033 [-1.089, 0.557], mean_best_reward: --\n",
      " 51181/100000: episode: 1376, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.078 [-1.062, 0.589], mean_best_reward: --\n",
      " 51214/100000: episode: 1377, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.046 [-0.601, 1.216], mean_best_reward: --\n",
      " 51277/100000: episode: 1378, duration: 0.297s, episode steps: 63, steps per second: 212, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.033 [-0.547, 1.181], mean_best_reward: --\n",
      " 51321/100000: episode: 1379, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-1.456, 0.646], mean_best_reward: --\n",
      " 51373/100000: episode: 1380, duration: 0.247s, episode steps: 52, steps per second: 210, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.063 [-0.821, 1.676], mean_best_reward: --\n",
      " 51401/100000: episode: 1381, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.476, 0.938], mean_best_reward: --\n",
      " 51436/100000: episode: 1382, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.132 [-1.601, 0.763], mean_best_reward: --\n",
      " 51456/100000: episode: 1383, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.546, 0.928], mean_best_reward: --\n",
      " 51490/100000: episode: 1384, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.074 [-0.639, 1.595], mean_best_reward: --\n",
      " 51522/100000: episode: 1385, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.260, 0.774], mean_best_reward: --\n",
      " 51580/100000: episode: 1386, duration: 0.285s, episode steps: 58, steps per second: 203, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.082 [-0.952, 0.827], mean_best_reward: --\n",
      " 51634/100000: episode: 1387, duration: 0.273s, episode steps: 54, steps per second: 198, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.064 [-0.794, 1.729], mean_best_reward: --\n",
      " 51672/100000: episode: 1388, duration: 0.174s, episode steps: 38, steps per second: 219, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.984, 0.390], mean_best_reward: --\n",
      " 51691/100000: episode: 1389, duration: 0.131s, episode steps: 19, steps per second: 145, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.068 [-0.992, 1.686], mean_best_reward: --\n",
      " 51741/100000: episode: 1390, duration: 0.228s, episode steps: 50, steps per second: 219, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.024 [-0.955, 1.324], mean_best_reward: --\n",
      " 51777/100000: episode: 1391, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.153, 0.772], mean_best_reward: --\n",
      " 51831/100000: episode: 1392, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.103 [-1.184, 0.734], mean_best_reward: --\n",
      " 51861/100000: episode: 1393, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.626, 1.013], mean_best_reward: --\n",
      " 51882/100000: episode: 1394, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.062 [-2.054, 1.333], mean_best_reward: --\n",
      " 51917/100000: episode: 1395, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.092 [-1.212, 0.570], mean_best_reward: --\n",
      " 51959/100000: episode: 1396, duration: 0.192s, episode steps: 42, steps per second: 219, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.114 [-1.010, 0.439], mean_best_reward: --\n",
      " 52011/100000: episode: 1397, duration: 0.255s, episode steps: 52, steps per second: 204, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.053 [-0.952, 0.612], mean_best_reward: --\n",
      " 52053/100000: episode: 1398, duration: 0.196s, episode steps: 42, steps per second: 214, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-1.541, 1.148], mean_best_reward: --\n",
      " 52077/100000: episode: 1399, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.980, 0.639], mean_best_reward: --\n",
      " 52133/100000: episode: 1400, duration: 0.294s, episode steps: 56, steps per second: 191, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.027 [-1.188, 0.954], mean_best_reward: --\n",
      " 52158/100000: episode: 1401, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.104 [-0.567, 1.536], mean_best_reward: 96.500000\n",
      " 52222/100000: episode: 1402, duration: 0.299s, episode steps: 64, steps per second: 214, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.132 [-0.949, 1.139], mean_best_reward: --\n",
      " 52263/100000: episode: 1403, duration: 0.221s, episode steps: 41, steps per second: 185, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.156 [-0.802, 1.312], mean_best_reward: --\n",
      " 52316/100000: episode: 1404, duration: 0.253s, episode steps: 53, steps per second: 209, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.052 [-0.800, 1.091], mean_best_reward: --\n",
      " 52372/100000: episode: 1405, duration: 0.355s, episode steps: 56, steps per second: 158, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.552, 1.189], mean_best_reward: --\n",
      " 52392/100000: episode: 1406, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.133 [-1.263, 0.755], mean_best_reward: --\n",
      " 52412/100000: episode: 1407, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.097 [-1.618, 0.939], mean_best_reward: --\n",
      " 52466/100000: episode: 1408, duration: 0.249s, episode steps: 54, steps per second: 217, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.103 [-1.609, 0.781], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52516/100000: episode: 1409, duration: 0.248s, episode steps: 50, steps per second: 201, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.159, 0.696], mean_best_reward: --\n",
      " 52539/100000: episode: 1410, duration: 0.119s, episode steps: 23, steps per second: 192, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.102 [-1.147, 0.577], mean_best_reward: --\n",
      " 52561/100000: episode: 1411, duration: 0.154s, episode steps: 22, steps per second: 143, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.053 [-0.991, 1.624], mean_best_reward: --\n",
      " 52608/100000: episode: 1412, duration: 0.290s, episode steps: 47, steps per second: 162, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.038 [-0.591, 1.142], mean_best_reward: --\n",
      " 52706/100000: episode: 1413, duration: 0.505s, episode steps: 98, steps per second: 194, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.847, 0.896], mean_best_reward: --\n",
      " 52746/100000: episode: 1414, duration: 0.235s, episode steps: 40, steps per second: 171, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.109 [-0.478, 0.878], mean_best_reward: --\n",
      " 52774/100000: episode: 1415, duration: 0.205s, episode steps: 28, steps per second: 136, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.060 [-0.932, 1.346], mean_best_reward: --\n",
      " 52815/100000: episode: 1416, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.071 [-0.561, 1.090], mean_best_reward: --\n",
      " 52836/100000: episode: 1417, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.059 [-0.803, 1.141], mean_best_reward: --\n",
      " 52847/100000: episode: 1418, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.114 [-0.958, 1.485], mean_best_reward: --\n",
      " 52879/100000: episode: 1419, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.823, 1.609], mean_best_reward: --\n",
      " 52895/100000: episode: 1420, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.105 [-0.796, 1.472], mean_best_reward: --\n",
      " 52917/100000: episode: 1421, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.091 [-1.490, 0.983], mean_best_reward: --\n",
      " 52945/100000: episode: 1422, duration: 0.212s, episode steps: 28, steps per second: 132, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.447, 1.087], mean_best_reward: --\n",
      " 52964/100000: episode: 1423, duration: 0.138s, episode steps: 19, steps per second: 138, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.078 [-1.219, 2.085], mean_best_reward: --\n",
      " 52982/100000: episode: 1424, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.097 [-0.613, 1.137], mean_best_reward: --\n",
      " 52999/100000: episode: 1425, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.073 [-1.175, 1.990], mean_best_reward: --\n",
      " 53037/100000: episode: 1426, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.054 [-0.998, 1.605], mean_best_reward: --\n",
      " 53096/100000: episode: 1427, duration: 0.358s, episode steps: 59, steps per second: 165, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.044 [-1.121, 0.911], mean_best_reward: --\n",
      " 53122/100000: episode: 1428, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.106 [-0.640, 1.034], mean_best_reward: --\n",
      " 53155/100000: episode: 1429, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.091 [-0.419, 1.123], mean_best_reward: --\n",
      " 53184/100000: episode: 1430, duration: 0.192s, episode steps: 29, steps per second: 151, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.083 [-1.585, 0.801], mean_best_reward: --\n",
      " 53234/100000: episode: 1431, duration: 0.307s, episode steps: 50, steps per second: 163, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.119 [-1.660, 0.522], mean_best_reward: --\n",
      " 53257/100000: episode: 1432, duration: 0.143s, episode steps: 23, steps per second: 161, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.098 [-1.113, 0.557], mean_best_reward: --\n",
      " 53289/100000: episode: 1433, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.639, 0.986], mean_best_reward: --\n",
      " 53313/100000: episode: 1434, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.053, 0.587], mean_best_reward: --\n",
      " 53328/100000: episode: 1435, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.116 [-1.353, 0.804], mean_best_reward: --\n",
      " 53359/100000: episode: 1436, duration: 0.144s, episode steps: 31, steps per second: 215, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.032 [-1.599, 1.189], mean_best_reward: --\n",
      " 53372/100000: episode: 1437, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.604, 1.325], mean_best_reward: --\n",
      " 53418/100000: episode: 1438, duration: 0.217s, episode steps: 46, steps per second: 212, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.122 [-0.391, 1.000], mean_best_reward: --\n",
      " 53541/100000: episode: 1439, duration: 0.586s, episode steps: 123, steps per second: 210, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.120 [-0.882, 1.763], mean_best_reward: --\n",
      " 53581/100000: episode: 1440, duration: 0.188s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.060 [-0.893, 0.607], mean_best_reward: --\n",
      " 53651/100000: episode: 1441, duration: 0.351s, episode steps: 70, steps per second: 200, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.125 [-0.712, 1.100], mean_best_reward: --\n",
      " 53683/100000: episode: 1442, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.113 [-0.904, 0.609], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53704/100000: episode: 1443, duration: 0.157s, episode steps: 21, steps per second: 134, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.095 [-0.959, 1.869], mean_best_reward: --\n",
      " 53721/100000: episode: 1444, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.114 [-0.551, 1.038], mean_best_reward: --\n",
      " 53786/100000: episode: 1445, duration: 0.390s, episode steps: 65, steps per second: 167, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.126 [-0.918, 0.563], mean_best_reward: --\n",
      " 53868/100000: episode: 1446, duration: 0.457s, episode steps: 82, steps per second: 179, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.940, 1.372], mean_best_reward: --\n",
      " 53900/100000: episode: 1447, duration: 0.216s, episode steps: 32, steps per second: 148, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.998, 0.386], mean_best_reward: --\n",
      " 53920/100000: episode: 1448, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.586, 1.175], mean_best_reward: --\n",
      " 53961/100000: episode: 1449, duration: 0.187s, episode steps: 41, steps per second: 219, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.006 [-0.940, 1.229], mean_best_reward: --\n",
      " 53982/100000: episode: 1450, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.066 [-1.147, 0.789], mean_best_reward: --\n",
      " 54030/100000: episode: 1451, duration: 0.219s, episode steps: 48, steps per second: 219, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.041 [-0.945, 1.112], mean_best_reward: 116.000000\n",
      " 54132/100000: episode: 1452, duration: 0.477s, episode steps: 102, steps per second: 214, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.058 [-0.737, 1.078], mean_best_reward: --\n",
      " 54149/100000: episode: 1453, duration: 0.080s, episode steps: 17, steps per second: 212, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.090 [-1.374, 0.754], mean_best_reward: --\n",
      " 54176/100000: episode: 1454, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.088 [-0.554, 1.121], mean_best_reward: --\n",
      " 54203/100000: episode: 1455, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.083 [-1.225, 0.631], mean_best_reward: --\n",
      " 54263/100000: episode: 1456, duration: 0.277s, episode steps: 60, steps per second: 217, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.150 [-1.125, 1.141], mean_best_reward: --\n",
      " 54291/100000: episode: 1457, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.082 [-0.609, 1.015], mean_best_reward: --\n",
      " 54368/100000: episode: 1458, duration: 0.349s, episode steps: 77, steps per second: 221, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.192 [-1.524, 1.023], mean_best_reward: --\n",
      " 54494/100000: episode: 1459, duration: 0.564s, episode steps: 126, steps per second: 223, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.070 [-1.227, 0.733], mean_best_reward: --\n",
      " 54525/100000: episode: 1460, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.048 [-1.464, 0.622], mean_best_reward: --\n",
      " 54563/100000: episode: 1461, duration: 0.184s, episode steps: 38, steps per second: 207, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.091 [-0.765, 1.181], mean_best_reward: --\n",
      " 54637/100000: episode: 1462, duration: 0.382s, episode steps: 74, steps per second: 194, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.011 [-1.333, 0.729], mean_best_reward: --\n",
      " 54677/100000: episode: 1463, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.537, 1.089], mean_best_reward: --\n",
      " 54704/100000: episode: 1464, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.031 [-1.443, 0.970], mean_best_reward: --\n",
      " 54729/100000: episode: 1465, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.096 [-1.722, 0.755], mean_best_reward: --\n",
      " 54788/100000: episode: 1466, duration: 0.265s, episode steps: 59, steps per second: 222, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.036 [-0.754, 0.942], mean_best_reward: --\n",
      " 54813/100000: episode: 1467, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.133 [-0.543, 0.948], mean_best_reward: --\n",
      " 54903/100000: episode: 1468, duration: 0.425s, episode steps: 90, steps per second: 212, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.163 [-0.918, 1.302], mean_best_reward: --\n",
      " 54963/100000: episode: 1469, duration: 0.286s, episode steps: 60, steps per second: 210, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.121 [-0.564, 0.843], mean_best_reward: --\n",
      " 54980/100000: episode: 1470, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.089 [-1.835, 0.949], mean_best_reward: --\n",
      " 55006/100000: episode: 1471, duration: 0.118s, episode steps: 26, steps per second: 220, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.932, 1.528], mean_best_reward: --\n",
      " 55052/100000: episode: 1472, duration: 0.275s, episode steps: 46, steps per second: 167, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.240, 0.783], mean_best_reward: --\n",
      " 55080/100000: episode: 1473, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.066 [-0.650, 1.434], mean_best_reward: --\n",
      " 55108/100000: episode: 1474, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.100 [-0.388, 0.867], mean_best_reward: --\n",
      " 55131/100000: episode: 1475, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.088 [-1.688, 0.802], mean_best_reward: --\n",
      " 55152/100000: episode: 1476, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.111 [-0.946, 1.918], mean_best_reward: --\n",
      " 55175/100000: episode: 1477, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.111 [-1.526, 0.573], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55240/100000: episode: 1478, duration: 0.336s, episode steps: 65, steps per second: 194, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.207 [-1.445, 0.589], mean_best_reward: --\n",
      " 55268/100000: episode: 1479, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.120, 0.824], mean_best_reward: --\n",
      " 55327/100000: episode: 1480, duration: 0.283s, episode steps: 59, steps per second: 209, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.093 [-0.625, 1.484], mean_best_reward: --\n",
      " 55369/100000: episode: 1481, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.058 [-0.955, 1.576], mean_best_reward: --\n",
      " 55433/100000: episode: 1482, duration: 0.322s, episode steps: 64, steps per second: 199, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.216, 0.537], mean_best_reward: --\n",
      " 55478/100000: episode: 1483, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.086 [-0.892, 1.331], mean_best_reward: --\n",
      " 55542/100000: episode: 1484, duration: 0.308s, episode steps: 64, steps per second: 208, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.123 [-1.031, 1.310], mean_best_reward: --\n",
      " 55607/100000: episode: 1485, duration: 0.312s, episode steps: 65, steps per second: 209, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.099 [-1.075, 0.733], mean_best_reward: --\n",
      " 55651/100000: episode: 1486, duration: 0.201s, episode steps: 44, steps per second: 218, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.125 [-0.552, 0.890], mean_best_reward: --\n",
      " 55680/100000: episode: 1487, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.012 [-1.851, 1.348], mean_best_reward: --\n",
      " 55749/100000: episode: 1488, duration: 0.324s, episode steps: 69, steps per second: 213, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.031 [-1.718, 0.839], mean_best_reward: --\n",
      " 55763/100000: episode: 1489, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.078 [-1.154, 1.736], mean_best_reward: --\n",
      " 55783/100000: episode: 1490, duration: 0.093s, episode steps: 20, steps per second: 216, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.553, 1.002], mean_best_reward: --\n",
      " 55848/100000: episode: 1491, duration: 0.320s, episode steps: 65, steps per second: 203, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.055 [-1.048, 0.598], mean_best_reward: --\n",
      " 55882/100000: episode: 1492, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.052 [-1.070, 1.270], mean_best_reward: --\n",
      " 55914/100000: episode: 1493, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.125 [-1.068, 0.369], mean_best_reward: --\n",
      " 55953/100000: episode: 1494, duration: 0.188s, episode steps: 39, steps per second: 207, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.018 [-0.955, 1.246], mean_best_reward: --\n",
      " 55972/100000: episode: 1495, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.066 [-1.153, 1.925], mean_best_reward: --\n",
      " 55988/100000: episode: 1496, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.102 [-1.498, 0.808], mean_best_reward: --\n",
      " 56038/100000: episode: 1497, duration: 0.238s, episode steps: 50, steps per second: 210, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-1.255, 0.643], mean_best_reward: --\n",
      " 56055/100000: episode: 1498, duration: 0.082s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.082 [-0.786, 1.202], mean_best_reward: --\n",
      " 56080/100000: episode: 1499, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.111 [-1.013, 0.399], mean_best_reward: --\n",
      " 56097/100000: episode: 1500, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.084 [-1.339, 0.836], mean_best_reward: --\n",
      " 56163/100000: episode: 1501, duration: 0.312s, episode steps: 66, steps per second: 211, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.220 [-1.294, 0.607], mean_best_reward: 95.000000\n",
      " 56190/100000: episode: 1502, duration: 0.125s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.072 [-1.170, 0.564], mean_best_reward: --\n",
      " 56211/100000: episode: 1503, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.121 [-1.042, 0.552], mean_best_reward: --\n",
      " 56242/100000: episode: 1504, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.071 [-1.310, 0.789], mean_best_reward: --\n",
      " 56263/100000: episode: 1505, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.052 [-0.816, 1.187], mean_best_reward: --\n",
      " 56300/100000: episode: 1506, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.085 [-1.383, 0.585], mean_best_reward: --\n",
      " 56316/100000: episode: 1507, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.050 [-1.021, 1.423], mean_best_reward: --\n",
      " 56366/100000: episode: 1508, duration: 0.281s, episode steps: 50, steps per second: 178, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.194 [-0.950, 1.420], mean_best_reward: --\n",
      " 56411/100000: episode: 1509, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.053 [-1.164, 1.168], mean_best_reward: --\n",
      " 56435/100000: episode: 1510, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-1.362, 0.800], mean_best_reward: --\n",
      " 56472/100000: episode: 1511, duration: 0.177s, episode steps: 37, steps per second: 209, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.081 [-1.337, 0.565], mean_best_reward: --\n",
      " 56492/100000: episode: 1512, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.881, 0.413], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56520/100000: episode: 1513, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.095 [-0.448, 1.318], mean_best_reward: --\n",
      " 56546/100000: episode: 1514, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-0.998, 0.464], mean_best_reward: --\n",
      " 56572/100000: episode: 1515, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.210, 0.419], mean_best_reward: --\n",
      " 56607/100000: episode: 1516, duration: 0.214s, episode steps: 35, steps per second: 163, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.076 [-0.853, 1.760], mean_best_reward: --\n",
      " 56626/100000: episode: 1517, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.097 [-1.225, 0.605], mean_best_reward: --\n",
      " 56657/100000: episode: 1518, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.118 [-0.576, 0.975], mean_best_reward: --\n",
      " 56689/100000: episode: 1519, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.085 [-0.775, 1.660], mean_best_reward: --\n",
      " 56708/100000: episode: 1520, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.090 [-0.769, 1.275], mean_best_reward: --\n",
      " 56731/100000: episode: 1521, duration: 0.109s, episode steps: 23, steps per second: 210, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.089 [-1.584, 0.936], mean_best_reward: --\n",
      " 56744/100000: episode: 1522, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.136 [-2.309, 1.327], mean_best_reward: --\n",
      " 56788/100000: episode: 1523, duration: 0.261s, episode steps: 44, steps per second: 169, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.031 [-1.680, 0.936], mean_best_reward: --\n",
      " 56803/100000: episode: 1524, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.101 [-1.572, 0.943], mean_best_reward: --\n",
      " 56845/100000: episode: 1525, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.391, 1.021], mean_best_reward: --\n",
      " 56916/100000: episode: 1526, duration: 0.414s, episode steps: 71, steps per second: 171, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.121 [-1.059, 0.873], mean_best_reward: --\n",
      " 56931/100000: episode: 1527, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.098 [-1.384, 0.781], mean_best_reward: --\n",
      " 56983/100000: episode: 1528, duration: 0.274s, episode steps: 52, steps per second: 190, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.001 [-1.361, 0.803], mean_best_reward: --\n",
      " 57023/100000: episode: 1529, duration: 0.187s, episode steps: 40, steps per second: 214, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.102 [-1.050, 0.551], mean_best_reward: --\n",
      " 57047/100000: episode: 1530, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.097 [-0.732, 0.437], mean_best_reward: --\n",
      " 57117/100000: episode: 1531, duration: 0.326s, episode steps: 70, steps per second: 215, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.024 [-0.822, 1.557], mean_best_reward: --\n",
      " 57162/100000: episode: 1532, duration: 0.210s, episode steps: 45, steps per second: 214, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.137 [-0.882, 0.421], mean_best_reward: --\n",
      " 57242/100000: episode: 1533, duration: 0.366s, episode steps: 80, steps per second: 219, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.220, 0.833], mean_best_reward: --\n",
      " 57289/100000: episode: 1534, duration: 0.224s, episode steps: 47, steps per second: 210, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.055 [-0.904, 1.496], mean_best_reward: --\n",
      " 57386/100000: episode: 1535, duration: 0.437s, episode steps: 97, steps per second: 222, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.023 [-0.939, 0.782], mean_best_reward: --\n",
      " 57414/100000: episode: 1536, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.065 [-0.735, 1.172], mean_best_reward: --\n",
      " 57442/100000: episode: 1537, duration: 0.131s, episode steps: 28, steps per second: 213, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.480, 0.658], mean_best_reward: --\n",
      " 57499/100000: episode: 1538, duration: 0.264s, episode steps: 57, steps per second: 216, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.014 [-1.133, 0.925], mean_best_reward: --\n",
      " 57541/100000: episode: 1539, duration: 0.190s, episode steps: 42, steps per second: 221, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.025 [-2.264, 1.561], mean_best_reward: --\n",
      " 57577/100000: episode: 1540, duration: 0.167s, episode steps: 36, steps per second: 216, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.114 [-0.918, 0.516], mean_best_reward: --\n",
      " 57593/100000: episode: 1541, duration: 0.076s, episode steps: 16, steps per second: 210, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.111 [-1.111, 0.574], mean_best_reward: --\n",
      " 57617/100000: episode: 1542, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.306, 0.750], mean_best_reward: --\n",
      " 57633/100000: episode: 1543, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-0.945, 0.610], mean_best_reward: --\n",
      " 57687/100000: episode: 1544, duration: 0.247s, episode steps: 54, steps per second: 219, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.023 [-1.142, 0.854], mean_best_reward: --\n",
      " 57709/100000: episode: 1545, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.088 [-0.968, 1.407], mean_best_reward: --\n",
      " 57757/100000: episode: 1546, duration: 0.220s, episode steps: 48, steps per second: 218, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.038 [-1.172, 1.388], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57857/100000: episode: 1547, duration: 0.447s, episode steps: 100, steps per second: 224, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.067 [-1.151, 0.771], mean_best_reward: --\n",
      " 57922/100000: episode: 1548, duration: 0.299s, episode steps: 65, steps per second: 217, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.141 [-1.173, 0.507], mean_best_reward: --\n",
      " 57935/100000: episode: 1549, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.092 [-0.803, 1.345], mean_best_reward: --\n",
      " 57988/100000: episode: 1550, duration: 0.241s, episode steps: 53, steps per second: 220, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.095 [-1.542, 1.457], mean_best_reward: --\n",
      " 58017/100000: episode: 1551, duration: 0.134s, episode steps: 29, steps per second: 216, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.113 [-0.967, 0.389], mean_best_reward: 88.500000\n",
      " 58062/100000: episode: 1552, duration: 0.205s, episode steps: 45, steps per second: 220, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.115 [-1.501, 0.796], mean_best_reward: --\n",
      " 58106/100000: episode: 1553, duration: 0.211s, episode steps: 44, steps per second: 209, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.130 [-0.823, 0.676], mean_best_reward: --\n",
      " 58140/100000: episode: 1554, duration: 0.160s, episode steps: 34, steps per second: 212, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.018 [-1.161, 1.671], mean_best_reward: --\n",
      " 58158/100000: episode: 1555, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.070 [-0.765, 1.315], mean_best_reward: --\n",
      " 58192/100000: episode: 1556, duration: 0.154s, episode steps: 34, steps per second: 220, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.059 [-1.145, 0.783], mean_best_reward: --\n",
      " 58224/100000: episode: 1557, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.045 [-1.324, 0.829], mean_best_reward: --\n",
      " 58252/100000: episode: 1558, duration: 0.337s, episode steps: 28, steps per second: 83, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.221, 0.572], mean_best_reward: --\n",
      " 58292/100000: episode: 1559, duration: 0.191s, episode steps: 40, steps per second: 210, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.115 [-0.926, 0.498], mean_best_reward: --\n",
      " 58303/100000: episode: 1560, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.115 [-1.521, 1.025], mean_best_reward: --\n",
      " 58356/100000: episode: 1561, duration: 0.292s, episode steps: 53, steps per second: 182, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.081 [-1.223, 0.589], mean_best_reward: --\n",
      " 58412/100000: episode: 1562, duration: 0.282s, episode steps: 56, steps per second: 198, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.162 [-0.705, 1.121], mean_best_reward: --\n",
      " 58442/100000: episode: 1563, duration: 0.204s, episode steps: 30, steps per second: 147, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.128 [-0.586, 0.962], mean_best_reward: --\n",
      " 58470/100000: episode: 1564, duration: 0.140s, episode steps: 28, steps per second: 201, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.176, 0.804], mean_best_reward: --\n",
      " 58553/100000: episode: 1565, duration: 0.438s, episode steps: 83, steps per second: 190, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.030 [-1.810, 0.790], mean_best_reward: --\n",
      " 58585/100000: episode: 1566, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-0.881, 0.407], mean_best_reward: --\n",
      " 58600/100000: episode: 1567, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.110 [-0.998, 0.633], mean_best_reward: --\n",
      " 58666/100000: episode: 1568, duration: 0.346s, episode steps: 66, steps per second: 191, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-1.142, 0.984], mean_best_reward: --\n",
      " 58696/100000: episode: 1569, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.066 [-0.974, 1.679], mean_best_reward: --\n",
      " 58787/100000: episode: 1570, duration: 0.494s, episode steps: 91, steps per second: 184, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.083 [-0.797, 1.116], mean_best_reward: --\n",
      " 58817/100000: episode: 1571, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.036 [-0.928, 1.468], mean_best_reward: --\n",
      " 58844/100000: episode: 1572, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.086 [-1.407, 0.579], mean_best_reward: --\n",
      " 58931/100000: episode: 1573, duration: 0.493s, episode steps: 87, steps per second: 177, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.042 [-1.502, 1.514], mean_best_reward: --\n",
      " 58975/100000: episode: 1574, duration: 0.210s, episode steps: 44, steps per second: 210, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.128 [-0.510, 1.633], mean_best_reward: --\n",
      " 59003/100000: episode: 1575, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.060 [-1.259, 0.588], mean_best_reward: --\n",
      " 59033/100000: episode: 1576, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.033 [-0.947, 1.614], mean_best_reward: --\n",
      " 59074/100000: episode: 1577, duration: 0.195s, episode steps: 41, steps per second: 210, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.091 [-0.633, 1.401], mean_best_reward: --\n",
      " 59112/100000: episode: 1578, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.101 [-0.500, 0.936], mean_best_reward: --\n",
      " 59169/100000: episode: 1579, duration: 0.265s, episode steps: 57, steps per second: 215, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.015 [-1.792, 0.759], mean_best_reward: --\n",
      " 59203/100000: episode: 1580, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.006 [-1.376, 1.836], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59227/100000: episode: 1581, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.115 [-1.337, 0.566], mean_best_reward: --\n",
      " 59296/100000: episode: 1582, duration: 0.330s, episode steps: 69, steps per second: 209, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.003 [-1.526, 1.003], mean_best_reward: --\n",
      " 59314/100000: episode: 1583, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.200, 0.807], mean_best_reward: --\n",
      " 59342/100000: episode: 1584, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.126 [-1.472, 0.399], mean_best_reward: --\n",
      " 59359/100000: episode: 1585, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.092 [-1.184, 0.800], mean_best_reward: --\n",
      " 59398/100000: episode: 1586, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.070 [-0.936, 0.546], mean_best_reward: --\n",
      " 59453/100000: episode: 1587, duration: 0.259s, episode steps: 55, steps per second: 212, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.127 [-0.545, 0.986], mean_best_reward: --\n",
      " 59493/100000: episode: 1588, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.598, 1.136], mean_best_reward: --\n",
      " 59542/100000: episode: 1589, duration: 0.228s, episode steps: 49, steps per second: 215, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.149 [-1.127, 0.473], mean_best_reward: --\n",
      " 59562/100000: episode: 1590, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.142 [-1.041, 0.545], mean_best_reward: --\n",
      " 59607/100000: episode: 1591, duration: 0.216s, episode steps: 45, steps per second: 209, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.060 [-0.623, 0.898], mean_best_reward: --\n",
      " 59628/100000: episode: 1592, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.111 [-0.920, 0.408], mean_best_reward: --\n",
      " 59710/100000: episode: 1593, duration: 0.393s, episode steps: 82, steps per second: 209, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.102 [-0.906, 0.887], mean_best_reward: --\n",
      " 59730/100000: episode: 1594, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-0.984, 0.591], mean_best_reward: --\n",
      " 59756/100000: episode: 1595, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.126, 0.599], mean_best_reward: --\n",
      " 59785/100000: episode: 1596, duration: 0.136s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.054 [-1.619, 1.147], mean_best_reward: --\n",
      " 59813/100000: episode: 1597, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.195, 0.769], mean_best_reward: --\n",
      " 59841/100000: episode: 1598, duration: 0.136s, episode steps: 28, steps per second: 205, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.038 [-0.837, 0.595], mean_best_reward: --\n",
      " 59920/100000: episode: 1599, duration: 0.528s, episode steps: 79, steps per second: 150, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.052 [-0.879, 0.814], mean_best_reward: --\n",
      " 59960/100000: episode: 1600, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.130 [-0.911, 0.499], mean_best_reward: --\n",
      " 59987/100000: episode: 1601, duration: 0.202s, episode steps: 27, steps per second: 134, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.051 [-1.460, 0.647], mean_best_reward: 133.500000\n",
      " 60011/100000: episode: 1602, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.076 [-1.664, 0.784], mean_best_reward: --\n",
      " 60061/100000: episode: 1603, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.045 [-0.986, 1.469], mean_best_reward: --\n",
      " 60106/100000: episode: 1604, duration: 0.297s, episode steps: 45, steps per second: 151, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.017 [-1.463, 0.998], mean_best_reward: --\n",
      " 60118/100000: episode: 1605, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.959, 1.606], mean_best_reward: --\n",
      " 60154/100000: episode: 1606, duration: 0.236s, episode steps: 36, steps per second: 153, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.920, 0.575], mean_best_reward: --\n",
      " 60186/100000: episode: 1607, duration: 0.216s, episode steps: 32, steps per second: 148, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.133, 0.599], mean_best_reward: --\n",
      " 60220/100000: episode: 1608, duration: 0.233s, episode steps: 34, steps per second: 146, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-1.205, 0.828], mean_best_reward: --\n",
      " 60271/100000: episode: 1609, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.016 [-0.896, 1.158], mean_best_reward: --\n",
      " 60330/100000: episode: 1610, duration: 0.379s, episode steps: 59, steps per second: 156, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.111 [-1.178, 0.611], mean_best_reward: --\n",
      " 60417/100000: episode: 1611, duration: 0.507s, episode steps: 87, steps per second: 172, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.177 [-0.773, 1.923], mean_best_reward: --\n",
      " 60439/100000: episode: 1612, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.108 [-0.745, 1.226], mean_best_reward: --\n",
      " 60454/100000: episode: 1613, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.114 [-0.562, 1.224], mean_best_reward: --\n",
      " 60503/100000: episode: 1614, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.132 [-1.534, 0.949], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60531/100000: episode: 1615, duration: 0.207s, episode steps: 28, steps per second: 135, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.949, 0.445], mean_best_reward: --\n",
      " 60553/100000: episode: 1616, duration: 0.158s, episode steps: 22, steps per second: 139, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.101 [-1.683, 0.782], mean_best_reward: --\n",
      " 60601/100000: episode: 1617, duration: 0.352s, episode steps: 48, steps per second: 136, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.195, 0.774], mean_best_reward: --\n",
      " 60655/100000: episode: 1618, duration: 0.375s, episode steps: 54, steps per second: 144, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.200 [-0.579, 1.271], mean_best_reward: --\n",
      " 60677/100000: episode: 1619, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.112 [-1.361, 0.744], mean_best_reward: --\n",
      " 60747/100000: episode: 1620, duration: 0.436s, episode steps: 70, steps per second: 161, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.052 [-0.677, 0.839], mean_best_reward: --\n",
      " 60787/100000: episode: 1621, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.045 [-1.224, 0.763], mean_best_reward: --\n",
      " 60816/100000: episode: 1622, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.060 [-0.951, 1.731], mean_best_reward: --\n",
      " 60854/100000: episode: 1623, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.155 [-0.966, 0.371], mean_best_reward: --\n",
      " 60903/100000: episode: 1624, duration: 0.281s, episode steps: 49, steps per second: 174, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.046 [-1.534, 1.195], mean_best_reward: --\n",
      " 60934/100000: episode: 1625, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.042 [-1.900, 0.993], mean_best_reward: --\n",
      " 60961/100000: episode: 1626, duration: 0.177s, episode steps: 27, steps per second: 153, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.062 [-1.568, 0.820], mean_best_reward: --\n",
      " 60988/100000: episode: 1627, duration: 0.160s, episode steps: 27, steps per second: 168, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.146 [-0.560, 1.197], mean_best_reward: --\n",
      " 61011/100000: episode: 1628, duration: 0.159s, episode steps: 23, steps per second: 145, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.092 [-1.163, 0.638], mean_best_reward: --\n",
      " 61046/100000: episode: 1629, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.119 [-1.084, 0.634], mean_best_reward: --\n",
      " 61073/100000: episode: 1630, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.058 [-1.051, 0.775], mean_best_reward: --\n",
      " 61097/100000: episode: 1631, duration: 0.129s, episode steps: 24, steps per second: 185, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.119 [-1.044, 0.567], mean_best_reward: --\n",
      " 61132/100000: episode: 1632, duration: 0.216s, episode steps: 35, steps per second: 162, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.139 [-1.032, 0.424], mean_best_reward: --\n",
      " 61173/100000: episode: 1633, duration: 0.263s, episode steps: 41, steps per second: 156, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.038 [-1.287, 0.991], mean_best_reward: --\n",
      " 61204/100000: episode: 1634, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.151 [-0.589, 0.928], mean_best_reward: --\n",
      " 61270/100000: episode: 1635, duration: 0.395s, episode steps: 66, steps per second: 167, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.155 [-1.263, 1.932], mean_best_reward: --\n",
      " 61340/100000: episode: 1636, duration: 0.393s, episode steps: 70, steps per second: 178, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.018 [-0.725, 0.626], mean_best_reward: --\n",
      " 61393/100000: episode: 1637, duration: 0.388s, episode steps: 53, steps per second: 136, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.081 [-0.917, 0.778], mean_best_reward: --\n",
      " 61405/100000: episode: 1638, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.095 [-0.951, 1.599], mean_best_reward: --\n",
      " 61477/100000: episode: 1639, duration: 0.538s, episode steps: 72, steps per second: 134, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.862, 1.187], mean_best_reward: --\n",
      " 61491/100000: episode: 1640, duration: 0.126s, episode steps: 14, steps per second: 112, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.072 [-1.583, 0.994], mean_best_reward: --\n",
      " 61510/100000: episode: 1641, duration: 0.181s, episode steps: 19, steps per second: 105, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.050 [-1.426, 0.982], mean_best_reward: --\n",
      " 61561/100000: episode: 1642, duration: 0.341s, episode steps: 51, steps per second: 150, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.052 [-0.736, 1.444], mean_best_reward: --\n",
      " 61605/100000: episode: 1643, duration: 0.363s, episode steps: 44, steps per second: 121, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.733, 1.104], mean_best_reward: --\n",
      " 61645/100000: episode: 1644, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-1.242, 0.843], mean_best_reward: --\n",
      " 61735/100000: episode: 1645, duration: 0.517s, episode steps: 90, steps per second: 174, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.203, 0.880], mean_best_reward: --\n",
      " 61752/100000: episode: 1646, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.108 [-1.219, 0.571], mean_best_reward: --\n",
      " 61775/100000: episode: 1647, duration: 0.165s, episode steps: 23, steps per second: 139, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.099 [-0.591, 1.132], mean_best_reward: --\n",
      " 61848/100000: episode: 1648, duration: 0.552s, episode steps: 73, steps per second: 132, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.024 [-0.847, 0.781], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61910/100000: episode: 1649, duration: 0.791s, episode steps: 62, steps per second: 78, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.136 [-1.680, 0.744], mean_best_reward: --\n",
      " 61926/100000: episode: 1650, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.102 [-1.787, 0.945], mean_best_reward: --\n",
      " 61967/100000: episode: 1651, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.031 [-1.287, 0.787], mean_best_reward: 84.500000\n",
      " 61982/100000: episode: 1652, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.128 [-0.548, 1.060], mean_best_reward: --\n",
      " 62040/100000: episode: 1653, duration: 0.318s, episode steps: 58, steps per second: 183, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.908, 1.331], mean_best_reward: --\n",
      " 62103/100000: episode: 1654, duration: 0.308s, episode steps: 63, steps per second: 204, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.011 [-0.817, 0.950], mean_best_reward: --\n",
      " 62124/100000: episode: 1655, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.042 [-1.203, 1.651], mean_best_reward: --\n",
      " 62155/100000: episode: 1656, duration: 0.223s, episode steps: 31, steps per second: 139, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.043 [-1.403, 0.765], mean_best_reward: --\n",
      " 62233/100000: episode: 1657, duration: 0.424s, episode steps: 78, steps per second: 184, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.016 [-0.994, 1.351], mean_best_reward: --\n",
      " 62278/100000: episode: 1658, duration: 0.371s, episode steps: 45, steps per second: 121, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.138 [-0.543, 1.739], mean_best_reward: --\n",
      " 62302/100000: episode: 1659, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-0.819, 0.352], mean_best_reward: --\n",
      " 62335/100000: episode: 1660, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.069 [-0.591, 1.189], mean_best_reward: --\n",
      " 62406/100000: episode: 1661, duration: 0.362s, episode steps: 71, steps per second: 196, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.091 [-0.888, 0.598], mean_best_reward: --\n",
      " 62442/100000: episode: 1662, duration: 0.166s, episode steps: 36, steps per second: 216, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.071 [-0.636, 1.647], mean_best_reward: --\n",
      " 62474/100000: episode: 1663, duration: 0.242s, episode steps: 32, steps per second: 132, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.098 [-0.351, 0.798], mean_best_reward: --\n",
      " 62539/100000: episode: 1664, duration: 0.363s, episode steps: 65, steps per second: 179, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.056 [-0.572, 0.913], mean_best_reward: --\n",
      " 62569/100000: episode: 1665, duration: 0.208s, episode steps: 30, steps per second: 144, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.063 [-1.708, 0.768], mean_best_reward: --\n",
      " 62617/100000: episode: 1666, duration: 0.272s, episode steps: 48, steps per second: 177, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.065 [-0.879, 0.663], mean_best_reward: --\n",
      " 62629/100000: episode: 1667, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.100 [-1.587, 1.001], mean_best_reward: --\n",
      " 62684/100000: episode: 1668, duration: 0.332s, episode steps: 55, steps per second: 166, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.084 [-0.792, 1.016], mean_best_reward: --\n",
      " 62718/100000: episode: 1669, duration: 0.201s, episode steps: 34, steps per second: 169, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.057 [-1.360, 0.943], mean_best_reward: --\n",
      " 62753/100000: episode: 1670, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.139 [-0.857, 0.370], mean_best_reward: --\n",
      " 62796/100000: episode: 1671, duration: 0.243s, episode steps: 43, steps per second: 177, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.148 [-0.941, 0.443], mean_best_reward: --\n",
      " 62853/100000: episode: 1672, duration: 0.300s, episode steps: 57, steps per second: 190, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.079 [-1.588, 1.377], mean_best_reward: --\n",
      " 62868/100000: episode: 1673, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.104 [-0.631, 1.043], mean_best_reward: --\n",
      " 62946/100000: episode: 1674, duration: 0.418s, episode steps: 78, steps per second: 186, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.028 [-0.812, 0.996], mean_best_reward: --\n",
      " 62975/100000: episode: 1675, duration: 0.302s, episode steps: 29, steps per second: 96, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.075 [-0.639, 1.138], mean_best_reward: --\n",
      " 63017/100000: episode: 1676, duration: 0.535s, episode steps: 42, steps per second: 79, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.077 [-0.777, 1.627], mean_best_reward: --\n",
      " 63206/100000: episode: 1677, duration: 1.643s, episode steps: 189, steps per second: 115, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.018 [-1.375, 1.003], mean_best_reward: --\n",
      " 63236/100000: episode: 1678, duration: 0.446s, episode steps: 30, steps per second: 67, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.040 [-0.902, 0.626], mean_best_reward: --\n",
      " 63252/100000: episode: 1679, duration: 0.173s, episode steps: 16, steps per second: 93, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.107 [-1.349, 0.628], mean_best_reward: --\n",
      " 63292/100000: episode: 1680, duration: 0.322s, episode steps: 40, steps per second: 124, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.906, 0.587], mean_best_reward: --\n",
      " 63328/100000: episode: 1681, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.095 [-1.148, 1.414], mean_best_reward: --\n",
      " 63379/100000: episode: 1682, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.077 [-1.124, 1.093], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63425/100000: episode: 1683, duration: 0.225s, episode steps: 46, steps per second: 204, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.019 [-1.110, 1.413], mean_best_reward: --\n",
      " 63466/100000: episode: 1684, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.154 [-0.638, 1.860], mean_best_reward: --\n",
      " 63560/100000: episode: 1685, duration: 0.658s, episode steps: 94, steps per second: 143, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.832, 0.931], mean_best_reward: --\n",
      " 63591/100000: episode: 1686, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.053 [-0.939, 1.293], mean_best_reward: --\n",
      " 63619/100000: episode: 1687, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.764, 1.486], mean_best_reward: --\n",
      " 63647/100000: episode: 1688, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.310, 0.764], mean_best_reward: --\n",
      " 63672/100000: episode: 1689, duration: 0.265s, episode steps: 25, steps per second: 94, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.099 [-0.618, 1.526], mean_best_reward: --\n",
      " 63705/100000: episode: 1690, duration: 0.320s, episode steps: 33, steps per second: 103, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.098 [-0.576, 1.029], mean_best_reward: --\n",
      " 63735/100000: episode: 1691, duration: 0.271s, episode steps: 30, steps per second: 111, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.107 [-0.646, 1.388], mean_best_reward: --\n",
      " 63796/100000: episode: 1692, duration: 0.298s, episode steps: 61, steps per second: 205, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.131 [-0.903, 0.440], mean_best_reward: --\n",
      " 63863/100000: episode: 1693, duration: 0.326s, episode steps: 67, steps per second: 205, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.083 [-1.047, 1.144], mean_best_reward: --\n",
      " 63925/100000: episode: 1694, duration: 0.294s, episode steps: 62, steps per second: 211, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.047 [-1.135, 0.915], mean_best_reward: --\n",
      " 63946/100000: episode: 1695, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.100 [-1.350, 0.559], mean_best_reward: --\n",
      " 63973/100000: episode: 1696, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.072 [-1.005, 0.638], mean_best_reward: --\n",
      " 64002/100000: episode: 1697, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.036 [-0.806, 1.315], mean_best_reward: --\n",
      " 64017/100000: episode: 1698, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-1.167, 0.572], mean_best_reward: --\n",
      " 64050/100000: episode: 1699, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.086 [-1.520, 0.808], mean_best_reward: --\n",
      " 64068/100000: episode: 1700, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.067 [-1.541, 0.805], mean_best_reward: --\n",
      " 64085/100000: episode: 1701, duration: 0.084s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.099 [-0.790, 1.229], mean_best_reward: 85.000000\n",
      " 64112/100000: episode: 1702, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.097 [-0.638, 1.568], mean_best_reward: --\n",
      " 64173/100000: episode: 1703, duration: 0.319s, episode steps: 61, steps per second: 191, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.189 [-1.257, 0.646], mean_best_reward: --\n",
      " 64185/100000: episode: 1704, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.366, 0.829], mean_best_reward: --\n",
      " 64218/100000: episode: 1705, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.071 [-1.129, 0.802], mean_best_reward: --\n",
      " 64261/100000: episode: 1706, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.124 [-0.740, 1.329], mean_best_reward: --\n",
      " 64278/100000: episode: 1707, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.086 [-1.120, 0.640], mean_best_reward: --\n",
      " 64317/100000: episode: 1708, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.082 [-0.983, 2.220], mean_best_reward: --\n",
      " 64339/100000: episode: 1709, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.056 [-1.185, 2.025], mean_best_reward: --\n",
      " 64391/100000: episode: 1710, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.118 [-0.593, 0.969], mean_best_reward: --\n",
      " 64409/100000: episode: 1711, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.067 [-0.794, 1.433], mean_best_reward: --\n",
      " 64421/100000: episode: 1712, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.825, 1.518], mean_best_reward: --\n",
      " 64466/100000: episode: 1713, duration: 0.214s, episode steps: 45, steps per second: 210, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.036 [-0.750, 1.026], mean_best_reward: --\n",
      " 64483/100000: episode: 1714, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.100 [-1.215, 0.577], mean_best_reward: --\n",
      " 64539/100000: episode: 1715, duration: 0.291s, episode steps: 56, steps per second: 193, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.212 [-0.875, 1.801], mean_best_reward: --\n",
      " 64592/100000: episode: 1716, duration: 0.292s, episode steps: 53, steps per second: 181, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.075 [-1.860, 0.759], mean_best_reward: --\n",
      " 64611/100000: episode: 1717, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.106 [-1.425, 0.617], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64663/100000: episode: 1718, duration: 0.358s, episode steps: 52, steps per second: 145, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.040 [-0.635, 1.096], mean_best_reward: --\n",
      " 64737/100000: episode: 1719, duration: 0.404s, episode steps: 74, steps per second: 183, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.194 [-0.516, 1.108], mean_best_reward: --\n",
      " 64754/100000: episode: 1720, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.091 [-1.396, 0.792], mean_best_reward: --\n",
      " 64779/100000: episode: 1721, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.120 [-1.251, 0.763], mean_best_reward: --\n",
      " 64803/100000: episode: 1722, duration: 0.116s, episode steps: 24, steps per second: 206, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.129 [-0.979, 0.576], mean_best_reward: --\n",
      " 64883/100000: episode: 1723, duration: 0.418s, episode steps: 80, steps per second: 192, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.107 [-1.046, 1.021], mean_best_reward: --\n",
      " 64912/100000: episode: 1724, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.123 [-0.854, 0.382], mean_best_reward: --\n",
      " 64961/100000: episode: 1725, duration: 0.253s, episode steps: 49, steps per second: 193, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.067 [-1.096, 0.434], mean_best_reward: --\n",
      " 64987/100000: episode: 1726, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.128 [-1.065, 0.608], mean_best_reward: --\n",
      " 65031/100000: episode: 1727, duration: 0.217s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.139 [-0.480, 1.036], mean_best_reward: --\n",
      " 65079/100000: episode: 1728, duration: 0.270s, episode steps: 48, steps per second: 177, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.037 [-0.785, 1.619], mean_best_reward: --\n",
      " 65146/100000: episode: 1729, duration: 0.356s, episode steps: 67, steps per second: 188, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.093 [-0.884, 1.877], mean_best_reward: --\n",
      " 65178/100000: episode: 1730, duration: 0.157s, episode steps: 32, steps per second: 203, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.124 [-1.627, 0.568], mean_best_reward: --\n",
      " 65224/100000: episode: 1731, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.017 [-1.932, 1.360], mean_best_reward: --\n",
      " 65251/100000: episode: 1732, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.097 [-1.184, 0.566], mean_best_reward: --\n",
      " 65309/100000: episode: 1733, duration: 0.336s, episode steps: 58, steps per second: 173, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.570, 1.079], mean_best_reward: --\n",
      " 65328/100000: episode: 1734, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.104 [-0.543, 1.057], mean_best_reward: --\n",
      " 65433/100000: episode: 1735, duration: 0.503s, episode steps: 105, steps per second: 209, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.007 [-1.002, 0.948], mean_best_reward: --\n",
      " 65476/100000: episode: 1736, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.120 [-0.381, 1.205], mean_best_reward: --\n",
      " 65498/100000: episode: 1737, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.065 [-1.304, 0.779], mean_best_reward: --\n",
      " 65560/100000: episode: 1738, duration: 0.514s, episode steps: 62, steps per second: 121, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.183 [-0.642, 1.643], mean_best_reward: --\n",
      " 65582/100000: episode: 1739, duration: 0.153s, episode steps: 22, steps per second: 144, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.088 [-1.848, 0.941], mean_best_reward: --\n",
      " 65666/100000: episode: 1740, duration: 0.462s, episode steps: 84, steps per second: 182, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.645, 1.105], mean_best_reward: --\n",
      " 65703/100000: episode: 1741, duration: 0.171s, episode steps: 37, steps per second: 216, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.113 [-0.709, 1.804], mean_best_reward: --\n",
      " 65735/100000: episode: 1742, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.152 [-0.762, 0.382], mean_best_reward: --\n",
      " 65777/100000: episode: 1743, duration: 0.324s, episode steps: 42, steps per second: 130, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.123 [-1.034, 0.424], mean_best_reward: --\n",
      " 65812/100000: episode: 1744, duration: 0.219s, episode steps: 35, steps per second: 160, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.025 [-1.547, 1.191], mean_best_reward: --\n",
      " 65848/100000: episode: 1745, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.779, 1.205], mean_best_reward: --\n",
      " 65870/100000: episode: 1746, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.552, 1.233], mean_best_reward: --\n",
      " 65887/100000: episode: 1747, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.097 [-1.457, 0.778], mean_best_reward: --\n",
      " 65918/100000: episode: 1748, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.160 [-0.605, 1.010], mean_best_reward: --\n",
      " 65976/100000: episode: 1749, duration: 0.316s, episode steps: 58, steps per second: 184, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.392, 1.016], mean_best_reward: --\n",
      " 66013/100000: episode: 1750, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.105 [-1.274, 0.791], mean_best_reward: --\n",
      " 66073/100000: episode: 1751, duration: 0.311s, episode steps: 60, steps per second: 193, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.436, 1.021], mean_best_reward: 109.500000\n",
      " 66085/100000: episode: 1752, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.971, 1.507], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66126/100000: episode: 1753, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.052 [-0.636, 0.927], mean_best_reward: --\n",
      " 66157/100000: episode: 1754, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.104 [-0.871, 0.415], mean_best_reward: --\n",
      " 66173/100000: episode: 1755, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.108 [-0.578, 1.243], mean_best_reward: --\n",
      " 66212/100000: episode: 1756, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.119 [-1.108, 1.149], mean_best_reward: --\n",
      " 66257/100000: episode: 1757, duration: 0.260s, episode steps: 45, steps per second: 173, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.114 [-0.828, 1.528], mean_best_reward: --\n",
      " 66271/100000: episode: 1758, duration: 0.103s, episode steps: 14, steps per second: 136, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.096 [-1.970, 1.154], mean_best_reward: --\n",
      " 66288/100000: episode: 1759, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.083 [-1.437, 1.017], mean_best_reward: --\n",
      " 66326/100000: episode: 1760, duration: 0.238s, episode steps: 38, steps per second: 160, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.058, 0.544], mean_best_reward: --\n",
      " 66344/100000: episode: 1761, duration: 0.127s, episode steps: 18, steps per second: 142, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.091 [-1.136, 0.617], mean_best_reward: --\n",
      " 66381/100000: episode: 1762, duration: 0.231s, episode steps: 37, steps per second: 160, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.080 [-1.052, 0.854], mean_best_reward: --\n",
      " 66424/100000: episode: 1763, duration: 0.266s, episode steps: 43, steps per second: 162, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.040 [-2.068, 1.695], mean_best_reward: --\n",
      " 66450/100000: episode: 1764, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.082 [-0.832, 1.806], mean_best_reward: --\n",
      " 66470/100000: episode: 1765, duration: 0.135s, episode steps: 20, steps per second: 149, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.629, 1.293], mean_best_reward: --\n",
      " 66564/100000: episode: 1766, duration: 0.592s, episode steps: 94, steps per second: 159, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.250 [-0.766, 1.316], mean_best_reward: --\n",
      " 66601/100000: episode: 1767, duration: 0.433s, episode steps: 37, steps per second: 85, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.121 [-1.014, 0.505], mean_best_reward: --\n",
      " 66644/100000: episode: 1768, duration: 0.381s, episode steps: 43, steps per second: 113, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.086 [-1.747, 0.649], mean_best_reward: --\n",
      " 66670/100000: episode: 1769, duration: 0.169s, episode steps: 26, steps per second: 154, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.183, 0.829], mean_best_reward: --\n",
      " 66693/100000: episode: 1770, duration: 0.150s, episode steps: 23, steps per second: 153, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.116 [-0.994, 0.550], mean_best_reward: --\n",
      " 66721/100000: episode: 1771, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.048 [-0.827, 1.310], mean_best_reward: --\n",
      " 66765/100000: episode: 1772, duration: 0.328s, episode steps: 44, steps per second: 134, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.445, 0.642], mean_best_reward: --\n",
      " 66778/100000: episode: 1773, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.077 [-1.403, 2.081], mean_best_reward: --\n",
      " 66817/100000: episode: 1774, duration: 0.261s, episode steps: 39, steps per second: 149, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.054 [-1.330, 0.638], mean_best_reward: --\n",
      " 66864/100000: episode: 1775, duration: 0.408s, episode steps: 47, steps per second: 115, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.041 [-1.026, 0.597], mean_best_reward: --\n",
      " 66880/100000: episode: 1776, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.387, 0.773], mean_best_reward: --\n",
      " 66910/100000: episode: 1777, duration: 0.189s, episode steps: 30, steps per second: 159, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-1.512, 1.140], mean_best_reward: --\n",
      " 66978/100000: episode: 1778, duration: 0.344s, episode steps: 68, steps per second: 198, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.101, 0.597], mean_best_reward: --\n",
      " 67019/100000: episode: 1779, duration: 0.231s, episode steps: 41, steps per second: 177, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.096 [-0.571, 1.151], mean_best_reward: --\n",
      " 67048/100000: episode: 1780, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.004 [-1.288, 1.016], mean_best_reward: --\n",
      " 67061/100000: episode: 1781, duration: 0.118s, episode steps: 13, steps per second: 110, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.112 [-1.920, 1.041], mean_best_reward: --\n",
      " 67099/100000: episode: 1782, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.630, 1.124], mean_best_reward: --\n",
      " 67130/100000: episode: 1783, duration: 0.238s, episode steps: 31, steps per second: 130, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.045 [-0.883, 0.636], mean_best_reward: --\n",
      " 67175/100000: episode: 1784, duration: 0.287s, episode steps: 45, steps per second: 157, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.098 [-1.243, 0.328], mean_best_reward: --\n",
      " 67204/100000: episode: 1785, duration: 0.209s, episode steps: 29, steps per second: 139, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.083 [-1.445, 0.644], mean_best_reward: --\n",
      " 67252/100000: episode: 1786, duration: 0.298s, episode steps: 48, steps per second: 161, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-1.041, 0.878], mean_best_reward: --\n",
      " 67276/100000: episode: 1787, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.077 [-1.304, 0.806], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67297/100000: episode: 1788, duration: 0.135s, episode steps: 21, steps per second: 156, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.078 [-1.057, 0.585], mean_best_reward: --\n",
      " 67359/100000: episode: 1789, duration: 0.368s, episode steps: 62, steps per second: 168, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.671, 1.077], mean_best_reward: --\n",
      " 67389/100000: episode: 1790, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.061 [-1.227, 0.649], mean_best_reward: --\n",
      " 67430/100000: episode: 1791, duration: 0.196s, episode steps: 41, steps per second: 209, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.105 [-1.382, 0.507], mean_best_reward: --\n",
      " 67470/100000: episode: 1792, duration: 0.243s, episode steps: 40, steps per second: 165, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.147 [-1.423, 0.563], mean_best_reward: --\n",
      " 67483/100000: episode: 1793, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.103 [-1.680, 1.141], mean_best_reward: --\n",
      " 67572/100000: episode: 1794, duration: 0.508s, episode steps: 89, steps per second: 175, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.222 [-1.494, 1.155], mean_best_reward: --\n",
      " 67592/100000: episode: 1795, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.562, 0.988], mean_best_reward: --\n",
      " 67639/100000: episode: 1796, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.029 [-0.930, 1.312], mean_best_reward: --\n",
      " 67716/100000: episode: 1797, duration: 0.400s, episode steps: 77, steps per second: 192, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.085 [-0.917, 1.140], mean_best_reward: --\n",
      " 67763/100000: episode: 1798, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.121 [-0.417, 1.043], mean_best_reward: --\n",
      " 67777/100000: episode: 1799, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.780, 1.343], mean_best_reward: --\n",
      " 67796/100000: episode: 1800, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.120 [-1.485, 0.583], mean_best_reward: --\n",
      " 67820/100000: episode: 1801, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.122 [-1.640, 0.637], mean_best_reward: 113.000000\n",
      " 67851/100000: episode: 1802, duration: 0.140s, episode steps: 31, steps per second: 221, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.141 [-0.542, 0.953], mean_best_reward: --\n",
      " 67894/100000: episode: 1803, duration: 0.193s, episode steps: 43, steps per second: 223, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.093 [-0.824, 1.223], mean_best_reward: --\n",
      " 67920/100000: episode: 1804, duration: 0.118s, episode steps: 26, steps per second: 221, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.811, 1.196], mean_best_reward: --\n",
      " 67936/100000: episode: 1805, duration: 0.111s, episode steps: 16, steps per second: 144, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.103 [-1.684, 0.947], mean_best_reward: --\n",
      " 67964/100000: episode: 1806, duration: 0.187s, episode steps: 28, steps per second: 150, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.055 [-2.051, 1.141], mean_best_reward: --\n",
      " 67998/100000: episode: 1807, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.745, 1.390], mean_best_reward: --\n",
      " 68055/100000: episode: 1808, duration: 0.300s, episode steps: 57, steps per second: 190, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.211 [-1.294, 0.598], mean_best_reward: --\n",
      " 68081/100000: episode: 1809, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.099 [-1.119, 0.559], mean_best_reward: --\n",
      " 68134/100000: episode: 1810, duration: 0.243s, episode steps: 53, steps per second: 218, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.090 [-0.774, 1.042], mean_best_reward: --\n",
      " 68199/100000: episode: 1811, duration: 0.332s, episode steps: 65, steps per second: 196, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.049 [-1.215, 1.848], mean_best_reward: --\n",
      " 68237/100000: episode: 1812, duration: 0.174s, episode steps: 38, steps per second: 218, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.146 [-0.425, 1.764], mean_best_reward: --\n",
      " 68319/100000: episode: 1813, duration: 0.392s, episode steps: 82, steps per second: 209, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.105 [-0.833, 0.669], mean_best_reward: --\n",
      " 68346/100000: episode: 1814, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.091 [-0.593, 1.288], mean_best_reward: --\n",
      " 68392/100000: episode: 1815, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.639, 1.084], mean_best_reward: --\n",
      " 68412/100000: episode: 1816, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.090 [-0.752, 1.606], mean_best_reward: --\n",
      " 68465/100000: episode: 1817, duration: 0.251s, episode steps: 53, steps per second: 211, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.064 [-0.771, 1.158], mean_best_reward: --\n",
      " 68495/100000: episode: 1818, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.065, 0.623], mean_best_reward: --\n",
      " 68513/100000: episode: 1819, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.077 [-0.956, 1.443], mean_best_reward: --\n",
      " 68545/100000: episode: 1820, duration: 0.143s, episode steps: 32, steps per second: 224, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.821, 1.122], mean_best_reward: --\n",
      " 68577/100000: episode: 1821, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.151 [-0.602, 1.069], mean_best_reward: --\n",
      " 68596/100000: episode: 1822, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.093 [-1.305, 0.744], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68684/100000: episode: 1823, duration: 0.406s, episode steps: 88, steps per second: 217, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.027 [-1.166, 1.271], mean_best_reward: --\n",
      " 68742/100000: episode: 1824, duration: 0.298s, episode steps: 58, steps per second: 194, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.122 [-0.885, 0.439], mean_best_reward: --\n",
      " 68767/100000: episode: 1825, duration: 0.116s, episode steps: 25, steps per second: 215, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.114 [-0.558, 1.190], mean_best_reward: --\n",
      " 68786/100000: episode: 1826, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.077 [-1.485, 0.985], mean_best_reward: --\n",
      " 68812/100000: episode: 1827, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.061 [-1.446, 0.752], mean_best_reward: --\n",
      " 68875/100000: episode: 1828, duration: 0.300s, episode steps: 63, steps per second: 210, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.097 [-0.825, 1.283], mean_best_reward: --\n",
      " 68911/100000: episode: 1829, duration: 0.160s, episode steps: 36, steps per second: 225, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.111 [-0.476, 1.763], mean_best_reward: --\n",
      " 68972/100000: episode: 1830, duration: 0.301s, episode steps: 61, steps per second: 203, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.100 [-0.975, 1.146], mean_best_reward: --\n",
      " 68990/100000: episode: 1831, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.773, 1.264], mean_best_reward: --\n",
      " 69013/100000: episode: 1832, duration: 0.110s, episode steps: 23, steps per second: 210, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.092 [-0.821, 1.215], mean_best_reward: --\n",
      " 69043/100000: episode: 1833, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.542, 1.135], mean_best_reward: --\n",
      " 69091/100000: episode: 1834, duration: 0.229s, episode steps: 48, steps per second: 209, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.114 [-0.717, 1.643], mean_best_reward: --\n",
      " 69111/100000: episode: 1835, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.109 [-0.966, 0.553], mean_best_reward: --\n",
      " 69167/100000: episode: 1836, duration: 0.273s, episode steps: 56, steps per second: 205, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.107 [-1.327, 0.794], mean_best_reward: --\n",
      " 69181/100000: episode: 1837, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.358, 0.818], mean_best_reward: --\n",
      " 69211/100000: episode: 1838, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.083 [-0.598, 0.876], mean_best_reward: --\n",
      " 69230/100000: episode: 1839, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.076 [-1.837, 1.008], mean_best_reward: --\n",
      " 69250/100000: episode: 1840, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.420, 0.991], mean_best_reward: --\n",
      " 69287/100000: episode: 1841, duration: 0.167s, episode steps: 37, steps per second: 222, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.068 [-1.004, 0.597], mean_best_reward: --\n",
      " 69314/100000: episode: 1842, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.986, 1.528], mean_best_reward: --\n",
      " 69337/100000: episode: 1843, duration: 0.107s, episode steps: 23, steps per second: 215, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.031 [-1.306, 0.774], mean_best_reward: --\n",
      " 69374/100000: episode: 1844, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.012 [-1.139, 1.460], mean_best_reward: --\n",
      " 69408/100000: episode: 1845, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.552, 0.995], mean_best_reward: --\n",
      " 69421/100000: episode: 1846, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.118 [-0.960, 1.666], mean_best_reward: --\n",
      " 69436/100000: episode: 1847, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.829, 0.942], mean_best_reward: --\n",
      " 69471/100000: episode: 1848, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.098 [-1.182, 0.577], mean_best_reward: --\n",
      " 69551/100000: episode: 1849, duration: 0.418s, episode steps: 80, steps per second: 191, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.133 [-0.580, 1.112], mean_best_reward: --\n",
      " 69600/100000: episode: 1850, duration: 0.244s, episode steps: 49, steps per second: 200, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.029 [-1.477, 0.767], mean_best_reward: --\n",
      " 69670/100000: episode: 1851, duration: 0.367s, episode steps: 70, steps per second: 191, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.021 [-1.347, 1.134], mean_best_reward: 88.500000\n",
      " 69683/100000: episode: 1852, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.122 [-1.489, 0.763], mean_best_reward: --\n",
      " 69705/100000: episode: 1853, duration: 0.099s, episode steps: 22, steps per second: 221, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.044 [-0.929, 1.440], mean_best_reward: --\n",
      " 69769/100000: episode: 1854, duration: 0.286s, episode steps: 64, steps per second: 224, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.016 [-0.653, 1.563], mean_best_reward: --\n",
      " 69787/100000: episode: 1855, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.082 [-1.164, 1.935], mean_best_reward: --\n",
      " 69816/100000: episode: 1856, duration: 0.139s, episode steps: 29, steps per second: 209, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.085 [-0.572, 1.317], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69881/100000: episode: 1857, duration: 0.289s, episode steps: 65, steps per second: 225, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.005 [-0.596, 1.130], mean_best_reward: --\n",
      " 69962/100000: episode: 1858, duration: 0.370s, episode steps: 81, steps per second: 219, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.093 [-1.075, 0.870], mean_best_reward: --\n",
      " 70010/100000: episode: 1859, duration: 0.228s, episode steps: 48, steps per second: 211, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-0.967, 0.739], mean_best_reward: --\n",
      " 70020/100000: episode: 1860, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.128 [-0.771, 1.399], mean_best_reward: --\n",
      " 70043/100000: episode: 1861, duration: 0.106s, episode steps: 23, steps per second: 218, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.079 [-1.418, 0.633], mean_best_reward: --\n",
      " 70067/100000: episode: 1862, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.425, 0.883], mean_best_reward: --\n",
      " 70119/100000: episode: 1863, duration: 0.245s, episode steps: 52, steps per second: 213, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.114 [-1.112, 0.807], mean_best_reward: --\n",
      " 70153/100000: episode: 1864, duration: 0.160s, episode steps: 34, steps per second: 212, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.009 [-0.766, 1.059], mean_best_reward: --\n",
      " 70170/100000: episode: 1865, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.119 [-0.764, 1.589], mean_best_reward: --\n",
      " 70198/100000: episode: 1866, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.019 [-1.336, 0.828], mean_best_reward: --\n",
      " 70222/100000: episode: 1867, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.150, 0.804], mean_best_reward: --\n",
      " 70304/100000: episode: 1868, duration: 0.360s, episode steps: 82, steps per second: 228, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.112 [-1.673, 1.530], mean_best_reward: --\n",
      " 70367/100000: episode: 1869, duration: 0.279s, episode steps: 63, steps per second: 226, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.097 [-0.452, 0.982], mean_best_reward: --\n",
      " 70459/100000: episode: 1870, duration: 0.442s, episode steps: 92, steps per second: 208, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.028 [-0.956, 1.057], mean_best_reward: --\n",
      " 70496/100000: episode: 1871, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.039 [-1.693, 0.799], mean_best_reward: --\n",
      " 70545/100000: episode: 1872, duration: 0.220s, episode steps: 49, steps per second: 223, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.054 [-0.364, 1.009], mean_best_reward: --\n",
      " 70573/100000: episode: 1873, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.603, 0.949], mean_best_reward: --\n",
      " 70660/100000: episode: 1874, duration: 0.407s, episode steps: 87, steps per second: 214, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.125 [-0.850, 1.443], mean_best_reward: --\n",
      " 70681/100000: episode: 1875, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.027 [-1.629, 1.187], mean_best_reward: --\n",
      " 70720/100000: episode: 1876, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.114 [-0.571, 0.978], mean_best_reward: --\n",
      " 70739/100000: episode: 1877, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.114 [-0.576, 1.194], mean_best_reward: --\n",
      " 70772/100000: episode: 1878, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.119 [-0.473, 0.936], mean_best_reward: --\n",
      " 70804/100000: episode: 1879, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.589, 1.143], mean_best_reward: --\n",
      " 70856/100000: episode: 1880, duration: 0.258s, episode steps: 52, steps per second: 201, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.136 [-1.020, 0.798], mean_best_reward: --\n",
      " 70890/100000: episode: 1881, duration: 0.153s, episode steps: 34, steps per second: 222, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.078 [-1.315, 0.791], mean_best_reward: --\n",
      " 70917/100000: episode: 1882, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.092 [-0.983, 0.414], mean_best_reward: --\n",
      " 70955/100000: episode: 1883, duration: 0.175s, episode steps: 38, steps per second: 218, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.063 [-1.682, 0.753], mean_best_reward: --\n",
      " 70972/100000: episode: 1884, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.309, 0.811], mean_best_reward: --\n",
      " 70989/100000: episode: 1885, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.072 [-1.000, 1.602], mean_best_reward: --\n",
      " 71037/100000: episode: 1886, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.119 [-0.420, 0.893], mean_best_reward: --\n",
      " 71066/100000: episode: 1887, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.030 [-1.583, 0.985], mean_best_reward: --\n",
      " 71141/100000: episode: 1888, duration: 0.354s, episode steps: 75, steps per second: 212, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.011 [-0.922, 1.520], mean_best_reward: --\n",
      " 71156/100000: episode: 1889, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.096 [-0.750, 1.477], mean_best_reward: --\n",
      " 71356/100000: episode: 1890, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.214 [-1.707, 1.308], mean_best_reward: --\n",
      " 71390/100000: episode: 1891, duration: 0.154s, episode steps: 34, steps per second: 220, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.762, 1.248], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71438/100000: episode: 1892, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.167 [-0.457, 0.966], mean_best_reward: --\n",
      " 71482/100000: episode: 1893, duration: 0.230s, episode steps: 44, steps per second: 192, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.030 [-0.625, 1.200], mean_best_reward: --\n",
      " 71495/100000: episode: 1894, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.121 [-0.740, 1.305], mean_best_reward: --\n",
      " 71517/100000: episode: 1895, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.093 [-0.598, 0.944], mean_best_reward: --\n",
      " 71540/100000: episode: 1896, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.105 [-0.803, 1.251], mean_best_reward: --\n",
      " 71564/100000: episode: 1897, duration: 0.114s, episode steps: 24, steps per second: 210, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.107 [-1.400, 0.612], mean_best_reward: --\n",
      " 71594/100000: episode: 1898, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.447, 0.611], mean_best_reward: --\n",
      " 71656/100000: episode: 1899, duration: 0.289s, episode steps: 62, steps per second: 214, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.113 [-2.181, 0.783], mean_best_reward: --\n",
      " 71681/100000: episode: 1900, duration: 0.133s, episode steps: 25, steps per second: 189, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.091 [-1.558, 0.601], mean_best_reward: --\n",
      " 71720/100000: episode: 1901, duration: 0.191s, episode steps: 39, steps per second: 205, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.136 [-0.350, 0.928], mean_best_reward: 94.000000\n",
      " 71763/100000: episode: 1902, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.157 [-0.562, 1.618], mean_best_reward: --\n",
      " 71814/100000: episode: 1903, duration: 0.242s, episode steps: 51, steps per second: 211, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.038 [-1.423, 0.727], mean_best_reward: --\n",
      " 71830/100000: episode: 1904, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.077 [-1.577, 1.027], mean_best_reward: --\n",
      " 71851/100000: episode: 1905, duration: 0.096s, episode steps: 21, steps per second: 218, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.071 [-1.401, 0.800], mean_best_reward: --\n",
      " 71884/100000: episode: 1906, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.091 [-0.595, 0.900], mean_best_reward: --\n",
      " 71910/100000: episode: 1907, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.052 [-2.108, 1.233], mean_best_reward: --\n",
      " 71975/100000: episode: 1908, duration: 0.301s, episode steps: 65, steps per second: 216, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.252 [-0.576, 1.300], mean_best_reward: --\n",
      " 72009/100000: episode: 1909, duration: 0.163s, episode steps: 34, steps per second: 208, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.138 [-0.584, 0.991], mean_best_reward: --\n",
      " 72027/100000: episode: 1910, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.068 [-1.448, 0.804], mean_best_reward: --\n",
      " 72109/100000: episode: 1911, duration: 0.373s, episode steps: 82, steps per second: 220, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.220 [-1.528, 1.013], mean_best_reward: --\n",
      " 72138/100000: episode: 1912, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.054 [-1.225, 0.800], mean_best_reward: --\n",
      " 72148/100000: episode: 1913, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.577, 0.998], mean_best_reward: --\n",
      " 72175/100000: episode: 1914, duration: 0.125s, episode steps: 27, steps per second: 216, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.065 [-0.786, 1.623], mean_best_reward: --\n",
      " 72199/100000: episode: 1915, duration: 0.110s, episode steps: 24, steps per second: 217, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.069 [-1.483, 0.958], mean_best_reward: --\n",
      " 72221/100000: episode: 1916, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.247, 0.815], mean_best_reward: --\n",
      " 72234/100000: episode: 1917, duration: 0.062s, episode steps: 13, steps per second: 210, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.111 [-1.521, 0.770], mean_best_reward: --\n",
      " 72251/100000: episode: 1918, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.072 [-0.808, 1.177], mean_best_reward: --\n",
      " 72275/100000: episode: 1919, duration: 0.111s, episode steps: 24, steps per second: 215, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.050 [-1.200, 1.874], mean_best_reward: --\n",
      " 72292/100000: episode: 1920, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.095 [-1.216, 0.650], mean_best_reward: --\n",
      " 72334/100000: episode: 1921, duration: 0.201s, episode steps: 42, steps per second: 209, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.685, 1.128], mean_best_reward: --\n",
      " 72367/100000: episode: 1922, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.083 [-1.048, 0.573], mean_best_reward: --\n",
      " 72388/100000: episode: 1923, duration: 0.098s, episode steps: 21, steps per second: 215, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.766, 1.661], mean_best_reward: --\n",
      " 72418/100000: episode: 1924, duration: 0.136s, episode steps: 30, steps per second: 221, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.145 [-1.164, 0.757], mean_best_reward: --\n",
      " 72439/100000: episode: 1925, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.076 [-1.365, 0.765], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72549/100000: episode: 1926, duration: 0.527s, episode steps: 110, steps per second: 209, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.055 [-1.406, 0.992], mean_best_reward: --\n",
      " 72601/100000: episode: 1927, duration: 0.248s, episode steps: 52, steps per second: 209, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.484, 0.782], mean_best_reward: --\n",
      " 72635/100000: episode: 1928, duration: 0.162s, episode steps: 34, steps per second: 209, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.088 [-0.600, 1.340], mean_best_reward: --\n",
      " 72713/100000: episode: 1929, duration: 0.352s, episode steps: 78, steps per second: 222, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.552, 1.390], mean_best_reward: --\n",
      " 72750/100000: episode: 1930, duration: 0.182s, episode steps: 37, steps per second: 204, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.026 [-1.514, 1.142], mean_best_reward: --\n",
      " 72772/100000: episode: 1931, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.091 [-0.427, 1.174], mean_best_reward: --\n",
      " 72821/100000: episode: 1932, duration: 0.226s, episode steps: 49, steps per second: 217, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.157 [-0.576, 1.116], mean_best_reward: --\n",
      " 72855/100000: episode: 1933, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-1.022, 1.423], mean_best_reward: --\n",
      " 72930/100000: episode: 1934, duration: 0.356s, episode steps: 75, steps per second: 210, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.017 [-1.418, 0.744], mean_best_reward: --\n",
      " 73042/100000: episode: 1935, duration: 0.510s, episode steps: 112, steps per second: 219, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.094 [-1.290, 1.705], mean_best_reward: --\n",
      " 73069/100000: episode: 1936, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.080 [-1.600, 0.780], mean_best_reward: --\n",
      " 73122/100000: episode: 1937, duration: 0.251s, episode steps: 53, steps per second: 211, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.061 [-1.324, 0.920], mean_best_reward: --\n",
      " 73153/100000: episode: 1938, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.116 [-0.402, 1.401], mean_best_reward: --\n",
      " 73201/100000: episode: 1939, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.612, 1.258], mean_best_reward: --\n",
      " 73225/100000: episode: 1940, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.101 [-1.313, 0.599], mean_best_reward: --\n",
      " 73244/100000: episode: 1941, duration: 0.089s, episode steps: 19, steps per second: 215, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.117 [-0.597, 0.957], mean_best_reward: --\n",
      " 73284/100000: episode: 1942, duration: 0.183s, episode steps: 40, steps per second: 219, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.410, 0.494], mean_best_reward: --\n",
      " 73303/100000: episode: 1943, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.113 [-0.592, 1.572], mean_best_reward: --\n",
      " 73335/100000: episode: 1944, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.041 [-1.393, 0.761], mean_best_reward: --\n",
      " 73408/100000: episode: 1945, duration: 0.356s, episode steps: 73, steps per second: 205, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.040 [-1.043, 0.726], mean_best_reward: --\n",
      " 73474/100000: episode: 1946, duration: 0.322s, episode steps: 66, steps per second: 205, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.175 [-0.428, 1.543], mean_best_reward: --\n",
      " 73551/100000: episode: 1947, duration: 0.384s, episode steps: 77, steps per second: 201, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.100 [-1.206, 1.144], mean_best_reward: --\n",
      " 73596/100000: episode: 1948, duration: 0.210s, episode steps: 45, steps per second: 215, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.050 [-0.639, 1.621], mean_best_reward: --\n",
      " 73645/100000: episode: 1949, duration: 0.246s, episode steps: 49, steps per second: 200, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.124 [-0.377, 0.934], mean_best_reward: --\n",
      " 73661/100000: episode: 1950, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.201, 0.563], mean_best_reward: --\n",
      " 73755/100000: episode: 1951, duration: 0.437s, episode steps: 94, steps per second: 215, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.071 [-0.992, 0.993], mean_best_reward: 80.500000\n",
      " 73773/100000: episode: 1952, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.329, 0.810], mean_best_reward: --\n",
      " 73830/100000: episode: 1953, duration: 0.267s, episode steps: 57, steps per second: 213, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.040 [-0.973, 0.748], mean_best_reward: --\n",
      " 73931/100000: episode: 1954, duration: 0.457s, episode steps: 101, steps per second: 221, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.078 [-1.268, 1.055], mean_best_reward: --\n",
      " 74009/100000: episode: 1955, duration: 0.430s, episode steps: 78, steps per second: 181, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.129 [-0.816, 1.175], mean_best_reward: --\n",
      " 74099/100000: episode: 1956, duration: 0.494s, episode steps: 90, steps per second: 182, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.087 [-1.559, 0.998], mean_best_reward: --\n",
      " 74131/100000: episode: 1957, duration: 0.218s, episode steps: 32, steps per second: 147, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.844, 1.233], mean_best_reward: --\n",
      " 74157/100000: episode: 1958, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.090 [-0.755, 1.475], mean_best_reward: --\n",
      " 74216/100000: episode: 1959, duration: 0.341s, episode steps: 59, steps per second: 173, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.103 [-0.601, 1.612], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74285/100000: episode: 1960, duration: 0.365s, episode steps: 69, steps per second: 189, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.003 [-1.005, 1.129], mean_best_reward: --\n",
      " 74329/100000: episode: 1961, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.236 [-0.924, 1.748], mean_best_reward: --\n",
      " 74393/100000: episode: 1962, duration: 0.362s, episode steps: 64, steps per second: 177, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.082 [-1.091, 1.407], mean_best_reward: --\n",
      " 74418/100000: episode: 1963, duration: 0.168s, episode steps: 25, steps per second: 148, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.088 [-0.439, 1.235], mean_best_reward: --\n",
      " 74452/100000: episode: 1964, duration: 0.232s, episode steps: 34, steps per second: 146, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.621, 1.232], mean_best_reward: --\n",
      " 74474/100000: episode: 1965, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.096 [-0.770, 1.480], mean_best_reward: --\n",
      " 74529/100000: episode: 1966, duration: 0.385s, episode steps: 55, steps per second: 143, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.094 [-1.052, 0.764], mean_best_reward: --\n",
      " 74554/100000: episode: 1967, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.084 [-0.830, 1.218], mean_best_reward: --\n",
      " 74618/100000: episode: 1968, duration: 0.371s, episode steps: 64, steps per second: 172, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.106 [-1.091, 1.170], mean_best_reward: --\n",
      " 74693/100000: episode: 1969, duration: 0.433s, episode steps: 75, steps per second: 173, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.078 [-0.645, 0.989], mean_best_reward: --\n",
      " 74732/100000: episode: 1970, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.087 [-1.392, 0.772], mean_best_reward: --\n",
      " 74795/100000: episode: 1971, duration: 0.313s, episode steps: 63, steps per second: 201, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.018 [-1.203, 1.439], mean_best_reward: --\n",
      " 74854/100000: episode: 1972, duration: 0.349s, episode steps: 59, steps per second: 169, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.064 [-1.104, 0.598], mean_best_reward: --\n",
      " 74902/100000: episode: 1973, duration: 0.320s, episode steps: 48, steps per second: 150, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.098 [-0.749, 0.896], mean_best_reward: --\n",
      " 74920/100000: episode: 1974, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.366, 0.747], mean_best_reward: --\n",
      " 74958/100000: episode: 1975, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.104 [-1.852, 0.948], mean_best_reward: --\n",
      " 74978/100000: episode: 1976, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.059 [-1.657, 1.156], mean_best_reward: --\n",
      " 75004/100000: episode: 1977, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.101 [-0.646, 1.549], mean_best_reward: --\n",
      " 75063/100000: episode: 1978, duration: 0.280s, episode steps: 59, steps per second: 210, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.175 [-1.341, 0.689], mean_best_reward: --\n",
      " 75095/100000: episode: 1979, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.115 [-0.597, 0.992], mean_best_reward: --\n",
      " 75143/100000: episode: 1980, duration: 0.219s, episode steps: 48, steps per second: 219, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.578, 0.616], mean_best_reward: --\n",
      " 75178/100000: episode: 1981, duration: 0.161s, episode steps: 35, steps per second: 217, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.029 [-1.311, 0.787], mean_best_reward: --\n",
      " 75208/100000: episode: 1982, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.629, 0.939], mean_best_reward: --\n",
      " 75230/100000: episode: 1983, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.074 [-1.307, 0.604], mean_best_reward: --\n",
      " 75264/100000: episode: 1984, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-0.639, 1.396], mean_best_reward: --\n",
      " 75316/100000: episode: 1985, duration: 0.291s, episode steps: 52, steps per second: 178, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-1.158, 0.615], mean_best_reward: --\n",
      " 75340/100000: episode: 1986, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.106 [-1.689, 0.770], mean_best_reward: --\n",
      " 75383/100000: episode: 1987, duration: 0.242s, episode steps: 43, steps per second: 178, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.097 [-1.139, 0.558], mean_best_reward: --\n",
      " 75410/100000: episode: 1988, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.123 [-0.382, 0.920], mean_best_reward: --\n",
      " 75457/100000: episode: 1989, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.086 [-0.742, 0.416], mean_best_reward: --\n",
      " 75473/100000: episode: 1990, duration: 0.113s, episode steps: 16, steps per second: 141, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.107 [-0.593, 0.999], mean_best_reward: --\n",
      " 75486/100000: episode: 1991, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.086 [-1.380, 2.145], mean_best_reward: --\n",
      " 75505/100000: episode: 1992, duration: 0.126s, episode steps: 19, steps per second: 151, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.076 [-1.385, 0.746], mean_best_reward: --\n",
      " 75526/100000: episode: 1993, duration: 0.163s, episode steps: 21, steps per second: 129, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.090 [-0.752, 1.412], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75566/100000: episode: 1994, duration: 0.269s, episode steps: 40, steps per second: 149, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.081 [-0.442, 0.891], mean_best_reward: --\n",
      " 75601/100000: episode: 1995, duration: 0.228s, episode steps: 35, steps per second: 153, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.109 [-0.769, 1.611], mean_best_reward: --\n",
      " 75619/100000: episode: 1996, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.082 [-0.756, 1.360], mean_best_reward: --\n",
      " 75656/100000: episode: 1997, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.063 [-0.974, 0.591], mean_best_reward: --\n",
      " 75690/100000: episode: 1998, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-1.103, 0.620], mean_best_reward: --\n",
      " 75707/100000: episode: 1999, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.105 [-0.559, 1.195], mean_best_reward: --\n",
      " 75756/100000: episode: 2000, duration: 0.236s, episode steps: 49, steps per second: 207, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.087 [-0.429, 1.104], mean_best_reward: --\n",
      " 75793/100000: episode: 2001, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.057 [-0.725, 1.089], mean_best_reward: 94.500000\n",
      " 75806/100000: episode: 2002, duration: 0.092s, episode steps: 13, steps per second: 142, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.106 [-1.610, 0.974], mean_best_reward: --\n",
      " 75839/100000: episode: 2003, duration: 0.151s, episode steps: 33, steps per second: 218, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.084 [-1.111, 0.475], mean_best_reward: --\n",
      " 75870/100000: episode: 2004, duration: 0.147s, episode steps: 31, steps per second: 211, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.104 [-1.089, 0.585], mean_best_reward: --\n",
      " 75914/100000: episode: 2005, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.490, 1.021], mean_best_reward: --\n",
      " 75990/100000: episode: 2006, duration: 0.389s, episode steps: 76, steps per second: 196, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.101 [-0.778, 1.055], mean_best_reward: --\n",
      " 76002/100000: episode: 2007, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.613, 0.971], mean_best_reward: --\n",
      " 76030/100000: episode: 2008, duration: 0.140s, episode steps: 28, steps per second: 199, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-0.972, 0.593], mean_best_reward: --\n",
      " 76068/100000: episode: 2009, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.005 [-1.563, 1.147], mean_best_reward: --\n",
      " 76081/100000: episode: 2010, duration: 0.062s, episode steps: 13, steps per second: 210, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.091 [-1.252, 0.800], mean_best_reward: --\n",
      " 76131/100000: episode: 2011, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.640, 0.989], mean_best_reward: --\n",
      " 76163/100000: episode: 2012, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.159 [-0.667, 0.967], mean_best_reward: --\n",
      " 76230/100000: episode: 2013, duration: 0.329s, episode steps: 67, steps per second: 204, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.072 [-0.560, 1.068], mean_best_reward: --\n",
      " 76255/100000: episode: 2014, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.065 [-1.538, 2.406], mean_best_reward: --\n",
      " 76317/100000: episode: 2015, duration: 0.360s, episode steps: 62, steps per second: 172, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.186 [-1.073, 0.385], mean_best_reward: --\n",
      " 76374/100000: episode: 2016, duration: 0.290s, episode steps: 57, steps per second: 196, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.152 [-0.949, 1.489], mean_best_reward: --\n",
      " 76472/100000: episode: 2017, duration: 0.434s, episode steps: 98, steps per second: 226, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.955, 1.007], mean_best_reward: --\n",
      " 76506/100000: episode: 2018, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.904, 0.542], mean_best_reward: --\n",
      " 76538/100000: episode: 2019, duration: 0.148s, episode steps: 32, steps per second: 216, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-1.241, 0.755], mean_best_reward: --\n",
      " 76574/100000: episode: 2020, duration: 0.163s, episode steps: 36, steps per second: 220, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.509, 1.291], mean_best_reward: --\n",
      " 76639/100000: episode: 2021, duration: 0.328s, episode steps: 65, steps per second: 198, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.112 [-1.188, 2.167], mean_best_reward: --\n",
      " 76662/100000: episode: 2022, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.111 [-1.169, 2.228], mean_best_reward: --\n",
      " 76735/100000: episode: 2023, duration: 0.376s, episode steps: 73, steps per second: 194, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.128 [-1.096, 1.138], mean_best_reward: --\n",
      " 76774/100000: episode: 2024, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.125 [-0.620, 0.929], mean_best_reward: --\n",
      " 76808/100000: episode: 2025, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.000 [-0.942, 1.286], mean_best_reward: --\n",
      " 76844/100000: episode: 2026, duration: 0.165s, episode steps: 36, steps per second: 219, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.225, 0.645], mean_best_reward: --\n",
      " 76866/100000: episode: 2027, duration: 0.101s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.811, 1.262], mean_best_reward: --\n",
      " 76878/100000: episode: 2028, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.107 [-1.153, 2.030], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76929/100000: episode: 2029, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.081 [-1.265, 0.647], mean_best_reward: --\n",
      " 76965/100000: episode: 2030, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.462, 1.088], mean_best_reward: --\n",
      " 76997/100000: episode: 2031, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.075 [-0.780, 1.694], mean_best_reward: --\n",
      " 77061/100000: episode: 2032, duration: 0.305s, episode steps: 64, steps per second: 210, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.029 [-0.917, 1.228], mean_best_reward: --\n",
      " 77098/100000: episode: 2033, duration: 0.183s, episode steps: 37, steps per second: 203, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.098 [-0.803, 0.954], mean_best_reward: --\n",
      " 77120/100000: episode: 2034, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.107 [-0.558, 1.365], mean_best_reward: --\n",
      " 77157/100000: episode: 2035, duration: 0.177s, episode steps: 37, steps per second: 209, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.112 [-1.223, 0.742], mean_best_reward: --\n",
      " 77179/100000: episode: 2036, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.591, 0.987], mean_best_reward: --\n",
      " 77267/100000: episode: 2037, duration: 0.409s, episode steps: 88, steps per second: 215, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.164 [-1.509, 1.243], mean_best_reward: --\n",
      " 77318/100000: episode: 2038, duration: 0.232s, episode steps: 51, steps per second: 220, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.061 [-0.421, 1.425], mean_best_reward: --\n",
      " 77353/100000: episode: 2039, duration: 0.165s, episode steps: 35, steps per second: 212, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.083 [-0.996, 0.449], mean_best_reward: --\n",
      " 77399/100000: episode: 2040, duration: 0.212s, episode steps: 46, steps per second: 217, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-0.983, 0.607], mean_best_reward: --\n",
      " 77428/100000: episode: 2041, duration: 0.131s, episode steps: 29, steps per second: 221, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.089 [-0.414, 1.098], mean_best_reward: --\n",
      " 77507/100000: episode: 2042, duration: 0.372s, episode steps: 79, steps per second: 212, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.089 [-0.855, 0.956], mean_best_reward: --\n",
      " 77525/100000: episode: 2043, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.626, 1.091], mean_best_reward: --\n",
      " 77542/100000: episode: 2044, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.110 [-1.103, 0.558], mean_best_reward: --\n",
      " 77568/100000: episode: 2045, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.099 [-0.928, 0.613], mean_best_reward: --\n",
      " 77599/100000: episode: 2046, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.073 [-0.792, 1.495], mean_best_reward: --\n",
      " 77630/100000: episode: 2047, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.085 [-1.410, 0.569], mean_best_reward: --\n",
      " 77673/100000: episode: 2048, duration: 0.204s, episode steps: 43, steps per second: 210, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.102 [-0.566, 1.096], mean_best_reward: --\n",
      " 77711/100000: episode: 2049, duration: 0.183s, episode steps: 38, steps per second: 207, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.144 [-1.039, 0.590], mean_best_reward: --\n",
      " 77762/100000: episode: 2050, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.157 [-0.682, 1.097], mean_best_reward: --\n",
      " 77793/100000: episode: 2051, duration: 0.144s, episode steps: 31, steps per second: 215, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.065 [-1.379, 0.997], mean_best_reward: 80.000000\n",
      " 77836/100000: episode: 2052, duration: 0.198s, episode steps: 43, steps per second: 217, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.020 [-1.068, 0.604], mean_best_reward: --\n",
      " 77890/100000: episode: 2053, duration: 0.253s, episode steps: 54, steps per second: 213, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.092 [-0.680, 0.944], mean_best_reward: --\n",
      " 77950/100000: episode: 2054, duration: 0.271s, episode steps: 60, steps per second: 222, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.199 [-1.135, 1.539], mean_best_reward: --\n",
      " 77972/100000: episode: 2055, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-1.002, 1.457], mean_best_reward: --\n",
      " 77991/100000: episode: 2056, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.110 [-1.457, 0.759], mean_best_reward: --\n",
      " 78022/100000: episode: 2057, duration: 0.143s, episode steps: 31, steps per second: 217, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.089 [-0.434, 1.015], mean_best_reward: --\n",
      " 78094/100000: episode: 2058, duration: 0.328s, episode steps: 72, steps per second: 219, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.859, 1.175], mean_best_reward: --\n",
      " 78128/100000: episode: 2059, duration: 0.154s, episode steps: 34, steps per second: 221, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.118 [-0.548, 1.059], mean_best_reward: --\n",
      " 78184/100000: episode: 2060, duration: 0.267s, episode steps: 56, steps per second: 210, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.711, 1.214], mean_best_reward: --\n",
      " 78194/100000: episode: 2061, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-1.685, 1.034], mean_best_reward: --\n",
      " 78247/100000: episode: 2062, duration: 0.249s, episode steps: 53, steps per second: 212, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.123 [-1.181, 1.010], mean_best_reward: --\n",
      " 78288/100000: episode: 2063, duration: 0.186s, episode steps: 41, steps per second: 220, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.131 [-0.502, 1.303], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78326/100000: episode: 2064, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.004, 0.440], mean_best_reward: --\n",
      " 78381/100000: episode: 2065, duration: 0.273s, episode steps: 55, steps per second: 202, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: 0.026 [-0.757, 1.591], mean_best_reward: --\n",
      " 78432/100000: episode: 2066, duration: 0.231s, episode steps: 51, steps per second: 220, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.137 [-0.623, 0.974], mean_best_reward: --\n",
      " 78491/100000: episode: 2067, duration: 0.296s, episode steps: 59, steps per second: 199, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.033 [-1.268, 1.823], mean_best_reward: --\n",
      " 78510/100000: episode: 2068, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.124 [-1.102, 0.566], mean_best_reward: --\n",
      " 78527/100000: episode: 2069, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.055 [-1.175, 1.829], mean_best_reward: --\n",
      " 78555/100000: episode: 2070, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-1.147, 0.576], mean_best_reward: --\n",
      " 78631/100000: episode: 2071, duration: 0.389s, episode steps: 76, steps per second: 196, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.033 [-1.890, 1.040], mean_best_reward: --\n",
      " 78668/100000: episode: 2072, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.105 [-0.925, 0.439], mean_best_reward: --\n",
      " 78713/100000: episode: 2073, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.082 [-1.004, 0.569], mean_best_reward: --\n",
      " 78773/100000: episode: 2074, duration: 0.312s, episode steps: 60, steps per second: 192, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.085 [-1.443, 0.574], mean_best_reward: --\n",
      " 78793/100000: episode: 2075, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.739, 1.331], mean_best_reward: --\n",
      " 78880/100000: episode: 2076, duration: 0.414s, episode steps: 87, steps per second: 210, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: 0.074 [-1.616, 2.558], mean_best_reward: --\n",
      " 78907/100000: episode: 2077, duration: 0.123s, episode steps: 27, steps per second: 220, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.065 [-0.951, 1.453], mean_best_reward: --\n",
      " 78938/100000: episode: 2078, duration: 0.142s, episode steps: 31, steps per second: 218, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.108 [-0.627, 1.582], mean_best_reward: --\n",
      " 78984/100000: episode: 2079, duration: 0.213s, episode steps: 46, steps per second: 216, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.520, 1.061], mean_best_reward: --\n",
      " 79012/100000: episode: 2080, duration: 0.127s, episode steps: 28, steps per second: 221, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.073 [-0.604, 1.082], mean_best_reward: --\n",
      " 79039/100000: episode: 2081, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.071 [-0.552, 1.181], mean_best_reward: --\n",
      " 79072/100000: episode: 2082, duration: 0.152s, episode steps: 33, steps per second: 218, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.086 [-0.534, 1.271], mean_best_reward: --\n",
      " 79148/100000: episode: 2083, duration: 0.335s, episode steps: 76, steps per second: 227, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-0.918, 1.264], mean_best_reward: --\n",
      " 79172/100000: episode: 2084, duration: 0.110s, episode steps: 24, steps per second: 218, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-0.795, 1.366], mean_best_reward: --\n",
      " 79216/100000: episode: 2085, duration: 0.205s, episode steps: 44, steps per second: 215, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.107 [-0.883, 0.728], mean_best_reward: --\n",
      " 79226/100000: episode: 2086, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.137 [-2.196, 1.345], mean_best_reward: --\n",
      " 79244/100000: episode: 2087, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.443, 1.029], mean_best_reward: --\n",
      " 79261/100000: episode: 2088, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.120 [-1.296, 0.571], mean_best_reward: --\n",
      " 79312/100000: episode: 2089, duration: 0.230s, episode steps: 51, steps per second: 221, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.103 [-1.257, 0.821], mean_best_reward: --\n",
      " 79351/100000: episode: 2090, duration: 0.175s, episode steps: 39, steps per second: 222, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.121 [-1.225, 0.938], mean_best_reward: --\n",
      " 79414/100000: episode: 2091, duration: 0.280s, episode steps: 63, steps per second: 225, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.023 [-0.766, 1.209], mean_best_reward: --\n",
      " 79443/100000: episode: 2092, duration: 0.140s, episode steps: 29, steps per second: 208, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.117 [-0.580, 0.930], mean_best_reward: --\n",
      " 79498/100000: episode: 2093, duration: 0.247s, episode steps: 55, steps per second: 223, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.169 [-0.518, 1.056], mean_best_reward: --\n",
      " 79515/100000: episode: 2094, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.096 [-1.527, 0.756], mean_best_reward: --\n",
      " 79559/100000: episode: 2095, duration: 0.210s, episode steps: 44, steps per second: 210, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.067 [-0.581, 1.341], mean_best_reward: --\n",
      " 79618/100000: episode: 2096, duration: 0.269s, episode steps: 59, steps per second: 219, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.116 [-1.108, 0.615], mean_best_reward: --\n",
      " 79673/100000: episode: 2097, duration: 0.252s, episode steps: 55, steps per second: 218, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.114 [-1.298, 0.828], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79721/100000: episode: 2098, duration: 0.233s, episode steps: 48, steps per second: 206, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.134 [-0.496, 1.551], mean_best_reward: --\n",
      " 79805/100000: episode: 2099, duration: 0.422s, episode steps: 84, steps per second: 199, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.220 [-1.843, 1.094], mean_best_reward: --\n",
      " 79867/100000: episode: 2100, duration: 0.300s, episode steps: 62, steps per second: 207, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.174 [-1.468, 0.595], mean_best_reward: --\n",
      " 79904/100000: episode: 2101, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.119 [-0.997, 0.604], mean_best_reward: 82.000000\n",
      " 79938/100000: episode: 2102, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.101 [-0.879, 0.628], mean_best_reward: --\n",
      " 79954/100000: episode: 2103, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.311, 0.835], mean_best_reward: --\n",
      " 79978/100000: episode: 2104, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.101 [-0.787, 1.607], mean_best_reward: --\n",
      " 80010/100000: episode: 2105, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.130 [-0.950, 0.555], mean_best_reward: --\n",
      " 80068/100000: episode: 2106, duration: 0.303s, episode steps: 58, steps per second: 191, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.451, 0.765], mean_best_reward: --\n",
      " 80089/100000: episode: 2107, duration: 0.098s, episode steps: 21, steps per second: 214, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.096 [-0.447, 1.062], mean_best_reward: --\n",
      " 80150/100000: episode: 2108, duration: 0.292s, episode steps: 61, steps per second: 209, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.142 [-0.521, 1.450], mean_best_reward: --\n",
      " 80167/100000: episode: 2109, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.062 [-2.068, 1.362], mean_best_reward: --\n",
      " 80195/100000: episode: 2110, duration: 0.130s, episode steps: 28, steps per second: 216, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.122 [-0.560, 1.149], mean_best_reward: --\n",
      " 80262/100000: episode: 2111, duration: 0.361s, episode steps: 67, steps per second: 186, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.054 [-1.015, 0.594], mean_best_reward: --\n",
      " 80280/100000: episode: 2112, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.077 [-1.284, 0.632], mean_best_reward: --\n",
      " 80347/100000: episode: 2113, duration: 0.357s, episode steps: 67, steps per second: 187, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.002 [-1.505, 1.148], mean_best_reward: --\n",
      " 80413/100000: episode: 2114, duration: 0.326s, episode steps: 66, steps per second: 202, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.077 [-0.957, 1.131], mean_best_reward: --\n",
      " 80441/100000: episode: 2115, duration: 0.182s, episode steps: 28, steps per second: 154, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.074 [-0.759, 1.534], mean_best_reward: --\n",
      " 80464/100000: episode: 2116, duration: 0.139s, episode steps: 23, steps per second: 165, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.123 [-0.570, 1.082], mean_best_reward: --\n",
      " 80489/100000: episode: 2117, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.128 [-1.238, 0.793], mean_best_reward: --\n",
      " 80555/100000: episode: 2118, duration: 0.327s, episode steps: 66, steps per second: 202, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.229 [-1.505, 0.783], mean_best_reward: --\n",
      " 80584/100000: episode: 2119, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.097 [-1.238, 0.506], mean_best_reward: --\n",
      " 80630/100000: episode: 2120, duration: 0.347s, episode steps: 46, steps per second: 132, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.153 [-0.749, 1.008], mean_best_reward: --\n",
      " 80685/100000: episode: 2121, duration: 0.277s, episode steps: 55, steps per second: 198, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.042 [-1.476, 0.820], mean_best_reward: --\n",
      " 80716/100000: episode: 2122, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.001 [-1.363, 1.853], mean_best_reward: --\n",
      " 80754/100000: episode: 2123, duration: 0.175s, episode steps: 38, steps per second: 217, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.130 [-0.988, 0.701], mean_best_reward: --\n",
      " 80869/100000: episode: 2124, duration: 0.566s, episode steps: 115, steps per second: 203, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.052 [-1.161, 0.977], mean_best_reward: --\n",
      " 80880/100000: episode: 2125, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.119 [-0.957, 1.573], mean_best_reward: --\n",
      " 80921/100000: episode: 2126, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.031 [-1.590, 1.392], mean_best_reward: --\n",
      " 80936/100000: episode: 2127, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.105 [-0.626, 1.081], mean_best_reward: --\n",
      " 80969/100000: episode: 2128, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.091 [-0.873, 0.985], mean_best_reward: --\n",
      " 81001/100000: episode: 2129, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.150 [-1.311, 0.571], mean_best_reward: --\n",
      " 81038/100000: episode: 2130, duration: 0.172s, episode steps: 37, steps per second: 215, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.025 [-0.608, 0.908], mean_best_reward: --\n",
      " 81084/100000: episode: 2131, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.106 [-1.072, 0.708], mean_best_reward: --\n",
      " 81119/100000: episode: 2132, duration: 0.165s, episode steps: 35, steps per second: 213, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.047 [-0.753, 1.418], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81159/100000: episode: 2133, duration: 0.185s, episode steps: 40, steps per second: 217, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.143 [-1.081, 0.624], mean_best_reward: --\n",
      " 81177/100000: episode: 2134, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.133 [-0.571, 1.041], mean_best_reward: --\n",
      " 81222/100000: episode: 2135, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.167 [-1.632, 0.756], mean_best_reward: --\n",
      " 81283/100000: episode: 2136, duration: 0.305s, episode steps: 61, steps per second: 200, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.100 [-0.968, 0.629], mean_best_reward: --\n",
      " 81299/100000: episode: 2137, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.116 [-0.758, 1.279], mean_best_reward: --\n",
      " 81310/100000: episode: 2138, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.351, 1.360], mean_best_reward: --\n",
      " 81320/100000: episode: 2139, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.129 [-1.851, 1.186], mean_best_reward: --\n",
      " 81383/100000: episode: 2140, duration: 0.339s, episode steps: 63, steps per second: 186, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.105 [-1.527, 0.613], mean_best_reward: --\n",
      " 81415/100000: episode: 2141, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.106 [-0.799, 0.548], mean_best_reward: --\n",
      " 81479/100000: episode: 2142, duration: 0.329s, episode steps: 64, steps per second: 195, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.701, 1.423], mean_best_reward: --\n",
      " 81499/100000: episode: 2143, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.629, 1.153], mean_best_reward: --\n",
      " 81543/100000: episode: 2144, duration: 0.247s, episode steps: 44, steps per second: 178, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.265, 0.571], mean_best_reward: --\n",
      " 81560/100000: episode: 2145, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.125 [-1.190, 0.733], mean_best_reward: --\n",
      " 81639/100000: episode: 2146, duration: 0.551s, episode steps: 79, steps per second: 143, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.003 [-0.973, 1.024], mean_best_reward: --\n",
      " 81652/100000: episode: 2147, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.116 [-1.849, 1.030], mean_best_reward: --\n",
      " 81757/100000: episode: 2148, duration: 0.637s, episode steps: 105, steps per second: 165, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.058 [-1.046, 1.221], mean_best_reward: --\n",
      " 81774/100000: episode: 2149, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.098 [-1.259, 0.754], mean_best_reward: --\n",
      " 81794/100000: episode: 2150, duration: 0.110s, episode steps: 20, steps per second: 183, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.089 [-1.314, 0.582], mean_best_reward: --\n",
      " 81910/100000: episode: 2151, duration: 0.577s, episode steps: 116, steps per second: 201, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.070 [-0.944, 1.155], mean_best_reward: 93.000000\n",
      " 81990/100000: episode: 2152, duration: 0.463s, episode steps: 80, steps per second: 173, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-1.404, 0.852], mean_best_reward: --\n",
      " 82025/100000: episode: 2153, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.064 [-0.740, 1.220], mean_best_reward: --\n",
      " 82060/100000: episode: 2154, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.002 [-1.217, 1.705], mean_best_reward: --\n",
      " 82079/100000: episode: 2155, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.078 [-1.304, 0.741], mean_best_reward: --\n",
      " 82109/100000: episode: 2156, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.068 [-1.104, 0.646], mean_best_reward: --\n",
      " 82129/100000: episode: 2157, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.067, 0.627], mean_best_reward: --\n",
      " 82207/100000: episode: 2158, duration: 0.404s, episode steps: 78, steps per second: 193, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-0.960, 1.121], mean_best_reward: --\n",
      " 82243/100000: episode: 2159, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.083 [-0.978, 1.931], mean_best_reward: --\n",
      " 82308/100000: episode: 2160, duration: 0.528s, episode steps: 65, steps per second: 123, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.111 [-0.931, 1.401], mean_best_reward: --\n",
      " 82358/100000: episode: 2161, duration: 0.245s, episode steps: 50, steps per second: 204, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.546, 0.895], mean_best_reward: --\n",
      " 82393/100000: episode: 2162, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.142 [-0.880, 0.537], mean_best_reward: --\n",
      " 82466/100000: episode: 2163, duration: 0.411s, episode steps: 73, steps per second: 178, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.061 [-1.387, 0.781], mean_best_reward: --\n",
      " 82503/100000: episode: 2164, duration: 0.238s, episode steps: 37, steps per second: 155, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.058 [-1.364, 0.958], mean_best_reward: --\n",
      " 82703/100000: episode: 2165, duration: 1.015s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-1.202, 1.146], mean_best_reward: --\n",
      " 82747/100000: episode: 2166, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.013 [-1.502, 0.947], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82784/100000: episode: 2167, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.096 [-1.086, 0.625], mean_best_reward: --\n",
      " 82818/100000: episode: 2168, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.082 [-0.952, 0.433], mean_best_reward: --\n",
      " 82869/100000: episode: 2169, duration: 0.328s, episode steps: 51, steps per second: 156, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.134 [-1.237, 0.802], mean_best_reward: --\n",
      " 82932/100000: episode: 2170, duration: 0.304s, episode steps: 63, steps per second: 207, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.137 [-0.716, 0.566], mean_best_reward: --\n",
      " 82947/100000: episode: 2171, duration: 0.076s, episode steps: 15, steps per second: 199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.104 [-0.762, 1.228], mean_best_reward: --\n",
      " 82995/100000: episode: 2172, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.036 [-0.574, 0.913], mean_best_reward: --\n",
      " 83027/100000: episode: 2173, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.085 [-0.562, 0.982], mean_best_reward: --\n",
      " 83047/100000: episode: 2174, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.151, 0.787], mean_best_reward: --\n",
      " 83072/100000: episode: 2175, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.140 [-0.774, 0.370], mean_best_reward: --\n",
      " 83110/100000: episode: 2176, duration: 0.180s, episode steps: 38, steps per second: 211, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.142 [-1.363, 0.592], mean_best_reward: --\n",
      " 83151/100000: episode: 2177, duration: 0.193s, episode steps: 41, steps per second: 212, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.085 [-0.835, 0.438], mean_best_reward: --\n",
      " 83173/100000: episode: 2178, duration: 0.102s, episode steps: 22, steps per second: 216, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.101 [-0.999, 0.621], mean_best_reward: --\n",
      " 83187/100000: episode: 2179, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.606, 1.033], mean_best_reward: --\n",
      " 83253/100000: episode: 2180, duration: 0.309s, episode steps: 66, steps per second: 213, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.156 [-1.212, 0.698], mean_best_reward: --\n",
      " 83274/100000: episode: 2181, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.064 [-0.733, 1.147], mean_best_reward: --\n",
      " 83303/100000: episode: 2182, duration: 0.192s, episode steps: 29, steps per second: 151, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.021 [-2.026, 1.348], mean_best_reward: --\n",
      " 83319/100000: episode: 2183, duration: 0.125s, episode steps: 16, steps per second: 128, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.824, 1.323], mean_best_reward: --\n",
      " 83342/100000: episode: 2184, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.058 [-1.677, 1.120], mean_best_reward: --\n",
      " 83370/100000: episode: 2185, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.052 [-1.202, 1.916], mean_best_reward: --\n",
      " 83424/100000: episode: 2186, duration: 0.281s, episode steps: 54, steps per second: 192, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.118 [-0.818, 1.201], mean_best_reward: --\n",
      " 83458/100000: episode: 2187, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.092 [-0.764, 1.813], mean_best_reward: --\n",
      " 83489/100000: episode: 2188, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.098 [-1.282, 0.549], mean_best_reward: --\n",
      " 83509/100000: episode: 2189, duration: 0.125s, episode steps: 20, steps per second: 161, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.075 [-0.964, 1.377], mean_best_reward: --\n",
      " 83520/100000: episode: 2190, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.128 [-2.173, 1.356], mean_best_reward: --\n",
      " 83544/100000: episode: 2191, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.067 [-0.617, 1.054], mean_best_reward: --\n",
      " 83614/100000: episode: 2192, duration: 0.357s, episode steps: 70, steps per second: 196, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.099 [-1.674, 1.883], mean_best_reward: --\n",
      " 83648/100000: episode: 2193, duration: 0.169s, episode steps: 34, steps per second: 202, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.100 [-1.261, 0.828], mean_best_reward: --\n",
      " 83666/100000: episode: 2194, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.046 [-1.348, 0.967], mean_best_reward: --\n",
      " 83688/100000: episode: 2195, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-1.119, 1.603], mean_best_reward: --\n",
      " 83756/100000: episode: 2196, duration: 0.372s, episode steps: 68, steps per second: 183, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.093 [-1.079, 0.633], mean_best_reward: --\n",
      " 83775/100000: episode: 2197, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.078 [-1.024, 1.901], mean_best_reward: --\n",
      " 83893/100000: episode: 2198, duration: 0.609s, episode steps: 118, steps per second: 194, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.032 [-1.324, 0.833], mean_best_reward: --\n",
      " 83972/100000: episode: 2199, duration: 0.449s, episode steps: 79, steps per second: 176, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.140 [-1.034, 0.788], mean_best_reward: --\n",
      " 83988/100000: episode: 2200, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.099 [-0.588, 1.129], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84010/100000: episode: 2201, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-1.053, 0.609], mean_best_reward: 119.500000\n",
      " 84039/100000: episode: 2202, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.085 [-0.966, 1.482], mean_best_reward: --\n",
      " 84074/100000: episode: 2203, duration: 0.213s, episode steps: 35, steps per second: 164, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.097 [-0.607, 0.995], mean_best_reward: --\n",
      " 84146/100000: episode: 2204, duration: 0.449s, episode steps: 72, steps per second: 161, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.056 [-1.632, 0.784], mean_best_reward: --\n",
      " 84201/100000: episode: 2205, duration: 0.345s, episode steps: 55, steps per second: 160, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.106 [-0.493, 1.215], mean_best_reward: --\n",
      " 84247/100000: episode: 2206, duration: 0.340s, episode steps: 46, steps per second: 135, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.978, 0.750], mean_best_reward: --\n",
      " 84295/100000: episode: 2207, duration: 0.297s, episode steps: 48, steps per second: 162, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.107 [-1.066, 0.825], mean_best_reward: --\n",
      " 84359/100000: episode: 2208, duration: 0.372s, episode steps: 64, steps per second: 172, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.013 [-0.794, 1.200], mean_best_reward: --\n",
      " 84458/100000: episode: 2209, duration: 0.558s, episode steps: 99, steps per second: 177, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.056 [-0.892, 0.587], mean_best_reward: --\n",
      " 84522/100000: episode: 2210, duration: 0.360s, episode steps: 64, steps per second: 178, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.059 [-0.893, 0.828], mean_best_reward: --\n",
      " 84563/100000: episode: 2211, duration: 0.237s, episode steps: 41, steps per second: 173, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.131 [-0.786, 0.912], mean_best_reward: --\n",
      " 84637/100000: episode: 2212, duration: 0.454s, episode steps: 74, steps per second: 163, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.003 [-1.248, 0.930], mean_best_reward: --\n",
      " 84663/100000: episode: 2213, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.054 [-1.139, 1.897], mean_best_reward: --\n",
      " 84685/100000: episode: 2214, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.027 [-1.454, 1.009], mean_best_reward: --\n",
      " 84705/100000: episode: 2215, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.608, 1.098], mean_best_reward: --\n",
      " 84724/100000: episode: 2216, duration: 0.123s, episode steps: 19, steps per second: 155, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.035 [-1.164, 1.690], mean_best_reward: --\n",
      " 84738/100000: episode: 2217, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.084 [-1.416, 0.830], mean_best_reward: --\n",
      " 84779/100000: episode: 2218, duration: 0.194s, episode steps: 41, steps per second: 212, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.139 [-1.055, 0.670], mean_best_reward: --\n",
      " 84810/100000: episode: 2219, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.036 [-1.173, 0.800], mean_best_reward: --\n",
      " 84827/100000: episode: 2220, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.109 [-1.620, 0.806], mean_best_reward: --\n",
      " 84842/100000: episode: 2221, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.104 [-0.800, 1.487], mean_best_reward: --\n",
      " 84890/100000: episode: 2222, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.153 [-1.113, 0.415], mean_best_reward: --\n",
      " 84931/100000: episode: 2223, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.079 [-1.296, 0.629], mean_best_reward: --\n",
      " 84950/100000: episode: 2224, duration: 0.149s, episode steps: 19, steps per second: 128, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.083 [-1.929, 1.039], mean_best_reward: --\n",
      " 84963/100000: episode: 2225, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.108 [-1.185, 1.901], mean_best_reward: --\n",
      " 85020/100000: episode: 2226, duration: 0.341s, episode steps: 57, steps per second: 167, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.010 [-0.745, 1.128], mean_best_reward: --\n",
      " 85059/100000: episode: 2227, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.039 [-1.335, 0.755], mean_best_reward: --\n",
      " 85081/100000: episode: 2228, duration: 0.167s, episode steps: 22, steps per second: 132, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.628, 0.964], mean_best_reward: --\n",
      " 85119/100000: episode: 2229, duration: 0.238s, episode steps: 38, steps per second: 160, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.086 [-1.742, 0.642], mean_best_reward: --\n",
      " 85129/100000: episode: 2230, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.145 [-1.165, 1.891], mean_best_reward: --\n",
      " 85162/100000: episode: 2231, duration: 0.233s, episode steps: 33, steps per second: 142, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.043 [-0.619, 1.021], mean_best_reward: --\n",
      " 85192/100000: episode: 2232, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.130, 0.569], mean_best_reward: --\n",
      " 85244/100000: episode: 2233, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.186, 0.805], mean_best_reward: --\n",
      " 85292/100000: episode: 2234, duration: 0.295s, episode steps: 48, steps per second: 163, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.083 [-0.945, 0.546], mean_best_reward: --\n",
      " 85328/100000: episode: 2235, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.081 [-0.991, 1.497], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85366/100000: episode: 2236, duration: 0.242s, episode steps: 38, steps per second: 157, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.156 [-0.585, 1.426], mean_best_reward: --\n",
      " 85393/100000: episode: 2237, duration: 0.208s, episode steps: 27, steps per second: 130, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.111 [-0.975, 0.549], mean_best_reward: --\n",
      " 85425/100000: episode: 2238, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.403, 0.941], mean_best_reward: --\n",
      " 85452/100000: episode: 2239, duration: 0.186s, episode steps: 27, steps per second: 145, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.061 [-0.800, 1.432], mean_best_reward: --\n",
      " 85494/100000: episode: 2240, duration: 0.256s, episode steps: 42, steps per second: 164, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.073 [-1.266, 0.646], mean_best_reward: --\n",
      " 85576/100000: episode: 2241, duration: 0.438s, episode steps: 82, steps per second: 187, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.110, 1.308], mean_best_reward: --\n",
      " 85639/100000: episode: 2242, duration: 0.346s, episode steps: 63, steps per second: 182, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.006 [-1.320, 1.157], mean_best_reward: --\n",
      " 85677/100000: episode: 2243, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.060 [-1.735, 0.802], mean_best_reward: --\n",
      " 85749/100000: episode: 2244, duration: 0.435s, episode steps: 72, steps per second: 166, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-1.124, 1.212], mean_best_reward: --\n",
      " 85800/100000: episode: 2245, duration: 0.248s, episode steps: 51, steps per second: 205, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.054 [-0.937, 0.599], mean_best_reward: --\n",
      " 85857/100000: episode: 2246, duration: 0.274s, episode steps: 57, steps per second: 208, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.170 [-0.737, 1.313], mean_best_reward: --\n",
      " 85893/100000: episode: 2247, duration: 0.163s, episode steps: 36, steps per second: 221, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.039 [-0.921, 0.615], mean_best_reward: --\n",
      " 85932/100000: episode: 2248, duration: 0.179s, episode steps: 39, steps per second: 218, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.089 [-1.285, 0.596], mean_best_reward: --\n",
      " 85995/100000: episode: 2249, duration: 0.331s, episode steps: 63, steps per second: 190, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.067 [-1.268, 0.819], mean_best_reward: --\n",
      " 86027/100000: episode: 2250, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.996, 0.367], mean_best_reward: --\n",
      " 86039/100000: episode: 2251, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.084 [-2.516, 1.617], mean_best_reward: 85.000000\n",
      " 86105/100000: episode: 2252, duration: 0.328s, episode steps: 66, steps per second: 201, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.166 [-1.135, 0.965], mean_best_reward: --\n",
      " 86158/100000: episode: 2253, duration: 0.250s, episode steps: 53, steps per second: 212, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.040 [-1.587, 0.730], mean_best_reward: --\n",
      " 86177/100000: episode: 2254, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.127 [-0.985, 0.576], mean_best_reward: --\n",
      " 86189/100000: episode: 2255, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.092 [-0.974, 1.537], mean_best_reward: --\n",
      " 86217/100000: episode: 2256, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.094 [-0.628, 1.001], mean_best_reward: --\n",
      " 86260/100000: episode: 2257, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.137 [-0.546, 0.827], mean_best_reward: --\n",
      " 86306/100000: episode: 2258, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.088 [-0.789, 1.103], mean_best_reward: --\n",
      " 86320/100000: episode: 2259, duration: 0.066s, episode steps: 14, steps per second: 213, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.974, 1.511], mean_best_reward: --\n",
      " 86373/100000: episode: 2260, duration: 0.244s, episode steps: 53, steps per second: 217, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.037 [-1.046, 0.592], mean_best_reward: --\n",
      " 86460/100000: episode: 2261, duration: 0.395s, episode steps: 87, steps per second: 220, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.049 [-1.340, 0.773], mean_best_reward: --\n",
      " 86507/100000: episode: 2262, duration: 0.211s, episode steps: 47, steps per second: 222, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.109 [-0.973, 0.766], mean_best_reward: --\n",
      " 86530/100000: episode: 2263, duration: 0.107s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.095 [-0.552, 1.061], mean_best_reward: --\n",
      " 86541/100000: episode: 2264, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-1.783, 0.991], mean_best_reward: --\n",
      " 86623/100000: episode: 2265, duration: 0.420s, episode steps: 82, steps per second: 195, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.092 [-0.824, 0.768], mean_best_reward: --\n",
      " 86681/100000: episode: 2266, duration: 0.293s, episode steps: 58, steps per second: 198, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.168 [-1.702, 0.813], mean_best_reward: --\n",
      " 86734/100000: episode: 2267, duration: 0.266s, episode steps: 53, steps per second: 199, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.205 [-0.777, 1.135], mean_best_reward: --\n",
      " 86772/100000: episode: 2268, duration: 0.172s, episode steps: 38, steps per second: 220, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.324, 0.822], mean_best_reward: --\n",
      " 86860/100000: episode: 2269, duration: 0.454s, episode steps: 88, steps per second: 194, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.133 [-1.207, 0.581], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86905/100000: episode: 2270, duration: 0.202s, episode steps: 45, steps per second: 223, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.127 [-1.015, 0.652], mean_best_reward: --\n",
      " 86944/100000: episode: 2271, duration: 0.175s, episode steps: 39, steps per second: 223, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.010 [-0.781, 1.063], mean_best_reward: --\n",
      " 86966/100000: episode: 2272, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.578, 0.914], mean_best_reward: --\n",
      " 86990/100000: episode: 2273, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.077 [-1.702, 0.798], mean_best_reward: --\n",
      " 87029/100000: episode: 2274, duration: 0.188s, episode steps: 39, steps per second: 207, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.040 [-0.966, 1.722], mean_best_reward: --\n",
      " 87074/100000: episode: 2275, duration: 0.207s, episode steps: 45, steps per second: 217, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.125 [-0.688, 1.171], mean_best_reward: --\n",
      " 87133/100000: episode: 2276, duration: 0.262s, episode steps: 59, steps per second: 225, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.055 [-0.797, 0.950], mean_best_reward: --\n",
      " 87187/100000: episode: 2277, duration: 0.240s, episode steps: 54, steps per second: 225, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.087 [-0.873, 0.549], mean_best_reward: --\n",
      " 87235/100000: episode: 2278, duration: 0.214s, episode steps: 48, steps per second: 224, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.135 [-1.499, 0.874], mean_best_reward: --\n",
      " 87251/100000: episode: 2279, duration: 0.076s, episode steps: 16, steps per second: 210, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.117 [-1.419, 0.758], mean_best_reward: --\n",
      " 87266/100000: episode: 2280, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.081 [-1.258, 0.825], mean_best_reward: --\n",
      " 87310/100000: episode: 2281, duration: 0.198s, episode steps: 44, steps per second: 223, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.187 [-1.127, 0.556], mean_best_reward: --\n",
      " 87329/100000: episode: 2282, duration: 0.087s, episode steps: 19, steps per second: 219, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.089 [-0.758, 1.290], mean_best_reward: --\n",
      " 87369/100000: episode: 2283, duration: 0.184s, episode steps: 40, steps per second: 217, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-1.352, 0.778], mean_best_reward: --\n",
      " 87390/100000: episode: 2284, duration: 0.097s, episode steps: 21, steps per second: 216, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.092 [-0.594, 0.969], mean_best_reward: --\n",
      " 87430/100000: episode: 2285, duration: 0.180s, episode steps: 40, steps per second: 223, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.161 [-0.497, 1.154], mean_best_reward: --\n",
      " 87479/100000: episode: 2286, duration: 0.227s, episode steps: 49, steps per second: 216, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.095 [-1.041, 0.556], mean_best_reward: --\n",
      " 87540/100000: episode: 2287, duration: 0.270s, episode steps: 61, steps per second: 226, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.065 [-0.615, 0.925], mean_best_reward: --\n",
      " 87622/100000: episode: 2288, duration: 0.366s, episode steps: 82, steps per second: 224, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.142 [-0.702, 1.029], mean_best_reward: --\n",
      " 87642/100000: episode: 2289, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.784, 1.592], mean_best_reward: --\n",
      " 87664/100000: episode: 2290, duration: 0.102s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-1.094, 0.441], mean_best_reward: --\n",
      " 87714/100000: episode: 2291, duration: 0.232s, episode steps: 50, steps per second: 216, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.087 [-0.790, 1.108], mean_best_reward: --\n",
      " 87765/100000: episode: 2292, duration: 0.227s, episode steps: 51, steps per second: 225, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.167 [-0.624, 1.520], mean_best_reward: --\n",
      " 87784/100000: episode: 2293, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.105 [-1.018, 0.427], mean_best_reward: --\n",
      " 87916/100000: episode: 2294, duration: 0.594s, episode steps: 132, steps per second: 222, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.214 [-1.216, 0.992], mean_best_reward: --\n",
      " 87940/100000: episode: 2295, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.088 [-1.163, 2.007], mean_best_reward: --\n",
      " 87975/100000: episode: 2296, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.084 [-0.741, 1.091], mean_best_reward: --\n",
      " 88070/100000: episode: 2297, duration: 0.420s, episode steps: 95, steps per second: 226, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.097 [-0.918, 1.138], mean_best_reward: --\n",
      " 88098/100000: episode: 2298, duration: 0.128s, episode steps: 28, steps per second: 218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.060 [-1.495, 0.664], mean_best_reward: --\n",
      " 88112/100000: episode: 2299, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.096 [-1.749, 1.035], mean_best_reward: --\n",
      " 88155/100000: episode: 2300, duration: 0.199s, episode steps: 43, steps per second: 216, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.084 [-0.802, 1.622], mean_best_reward: --\n",
      " 88170/100000: episode: 2301, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.067 [-1.170, 1.773], mean_best_reward: 111.000000\n",
      " 88186/100000: episode: 2302, duration: 0.075s, episode steps: 16, steps per second: 214, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.782, 1.227], mean_best_reward: --\n",
      " 88220/100000: episode: 2303, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.104 [-0.447, 0.783], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88347/100000: episode: 2304, duration: 0.562s, episode steps: 127, steps per second: 226, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.207 [-1.350, 2.152], mean_best_reward: --\n",
      " 88371/100000: episode: 2305, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.058 [-0.600, 1.326], mean_best_reward: --\n",
      " 88401/100000: episode: 2306, duration: 0.139s, episode steps: 30, steps per second: 216, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.123 [-0.719, 0.414], mean_best_reward: --\n",
      " 88430/100000: episode: 2307, duration: 0.132s, episode steps: 29, steps per second: 219, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.083 [-0.915, 0.591], mean_best_reward: --\n",
      " 88454/100000: episode: 2308, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.092 [-1.480, 0.600], mean_best_reward: --\n",
      " 88495/100000: episode: 2309, duration: 0.184s, episode steps: 41, steps per second: 223, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.058 [-0.597, 1.198], mean_best_reward: --\n",
      " 88518/100000: episode: 2310, duration: 0.107s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.058 [-1.178, 0.760], mean_best_reward: --\n",
      " 88532/100000: episode: 2311, duration: 0.066s, episode steps: 14, steps per second: 212, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.345, 0.777], mean_best_reward: --\n",
      " 88546/100000: episode: 2312, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.767, 1.267], mean_best_reward: --\n",
      " 88600/100000: episode: 2313, duration: 0.261s, episode steps: 54, steps per second: 207, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.381, 0.600], mean_best_reward: --\n",
      " 88613/100000: episode: 2314, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.107 [-0.944, 1.772], mean_best_reward: --\n",
      " 88632/100000: episode: 2315, duration: 0.087s, episode steps: 19, steps per second: 218, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.085 [-1.214, 0.631], mean_best_reward: --\n",
      " 88652/100000: episode: 2316, duration: 0.096s, episode steps: 20, steps per second: 209, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.036 [-1.303, 0.993], mean_best_reward: --\n",
      " 88663/100000: episode: 2317, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.115 [-1.526, 1.019], mean_best_reward: --\n",
      " 88683/100000: episode: 2318, duration: 0.092s, episode steps: 20, steps per second: 218, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.109 [-1.286, 0.742], mean_best_reward: --\n",
      " 88716/100000: episode: 2319, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.114 [-0.620, 0.935], mean_best_reward: --\n",
      " 88763/100000: episode: 2320, duration: 0.221s, episode steps: 47, steps per second: 213, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.033 [-0.729, 1.032], mean_best_reward: --\n",
      " 88829/100000: episode: 2321, duration: 0.300s, episode steps: 66, steps per second: 220, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.006 [-1.307, 1.800], mean_best_reward: --\n",
      " 88850/100000: episode: 2322, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.071 [-0.584, 1.043], mean_best_reward: --\n",
      " 88885/100000: episode: 2323, duration: 0.161s, episode steps: 35, steps per second: 217, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.053 [-0.596, 1.320], mean_best_reward: --\n",
      " 88906/100000: episode: 2324, duration: 0.096s, episode steps: 21, steps per second: 218, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.055 [-1.015, 1.453], mean_best_reward: --\n",
      " 88925/100000: episode: 2325, duration: 0.088s, episode steps: 19, steps per second: 217, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.067 [-1.323, 2.017], mean_best_reward: --\n",
      " 88992/100000: episode: 2326, duration: 0.300s, episode steps: 67, steps per second: 223, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.181 [-0.507, 0.956], mean_best_reward: --\n",
      " 89019/100000: episode: 2327, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.083 [-0.802, 1.432], mean_best_reward: --\n",
      " 89045/100000: episode: 2328, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.080 [-1.304, 0.604], mean_best_reward: --\n",
      " 89065/100000: episode: 2329, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.097 [-1.754, 0.827], mean_best_reward: --\n",
      " 89105/100000: episode: 2330, duration: 0.184s, episode steps: 40, steps per second: 218, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-0.636, 1.519], mean_best_reward: --\n",
      " 89129/100000: episode: 2331, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.052, 0.565], mean_best_reward: --\n",
      " 89139/100000: episode: 2332, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.139 [-1.935, 1.141], mean_best_reward: --\n",
      " 89165/100000: episode: 2333, duration: 0.119s, episode steps: 26, steps per second: 218, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.035 [-1.404, 0.819], mean_best_reward: --\n",
      " 89178/100000: episode: 2334, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.117 [-1.225, 0.747], mean_best_reward: --\n",
      " 89223/100000: episode: 2335, duration: 0.211s, episode steps: 45, steps per second: 214, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.109 [-1.302, 0.706], mean_best_reward: --\n",
      " 89240/100000: episode: 2336, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.109 [-1.037, 0.599], mean_best_reward: --\n",
      " 89307/100000: episode: 2337, duration: 0.298s, episode steps: 67, steps per second: 225, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.145 [-0.797, 1.069], mean_best_reward: --\n",
      " 89331/100000: episode: 2338, duration: 0.110s, episode steps: 24, steps per second: 219, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.593, 1.117], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89366/100000: episode: 2339, duration: 0.159s, episode steps: 35, steps per second: 220, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.113 [-1.243, 0.468], mean_best_reward: --\n",
      " 89383/100000: episode: 2340, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.072 [-0.985, 1.671], mean_best_reward: --\n",
      " 89394/100000: episode: 2341, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-0.953, 1.551], mean_best_reward: --\n",
      " 89421/100000: episode: 2342, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.664, 0.951], mean_best_reward: --\n",
      " 89459/100000: episode: 2343, duration: 0.170s, episode steps: 38, steps per second: 224, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.240, 0.607], mean_best_reward: --\n",
      " 89502/100000: episode: 2344, duration: 0.196s, episode steps: 43, steps per second: 220, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.101 [-0.669, 0.535], mean_best_reward: --\n",
      " 89513/100000: episode: 2345, duration: 0.053s, episode steps: 11, steps per second: 208, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-1.000, 1.664], mean_best_reward: --\n",
      " 89567/100000: episode: 2346, duration: 0.242s, episode steps: 54, steps per second: 223, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.082 [-1.030, 0.830], mean_best_reward: --\n",
      " 89604/100000: episode: 2347, duration: 0.166s, episode steps: 37, steps per second: 223, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.034 [-0.813, 1.376], mean_best_reward: --\n",
      " 89656/100000: episode: 2348, duration: 0.245s, episode steps: 52, steps per second: 213, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.088 [-0.630, 0.836], mean_best_reward: --\n",
      " 89715/100000: episode: 2349, duration: 0.281s, episode steps: 59, steps per second: 210, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.042 [-1.880, 0.842], mean_best_reward: --\n",
      " 89742/100000: episode: 2350, duration: 0.124s, episode steps: 27, steps per second: 219, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.048 [-1.988, 1.221], mean_best_reward: --\n",
      " 89780/100000: episode: 2351, duration: 0.172s, episode steps: 38, steps per second: 220, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.137 [-0.932, 0.511], mean_best_reward: 114.500000\n",
      " 89799/100000: episode: 2352, duration: 0.088s, episode steps: 19, steps per second: 217, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.051 [-1.213, 1.902], mean_best_reward: --\n",
      " 89850/100000: episode: 2353, duration: 0.238s, episode steps: 51, steps per second: 214, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.044 [-1.023, 0.592], mean_best_reward: --\n",
      " 89867/100000: episode: 2354, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.104 [-1.263, 0.738], mean_best_reward: --\n",
      " 89927/100000: episode: 2355, duration: 0.267s, episode steps: 60, steps per second: 225, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.601, 1.182], mean_best_reward: --\n",
      " 89960/100000: episode: 2356, duration: 0.151s, episode steps: 33, steps per second: 219, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.084 [-0.736, 1.261], mean_best_reward: --\n",
      " 90005/100000: episode: 2357, duration: 0.201s, episode steps: 45, steps per second: 223, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.097 [-0.880, 0.922], mean_best_reward: --\n",
      " 90056/100000: episode: 2358, duration: 0.230s, episode steps: 51, steps per second: 222, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.120 [-1.304, 0.692], mean_best_reward: --\n",
      " 90087/100000: episode: 2359, duration: 0.145s, episode steps: 31, steps per second: 213, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.081 [-0.787, 1.451], mean_best_reward: --\n",
      " 90110/100000: episode: 2360, duration: 0.107s, episode steps: 23, steps per second: 214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.076 [-0.749, 1.463], mean_best_reward: --\n",
      " 90140/100000: episode: 2361, duration: 0.135s, episode steps: 30, steps per second: 221, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.068 [-0.619, 1.140], mean_best_reward: --\n",
      " 90188/100000: episode: 2362, duration: 0.217s, episode steps: 48, steps per second: 222, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.405, 1.391], mean_best_reward: --\n",
      " 90215/100000: episode: 2363, duration: 0.123s, episode steps: 27, steps per second: 220, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.129 [-0.548, 0.966], mean_best_reward: --\n",
      " 90252/100000: episode: 2364, duration: 0.168s, episode steps: 37, steps per second: 221, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.138 [-0.539, 1.507], mean_best_reward: --\n",
      " 90281/100000: episode: 2365, duration: 0.136s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.124 [-0.564, 0.958], mean_best_reward: --\n",
      " 90328/100000: episode: 2366, duration: 0.217s, episode steps: 47, steps per second: 217, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.035 [-1.051, 0.637], mean_best_reward: --\n",
      " 90348/100000: episode: 2367, duration: 0.091s, episode steps: 20, steps per second: 219, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.082 [-0.754, 1.411], mean_best_reward: --\n",
      " 90368/100000: episode: 2368, duration: 0.091s, episode steps: 20, steps per second: 219, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.044 [-0.837, 1.388], mean_best_reward: --\n",
      " 90387/100000: episode: 2369, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.078 [-1.368, 0.644], mean_best_reward: --\n",
      " 90415/100000: episode: 2370, duration: 0.128s, episode steps: 28, steps per second: 219, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.091 [-0.438, 1.076], mean_best_reward: --\n",
      " 90521/100000: episode: 2371, duration: 0.477s, episode steps: 106, steps per second: 222, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.870, 0.967], mean_best_reward: --\n",
      " 90558/100000: episode: 2372, duration: 0.166s, episode steps: 37, steps per second: 222, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.106 [-0.914, 0.574], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90613/100000: episode: 2373, duration: 0.245s, episode steps: 55, steps per second: 224, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: 0.004 [-1.249, 1.001], mean_best_reward: --\n",
      " 90677/100000: episode: 2374, duration: 0.283s, episode steps: 64, steps per second: 226, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.088 [-0.476, 0.890], mean_best_reward: --\n",
      " 90706/100000: episode: 2375, duration: 0.132s, episode steps: 29, steps per second: 220, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.101 [-1.083, 0.492], mean_best_reward: --\n",
      " 90735/100000: episode: 2376, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.049 [-1.031, 1.765], mean_best_reward: --\n",
      " 90762/100000: episode: 2377, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.077 [-0.610, 1.418], mean_best_reward: --\n",
      " 90776/100000: episode: 2378, duration: 0.066s, episode steps: 14, steps per second: 213, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.270, 0.771], mean_best_reward: --\n",
      " 90792/100000: episode: 2379, duration: 0.075s, episode steps: 16, steps per second: 212, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.119 [-0.567, 1.111], mean_best_reward: --\n",
      " 90827/100000: episode: 2380, duration: 0.158s, episode steps: 35, steps per second: 221, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.061 [-0.900, 1.114], mean_best_reward: --\n",
      " 90862/100000: episode: 2381, duration: 0.159s, episode steps: 35, steps per second: 221, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.088 [-1.172, 0.567], mean_best_reward: --\n",
      " 90890/100000: episode: 2382, duration: 0.126s, episode steps: 28, steps per second: 222, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.111 [-1.151, 0.751], mean_best_reward: --\n",
      " 90932/100000: episode: 2383, duration: 0.191s, episode steps: 42, steps per second: 220, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.094 [-0.820, 0.566], mean_best_reward: --\n",
      " 90957/100000: episode: 2384, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.069 [-0.629, 1.028], mean_best_reward: --\n",
      " 90977/100000: episode: 2385, duration: 0.094s, episode steps: 20, steps per second: 212, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.081 [-0.608, 1.157], mean_best_reward: --\n",
      " 90991/100000: episode: 2386, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.096 [-0.557, 1.105], mean_best_reward: --\n",
      " 91057/100000: episode: 2387, duration: 0.295s, episode steps: 66, steps per second: 224, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.057 [-1.246, 0.654], mean_best_reward: --\n",
      " 91095/100000: episode: 2388, duration: 0.170s, episode steps: 38, steps per second: 223, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.130 [-0.583, 1.252], mean_best_reward: --\n",
      " 91120/100000: episode: 2389, duration: 0.116s, episode steps: 25, steps per second: 216, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.060 [-1.175, 0.626], mean_best_reward: --\n",
      " 91156/100000: episode: 2390, duration: 0.166s, episode steps: 36, steps per second: 217, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.126 [-1.216, 0.779], mean_best_reward: --\n",
      " 91189/100000: episode: 2391, duration: 0.155s, episode steps: 33, steps per second: 212, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.087 [-0.632, 0.996], mean_best_reward: --\n",
      " 91204/100000: episode: 2392, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-0.977, 1.775], mean_best_reward: --\n",
      " 91360/100000: episode: 2393, duration: 0.705s, episode steps: 156, steps per second: 221, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.163 [-0.872, 1.345], mean_best_reward: --\n",
      " 91381/100000: episode: 2394, duration: 0.104s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.055 [-1.279, 0.795], mean_best_reward: --\n",
      " 91418/100000: episode: 2395, duration: 0.170s, episode steps: 37, steps per second: 218, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.103 [-1.335, 0.595], mean_best_reward: --\n",
      " 91448/100000: episode: 2396, duration: 0.135s, episode steps: 30, steps per second: 223, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.114 [-0.988, 0.381], mean_best_reward: --\n",
      " 91513/100000: episode: 2397, duration: 0.291s, episode steps: 65, steps per second: 223, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.086 [-0.965, 1.245], mean_best_reward: --\n",
      " 91546/100000: episode: 2398, duration: 0.148s, episode steps: 33, steps per second: 222, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.058 [-0.594, 1.431], mean_best_reward: --\n",
      " 91614/100000: episode: 2399, duration: 0.311s, episode steps: 68, steps per second: 219, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-1.652, 1.574], mean_best_reward: --\n",
      " 91669/100000: episode: 2400, duration: 0.247s, episode steps: 55, steps per second: 223, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.158 [-0.929, 0.526], mean_best_reward: --\n",
      " 91698/100000: episode: 2401, duration: 0.134s, episode steps: 29, steps per second: 217, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.065 [-1.130, 0.795], mean_best_reward: 86.500000\n",
      " 91721/100000: episode: 2402, duration: 0.107s, episode steps: 23, steps per second: 214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.080 [-1.240, 0.813], mean_best_reward: --\n",
      " 91748/100000: episode: 2403, duration: 0.122s, episode steps: 27, steps per second: 221, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.122 [-0.599, 1.033], mean_best_reward: --\n",
      " 91775/100000: episode: 2404, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.111 [-0.783, 1.174], mean_best_reward: --\n",
      " 91838/100000: episode: 2405, duration: 0.289s, episode steps: 63, steps per second: 218, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.213 [-1.432, 0.869], mean_best_reward: --\n",
      " 91902/100000: episode: 2406, duration: 0.304s, episode steps: 64, steps per second: 210, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.201 [-0.942, 1.506], mean_best_reward: --\n",
      " 91928/100000: episode: 2407, duration: 0.119s, episode steps: 26, steps per second: 219, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.611, 1.172], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91956/100000: episode: 2408, duration: 0.129s, episode steps: 28, steps per second: 217, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.090 [-1.854, 0.763], mean_best_reward: --\n",
      " 92063/100000: episode: 2409, duration: 0.479s, episode steps: 107, steps per second: 223, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.155 [-0.994, 1.149], mean_best_reward: --\n",
      " 92104/100000: episode: 2410, duration: 0.183s, episode steps: 41, steps per second: 224, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.106 [-0.978, 0.764], mean_best_reward: --\n",
      " 92133/100000: episode: 2411, duration: 0.134s, episode steps: 29, steps per second: 217, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.084 [-0.637, 1.113], mean_best_reward: --\n",
      " 92161/100000: episode: 2412, duration: 0.127s, episode steps: 28, steps per second: 220, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.206, 0.372], mean_best_reward: --\n",
      " 92225/100000: episode: 2413, duration: 0.285s, episode steps: 64, steps per second: 224, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.123 [-0.613, 1.230], mean_best_reward: --\n",
      " 92281/100000: episode: 2414, duration: 0.262s, episode steps: 56, steps per second: 214, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.169 [-0.802, 1.470], mean_best_reward: --\n",
      " 92316/100000: episode: 2415, duration: 0.158s, episode steps: 35, steps per second: 222, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.066 [-0.813, 1.238], mean_best_reward: --\n",
      " 92387/100000: episode: 2416, duration: 0.320s, episode steps: 71, steps per second: 222, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.033 [-0.747, 1.029], mean_best_reward: --\n",
      " 92415/100000: episode: 2417, duration: 0.127s, episode steps: 28, steps per second: 221, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.108 [-0.604, 0.988], mean_best_reward: --\n",
      " 92427/100000: episode: 2418, duration: 0.057s, episode steps: 12, steps per second: 211, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.109 [-0.930, 1.513], mean_best_reward: --\n",
      " 92490/100000: episode: 2419, duration: 0.288s, episode steps: 63, steps per second: 218, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.025 [-0.771, 1.410], mean_best_reward: --\n",
      " 92530/100000: episode: 2420, duration: 0.179s, episode steps: 40, steps per second: 224, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.011 [-1.203, 1.437], mean_best_reward: --\n",
      " 92583/100000: episode: 2421, duration: 0.237s, episode steps: 53, steps per second: 223, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.050 [-1.194, 0.615], mean_best_reward: --\n",
      " 92599/100000: episode: 2422, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.137, 0.554], mean_best_reward: --\n",
      " 92646/100000: episode: 2423, duration: 0.211s, episode steps: 47, steps per second: 223, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.059 [-0.813, 0.628], mean_best_reward: --\n",
      " 92672/100000: episode: 2424, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.111 [-0.408, 1.498], mean_best_reward: --\n",
      " 92704/100000: episode: 2425, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.223, 0.775], mean_best_reward: --\n",
      " 92741/100000: episode: 2426, duration: 0.177s, episode steps: 37, steps per second: 209, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.073 [-1.209, 0.780], mean_best_reward: --\n",
      " 92798/100000: episode: 2427, duration: 0.268s, episode steps: 57, steps per second: 213, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.166 [-1.577, 0.916], mean_best_reward: --\n",
      " 92824/100000: episode: 2428, duration: 0.119s, episode steps: 26, steps per second: 219, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.100 [-1.231, 0.570], mean_best_reward: --\n",
      " 92851/100000: episode: 2429, duration: 0.125s, episode steps: 27, steps per second: 216, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.070 [-0.660, 1.628], mean_best_reward: --\n",
      " 92905/100000: episode: 2430, duration: 0.254s, episode steps: 54, steps per second: 213, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.067 [-1.508, 0.650], mean_best_reward: --\n",
      " 92922/100000: episode: 2431, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.093 [-1.464, 0.942], mean_best_reward: --\n",
      " 92956/100000: episode: 2432, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.087 [-1.484, 0.562], mean_best_reward: --\n",
      " 92979/100000: episode: 2433, duration: 0.108s, episode steps: 23, steps per second: 212, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.071 [-1.507, 0.966], mean_best_reward: --\n",
      " 92998/100000: episode: 2434, duration: 0.088s, episode steps: 19, steps per second: 215, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.063 [-0.652, 1.319], mean_best_reward: --\n",
      " 93017/100000: episode: 2435, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.086 [-1.142, 0.766], mean_best_reward: --\n",
      " 93036/100000: episode: 2436, duration: 0.091s, episode steps: 19, steps per second: 208, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.075 [-0.984, 1.799], mean_best_reward: --\n",
      " 93074/100000: episode: 2437, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.028 [-1.189, 1.459], mean_best_reward: --\n",
      " 93094/100000: episode: 2438, duration: 0.095s, episode steps: 20, steps per second: 209, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.106 [-1.238, 0.544], mean_best_reward: --\n",
      " 93155/100000: episode: 2439, duration: 0.296s, episode steps: 61, steps per second: 206, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.137 [-0.655, 1.144], mean_best_reward: --\n",
      " 93185/100000: episode: 2440, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.110 [-0.586, 0.960], mean_best_reward: --\n",
      " 93214/100000: episode: 2441, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.079 [-0.558, 1.046], mean_best_reward: --\n",
      " 93227/100000: episode: 2442, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.336, 0.818], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93358/100000: episode: 2443, duration: 0.591s, episode steps: 131, steps per second: 222, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.034 [-1.140, 0.965], mean_best_reward: --\n",
      " 93396/100000: episode: 2444, duration: 0.173s, episode steps: 38, steps per second: 219, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.359, 0.999], mean_best_reward: --\n",
      " 93476/100000: episode: 2445, duration: 0.375s, episode steps: 80, steps per second: 213, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.103 [-0.771, 1.689], mean_best_reward: --\n",
      " 93510/100000: episode: 2446, duration: 0.155s, episode steps: 34, steps per second: 220, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.041 [-0.965, 1.744], mean_best_reward: --\n",
      " 93543/100000: episode: 2447, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.090 [-0.620, 1.154], mean_best_reward: --\n",
      " 93571/100000: episode: 2448, duration: 0.129s, episode steps: 28, steps per second: 218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.123, 0.768], mean_best_reward: --\n",
      " 93602/100000: episode: 2449, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.106 [-1.437, 0.600], mean_best_reward: --\n",
      " 93639/100000: episode: 2450, duration: 0.175s, episode steps: 37, steps per second: 211, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.177 [-0.742, 1.207], mean_best_reward: --\n",
      " 93673/100000: episode: 2451, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.075 [-1.465, 0.586], mean_best_reward: 100.000000\n",
      " 93690/100000: episode: 2452, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.099 [-0.939, 0.548], mean_best_reward: --\n",
      " 93730/100000: episode: 2453, duration: 0.185s, episode steps: 40, steps per second: 216, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.135 [-1.071, 0.585], mean_best_reward: --\n",
      " 93753/100000: episode: 2454, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.125 [-0.343, 0.980], mean_best_reward: --\n",
      " 93786/100000: episode: 2455, duration: 0.150s, episode steps: 33, steps per second: 220, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.067 [-1.137, 0.430], mean_best_reward: --\n",
      " 93825/100000: episode: 2456, duration: 0.182s, episode steps: 39, steps per second: 215, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.085 [-0.818, 0.437], mean_best_reward: --\n",
      " 93840/100000: episode: 2457, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.976, 1.636], mean_best_reward: --\n",
      " 93873/100000: episode: 2458, duration: 0.150s, episode steps: 33, steps per second: 220, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.054 [-2.074, 1.048], mean_best_reward: --\n",
      " 93899/100000: episode: 2459, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.082 [-2.210, 1.151], mean_best_reward: --\n",
      " 93934/100000: episode: 2460, duration: 0.158s, episode steps: 35, steps per second: 222, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.067 [-1.510, 0.647], mean_best_reward: --\n",
      " 93972/100000: episode: 2461, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.081 [-0.508, 1.322], mean_best_reward: --\n",
      " 94009/100000: episode: 2462, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.087 [-1.164, 2.090], mean_best_reward: --\n",
      " 94031/100000: episode: 2463, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.062 [-1.816, 1.129], mean_best_reward: --\n",
      " 94072/100000: episode: 2464, duration: 0.185s, episode steps: 41, steps per second: 221, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.168 [-1.108, 0.428], mean_best_reward: --\n",
      " 94148/100000: episode: 2465, duration: 0.338s, episode steps: 76, steps per second: 225, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-1.151, 0.956], mean_best_reward: --\n",
      " 94184/100000: episode: 2466, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.136 [-0.963, 1.117], mean_best_reward: --\n",
      " 94262/100000: episode: 2467, duration: 0.348s, episode steps: 78, steps per second: 224, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.201 [-0.976, 1.370], mean_best_reward: --\n",
      " 94316/100000: episode: 2468, duration: 0.243s, episode steps: 54, steps per second: 222, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.077 [-0.639, 0.887], mean_best_reward: --\n",
      " 94407/100000: episode: 2469, duration: 0.412s, episode steps: 91, steps per second: 221, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.133 [-1.115, 0.901], mean_best_reward: --\n",
      " 94438/100000: episode: 2470, duration: 0.141s, episode steps: 31, steps per second: 220, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.020 [-1.010, 1.680], mean_best_reward: --\n",
      " 94477/100000: episode: 2471, duration: 0.183s, episode steps: 39, steps per second: 214, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.087 [-1.420, 0.583], mean_best_reward: --\n",
      " 94494/100000: episode: 2472, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.073 [-0.958, 1.450], mean_best_reward: --\n",
      " 94527/100000: episode: 2473, duration: 0.150s, episode steps: 33, steps per second: 220, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.194 [-1.296, 0.541], mean_best_reward: --\n",
      " 94606/100000: episode: 2474, duration: 0.356s, episode steps: 79, steps per second: 222, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.055 [-1.094, 0.902], mean_best_reward: --\n",
      " 94641/100000: episode: 2475, duration: 0.160s, episode steps: 35, steps per second: 219, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.041 [-0.817, 1.575], mean_best_reward: --\n",
      " 94665/100000: episode: 2476, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.107 [-1.403, 0.545], mean_best_reward: --\n",
      " 94706/100000: episode: 2477, duration: 0.183s, episode steps: 41, steps per second: 224, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.008 [-1.117, 1.415], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94724/100000: episode: 2478, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.064 [-1.907, 1.163], mean_best_reward: --\n",
      " 94772/100000: episode: 2479, duration: 0.215s, episode steps: 48, steps per second: 223, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.113 [-0.468, 1.019], mean_best_reward: --\n",
      " 94815/100000: episode: 2480, duration: 0.194s, episode steps: 43, steps per second: 222, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.116 [-1.268, 0.799], mean_best_reward: --\n",
      " 94834/100000: episode: 2481, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.061 [-0.831, 1.520], mean_best_reward: --\n",
      " 94902/100000: episode: 2482, duration: 0.304s, episode steps: 68, steps per second: 224, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.152 [-1.176, 0.592], mean_best_reward: --\n",
      " 94930/100000: episode: 2483, duration: 0.129s, episode steps: 28, steps per second: 218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.060 [-0.578, 1.142], mean_best_reward: --\n",
      " 95011/100000: episode: 2484, duration: 0.361s, episode steps: 81, steps per second: 224, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.001 [-0.993, 0.769], mean_best_reward: --\n",
      " 95060/100000: episode: 2485, duration: 0.230s, episode steps: 49, steps per second: 213, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.090 [-1.319, 0.715], mean_best_reward: --\n",
      " 95076/100000: episode: 2486, duration: 0.075s, episode steps: 16, steps per second: 212, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.108 [-0.844, 1.720], mean_best_reward: --\n",
      " 95101/100000: episode: 2487, duration: 0.114s, episode steps: 25, steps per second: 219, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.105 [-1.167, 0.629], mean_best_reward: --\n",
      " 95109/100000: episode: 2488, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.145 [-1.914, 1.181], mean_best_reward: --\n",
      " 95153/100000: episode: 2489, duration: 0.198s, episode steps: 44, steps per second: 222, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-0.926, 0.603], mean_best_reward: --\n",
      " 95201/100000: episode: 2490, duration: 0.215s, episode steps: 48, steps per second: 223, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.060 [-0.838, 1.562], mean_best_reward: --\n",
      " 95230/100000: episode: 2491, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.086 [-0.623, 1.024], mean_best_reward: --\n",
      " 95369/100000: episode: 2492, duration: 0.629s, episode steps: 139, steps per second: 221, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.136 [-0.869, 1.278], mean_best_reward: --\n",
      " 95389/100000: episode: 2493, duration: 0.093s, episode steps: 20, steps per second: 216, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.824, 1.695], mean_best_reward: --\n",
      " 95414/100000: episode: 2494, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.812, 1.139], mean_best_reward: --\n",
      " 95457/100000: episode: 2495, duration: 0.193s, episode steps: 43, steps per second: 222, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.001 [-0.977, 0.634], mean_best_reward: --\n",
      " 95512/100000: episode: 2496, duration: 0.257s, episode steps: 55, steps per second: 214, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.015 [-0.737, 1.085], mean_best_reward: --\n",
      " 95536/100000: episode: 2497, duration: 0.110s, episode steps: 24, steps per second: 218, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.022, 0.617], mean_best_reward: --\n",
      " 95609/100000: episode: 2498, duration: 0.337s, episode steps: 73, steps per second: 217, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.081 [-0.974, 1.070], mean_best_reward: --\n",
      " 95626/100000: episode: 2499, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.128 [-0.552, 1.031], mean_best_reward: --\n",
      " 95694/100000: episode: 2500, duration: 0.314s, episode steps: 68, steps per second: 216, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.106 [-0.701, 1.170], mean_best_reward: --\n",
      " 95739/100000: episode: 2501, duration: 0.205s, episode steps: 45, steps per second: 220, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.019 [-0.893, 1.020], mean_best_reward: 81.500000\n",
      " 95766/100000: episode: 2502, duration: 0.124s, episode steps: 27, steps per second: 218, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.138 [-1.614, 0.717], mean_best_reward: --\n",
      " 95814/100000: episode: 2503, duration: 0.217s, episode steps: 48, steps per second: 221, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.015 [-0.639, 1.100], mean_best_reward: --\n",
      " 95826/100000: episode: 2504, duration: 0.057s, episode steps: 12, steps per second: 209, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-1.022, 1.534], mean_best_reward: --\n",
      " 95892/100000: episode: 2505, duration: 0.296s, episode steps: 66, steps per second: 223, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.165 [-1.288, 0.766], mean_best_reward: --\n",
      " 95920/100000: episode: 2506, duration: 0.134s, episode steps: 28, steps per second: 208, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.075 [-1.704, 0.807], mean_best_reward: --\n",
      " 95968/100000: episode: 2507, duration: 0.217s, episode steps: 48, steps per second: 222, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.026 [-1.033, 1.481], mean_best_reward: --\n",
      " 95993/100000: episode: 2508, duration: 0.116s, episode steps: 25, steps per second: 216, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.037 [-1.175, 1.892], mean_best_reward: --\n",
      " 96019/100000: episode: 2509, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.220, 0.639], mean_best_reward: --\n",
      " 96049/100000: episode: 2510, duration: 0.135s, episode steps: 30, steps per second: 222, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.065 [-0.629, 1.477], mean_best_reward: --\n",
      " 96093/100000: episode: 2511, duration: 0.198s, episode steps: 44, steps per second: 223, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.066 [-1.078, 0.550], mean_best_reward: --\n",
      " 96124/100000: episode: 2512, duration: 0.146s, episode steps: 31, steps per second: 212, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.109 [-0.369, 1.180], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96138/100000: episode: 2513, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.089 [-1.613, 1.019], mean_best_reward: --\n",
      " 96162/100000: episode: 2514, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.110 [-1.603, 0.648], mean_best_reward: --\n",
      " 96213/100000: episode: 2515, duration: 0.243s, episode steps: 51, steps per second: 210, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.128 [-0.496, 1.316], mean_best_reward: --\n",
      " 96232/100000: episode: 2516, duration: 0.088s, episode steps: 19, steps per second: 215, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.108 [-0.607, 1.213], mean_best_reward: --\n",
      " 96260/100000: episode: 2517, duration: 0.130s, episode steps: 28, steps per second: 216, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.074 [-0.924, 0.614], mean_best_reward: --\n",
      " 96306/100000: episode: 2518, duration: 0.206s, episode steps: 46, steps per second: 223, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.122 [-1.246, 0.636], mean_best_reward: --\n",
      " 96349/100000: episode: 2519, duration: 0.205s, episode steps: 43, steps per second: 209, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.148 [-0.953, 0.510], mean_best_reward: --\n",
      " 96373/100000: episode: 2520, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.081 [-0.612, 1.268], mean_best_reward: --\n",
      " 96420/100000: episode: 2521, duration: 0.211s, episode steps: 47, steps per second: 223, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.053 [-1.331, 1.437], mean_best_reward: --\n",
      " 96488/100000: episode: 2522, duration: 0.303s, episode steps: 68, steps per second: 224, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-1.163, 1.180], mean_best_reward: --\n",
      " 96506/100000: episode: 2523, duration: 0.084s, episode steps: 18, steps per second: 213, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.068 [-1.850, 1.191], mean_best_reward: --\n",
      " 96522/100000: episode: 2524, duration: 0.075s, episode steps: 16, steps per second: 213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.587, 1.131], mean_best_reward: --\n",
      " 96564/100000: episode: 2525, duration: 0.196s, episode steps: 42, steps per second: 214, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.628, 0.975], mean_best_reward: --\n",
      " 96626/100000: episode: 2526, duration: 0.277s, episode steps: 62, steps per second: 224, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-1.211, 1.097], mean_best_reward: --\n",
      " 96645/100000: episode: 2527, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.093 [-1.206, 0.606], mean_best_reward: --\n",
      " 96658/100000: episode: 2528, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-1.710, 1.003], mean_best_reward: --\n",
      " 96700/100000: episode: 2529, duration: 0.190s, episode steps: 42, steps per second: 221, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.100 [-1.548, 1.009], mean_best_reward: --\n",
      " 96725/100000: episode: 2530, duration: 0.114s, episode steps: 25, steps per second: 219, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.090 [-0.741, 1.254], mean_best_reward: --\n",
      " 96738/100000: episode: 2531, duration: 0.061s, episode steps: 13, steps per second: 212, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.112 [-1.221, 0.790], mean_best_reward: --\n",
      " 96772/100000: episode: 2532, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.022 [-0.782, 1.212], mean_best_reward: --\n",
      " 96807/100000: episode: 2533, duration: 0.160s, episode steps: 35, steps per second: 219, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.113 [-0.966, 0.630], mean_best_reward: --\n",
      " 96826/100000: episode: 2534, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.078 [-0.950, 1.547], mean_best_reward: --\n",
      " 96861/100000: episode: 2535, duration: 0.157s, episode steps: 35, steps per second: 223, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.086 [-1.043, 0.635], mean_best_reward: --\n",
      " 96891/100000: episode: 2536, duration: 0.138s, episode steps: 30, steps per second: 218, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.032 [-0.629, 1.119], mean_best_reward: --\n",
      " 96913/100000: episode: 2537, duration: 0.102s, episode steps: 22, steps per second: 215, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.089 [-1.443, 1.017], mean_best_reward: --\n",
      " 96959/100000: episode: 2538, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.061 [-0.605, 0.966], mean_best_reward: --\n",
      " 96980/100000: episode: 2539, duration: 0.098s, episode steps: 21, steps per second: 213, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.083 [-1.514, 0.773], mean_best_reward: --\n",
      " 97037/100000: episode: 2540, duration: 0.263s, episode steps: 57, steps per second: 216, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.112 [-0.853, 0.627], mean_best_reward: --\n",
      " 97065/100000: episode: 2541, duration: 0.128s, episode steps: 28, steps per second: 219, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.740, 1.147], mean_best_reward: --\n",
      " 97084/100000: episode: 2542, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.077 [-1.227, 0.648], mean_best_reward: --\n",
      " 97123/100000: episode: 2543, duration: 0.176s, episode steps: 39, steps per second: 221, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.054 [-1.143, 0.581], mean_best_reward: --\n",
      " 97162/100000: episode: 2544, duration: 0.176s, episode steps: 39, steps per second: 222, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.060 [-0.588, 1.052], mean_best_reward: --\n",
      " 97208/100000: episode: 2545, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.555, 1.398], mean_best_reward: --\n",
      " 97257/100000: episode: 2546, duration: 0.224s, episode steps: 49, steps per second: 218, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: 0.007 [-1.339, 1.905], mean_best_reward: --\n",
      " 97273/100000: episode: 2547, duration: 0.075s, episode steps: 16, steps per second: 213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.087 [-1.944, 1.163], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97305/100000: episode: 2548, duration: 0.148s, episode steps: 32, steps per second: 217, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.107 [-0.917, 0.631], mean_best_reward: --\n",
      " 97347/100000: episode: 2549, duration: 0.189s, episode steps: 42, steps per second: 222, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.155, 0.773], mean_best_reward: --\n",
      " 97369/100000: episode: 2550, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.341, 0.827], mean_best_reward: --\n",
      " 97407/100000: episode: 2551, duration: 0.171s, episode steps: 38, steps per second: 223, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.145 [-0.929, 0.618], mean_best_reward: 79.000000\n",
      " 97431/100000: episode: 2552, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.079 [-0.748, 1.279], mean_best_reward: --\n",
      " 97479/100000: episode: 2553, duration: 0.218s, episode steps: 48, steps per second: 221, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.118 [-0.817, 1.634], mean_best_reward: --\n",
      " 97520/100000: episode: 2554, duration: 0.186s, episode steps: 41, steps per second: 220, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.075 [-0.797, 0.995], mean_best_reward: --\n",
      " 97551/100000: episode: 2555, duration: 0.143s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.138 [-1.280, 0.616], mean_best_reward: --\n",
      " 97607/100000: episode: 2556, duration: 0.301s, episode steps: 56, steps per second: 186, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.033 [-0.891, 0.973], mean_best_reward: --\n",
      " 97629/100000: episode: 2557, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.058, 0.439], mean_best_reward: --\n",
      " 97674/100000: episode: 2558, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.018 [-1.012, 1.242], mean_best_reward: --\n",
      " 97691/100000: episode: 2559, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.050 [-0.994, 1.446], mean_best_reward: --\n",
      " 97722/100000: episode: 2560, duration: 0.209s, episode steps: 31, steps per second: 148, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.129 [-1.035, 0.604], mean_best_reward: --\n",
      " 97787/100000: episode: 2561, duration: 0.360s, episode steps: 65, steps per second: 180, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.108 [-0.625, 1.096], mean_best_reward: --\n",
      " 97835/100000: episode: 2562, duration: 0.285s, episode steps: 48, steps per second: 169, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.116 [-0.633, 0.866], mean_best_reward: --\n",
      " 97901/100000: episode: 2563, duration: 0.343s, episode steps: 66, steps per second: 192, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.168 [-0.946, 0.979], mean_best_reward: --\n",
      " 97954/100000: episode: 2564, duration: 0.256s, episode steps: 53, steps per second: 207, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.107 [-1.184, 0.469], mean_best_reward: --\n",
      " 97985/100000: episode: 2565, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.065 [-0.574, 1.119], mean_best_reward: --\n",
      " 98054/100000: episode: 2566, duration: 0.426s, episode steps: 69, steps per second: 162, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.150 [-1.545, 0.533], mean_best_reward: --\n",
      " 98087/100000: episode: 2567, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.089 [-0.476, 1.401], mean_best_reward: --\n",
      " 98114/100000: episode: 2568, duration: 0.208s, episode steps: 27, steps per second: 130, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.106 [-0.988, 0.574], mean_best_reward: --\n",
      " 98138/100000: episode: 2569, duration: 0.132s, episode steps: 24, steps per second: 183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.929, 1.432], mean_best_reward: --\n",
      " 98159/100000: episode: 2570, duration: 0.157s, episode steps: 21, steps per second: 134, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.078 [-0.600, 1.380], mean_best_reward: --\n",
      " 98191/100000: episode: 2571, duration: 0.223s, episode steps: 32, steps per second: 144, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.081 [-1.167, 0.800], mean_best_reward: --\n",
      " 98218/100000: episode: 2572, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.089 [-0.754, 1.199], mean_best_reward: --\n",
      " 98252/100000: episode: 2573, duration: 0.201s, episode steps: 34, steps per second: 170, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.105 [-0.978, 0.578], mean_best_reward: --\n",
      " 98301/100000: episode: 2574, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.035 [-0.773, 1.307], mean_best_reward: --\n",
      " 98351/100000: episode: 2575, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.142 [-0.747, 1.511], mean_best_reward: --\n",
      " 98393/100000: episode: 2576, duration: 0.277s, episode steps: 42, steps per second: 151, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.826, 1.203], mean_best_reward: --\n",
      " 98430/100000: episode: 2577, duration: 0.246s, episode steps: 37, steps per second: 150, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: 0.041 [-1.720, 1.508], mean_best_reward: --\n",
      " 98440/100000: episode: 2578, duration: 0.088s, episode steps: 10, steps per second: 113, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.141 [-1.143, 1.989], mean_best_reward: --\n",
      " 98489/100000: episode: 2579, duration: 0.299s, episode steps: 49, steps per second: 164, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.121 [-1.259, 0.606], mean_best_reward: --\n",
      " 98516/100000: episode: 2580, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.096 [-1.019, 0.588], mean_best_reward: --\n",
      " 98549/100000: episode: 2581, duration: 0.254s, episode steps: 33, steps per second: 130, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.041 [-0.922, 1.306], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98603/100000: episode: 2582, duration: 0.313s, episode steps: 54, steps per second: 172, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.535, 0.608], mean_best_reward: --\n",
      " 98639/100000: episode: 2583, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.101 [-0.924, 0.538], mean_best_reward: --\n",
      " 98661/100000: episode: 2584, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.038 [-2.211, 1.531], mean_best_reward: --\n",
      " 98711/100000: episode: 2585, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.133 [-0.935, 1.336], mean_best_reward: --\n",
      " 98794/100000: episode: 2586, duration: 0.500s, episode steps: 83, steps per second: 166, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.064 [-1.152, 1.220], mean_best_reward: --\n",
      " 98806/100000: episode: 2587, duration: 0.105s, episode steps: 12, steps per second: 114, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-0.985, 1.676], mean_best_reward: --\n",
      " 98836/100000: episode: 2588, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.031, 0.406], mean_best_reward: --\n",
      " 98863/100000: episode: 2589, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.782, 1.507], mean_best_reward: --\n",
      " 98897/100000: episode: 2590, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.140 [-0.471, 0.951], mean_best_reward: --\n",
      " 99070/100000: episode: 2591, duration: 0.931s, episode steps: 173, steps per second: 186, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.016 [-1.630, 0.880], mean_best_reward: --\n",
      " 99111/100000: episode: 2592, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.147 [-0.987, 1.356], mean_best_reward: --\n",
      " 99129/100000: episode: 2593, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-2.061, 1.212], mean_best_reward: --\n",
      " 99156/100000: episode: 2594, duration: 0.126s, episode steps: 27, steps per second: 214, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.082 [-0.557, 1.134], mean_best_reward: --\n",
      " 99206/100000: episode: 2595, duration: 0.234s, episode steps: 50, steps per second: 214, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.048 [-1.427, 0.774], mean_best_reward: --\n",
      " 99240/100000: episode: 2596, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.153 [-1.191, 0.509], mean_best_reward: --\n",
      " 99269/100000: episode: 2597, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.058 [-0.783, 1.194], mean_best_reward: --\n",
      " 99305/100000: episode: 2598, duration: 0.164s, episode steps: 36, steps per second: 220, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.121 [-0.427, 0.964], mean_best_reward: --\n",
      " 99327/100000: episode: 2599, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.563, 1.164], mean_best_reward: --\n",
      " 99378/100000: episode: 2600, duration: 0.232s, episode steps: 51, steps per second: 220, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.136 [-0.900, 0.628], mean_best_reward: --\n",
      " 99394/100000: episode: 2601, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.081 [-1.249, 0.810], mean_best_reward: 85.000000\n",
      " 99448/100000: episode: 2602, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.003 [-1.127, 1.853], mean_best_reward: --\n",
      " 99502/100000: episode: 2603, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.169 [-0.914, 1.666], mean_best_reward: --\n",
      " 99596/100000: episode: 2604, duration: 0.453s, episode steps: 94, steps per second: 208, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.155 [-0.941, 0.880], mean_best_reward: --\n",
      " 99643/100000: episode: 2605, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.103 [-1.079, 0.585], mean_best_reward: --\n",
      " 99687/100000: episode: 2606, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.178, 0.591], mean_best_reward: --\n",
      " 99739/100000: episode: 2607, duration: 0.235s, episode steps: 52, steps per second: 221, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.583, 0.980], mean_best_reward: --\n",
      " 99780/100000: episode: 2608, duration: 0.215s, episode steps: 41, steps per second: 190, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.102 [-1.315, 0.508], mean_best_reward: --\n",
      " 99830/100000: episode: 2609, duration: 0.230s, episode steps: 50, steps per second: 218, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.660, 1.232], mean_best_reward: --\n",
      " 99861/100000: episode: 2610, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.103 [-0.626, 1.133], mean_best_reward: --\n",
      " 99886/100000: episode: 2611, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.115 [-0.554, 1.226], mean_best_reward: --\n",
      " 99927/100000: episode: 2612, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.059 [-0.624, 1.098], mean_best_reward: --\n",
      " 99965/100000: episode: 2613, duration: 0.270s, episode steps: 38, steps per second: 141, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.139, 0.613], mean_best_reward: --\n",
      "done, took 532.645 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb3f0efe640>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we save the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate our algorithm for 5 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 124.000, steps: 124\n",
      "Episode 2: reward: 51.000, steps: 51\n",
      "Episode 3: reward: 57.000, steps: 57\n",
      "Episode 4: reward: 53.000, steps: 53\n",
      "Episode 5: reward: 60.000, steps: 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb3f1099190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Deep network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "    24/100000: episode: 1, duration: 0.170s, episode steps: 24, steps per second: 141, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.062 [-1.615, 1.017], mean_best_reward: --\n",
      "    36/100000: episode: 2, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.118 [-2.122, 1.199], mean_best_reward: --\n",
      "    48/100000: episode: 3, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.103 [-2.966, 1.913], mean_best_reward: --\n",
      "    66/100000: episode: 4, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.160, 2.019], mean_best_reward: --\n",
      "    79/100000: episode: 5, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.121 [-1.735, 2.770], mean_best_reward: --\n",
      "    90/100000: episode: 6, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.111 [-1.419, 2.340], mean_best_reward: --\n",
      "   113/100000: episode: 7, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.041 [-1.143, 1.749], mean_best_reward: --\n",
      "   127/100000: episode: 8, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.084 [-0.842, 1.461], mean_best_reward: --\n",
      "   138/100000: episode: 9, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.098 [-1.548, 2.383], mean_best_reward: --\n",
      "   153/100000: episode: 10, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.064 [-1.665, 1.004], mean_best_reward: --\n",
      "   167/100000: episode: 11, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.116 [-2.727, 1.744], mean_best_reward: --\n",
      "   181/100000: episode: 12, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.085 [-1.006, 1.559], mean_best_reward: --\n",
      "   191/100000: episode: 13, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.942, 3.008], mean_best_reward: --\n",
      "   208/100000: episode: 14, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.107 [-1.359, 2.390], mean_best_reward: --\n",
      "   222/100000: episode: 15, duration: 0.104s, episode steps: 14, steps per second: 135, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.095 [-1.596, 0.801], mean_best_reward: --\n",
      "   232/100000: episode: 16, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.108 [-1.806, 2.694], mean_best_reward: --\n",
      "   247/100000: episode: 17, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.079 [-2.294, 1.396], mean_best_reward: --\n",
      "   273/100000: episode: 18, duration: 0.121s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.038 [-1.545, 2.417], mean_best_reward: --\n",
      "   286/100000: episode: 19, duration: 0.064s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.106 [-0.949, 1.754], mean_best_reward: --\n",
      "   300/100000: episode: 20, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.084 [-1.978, 1.172], mean_best_reward: --\n",
      "   315/100000: episode: 21, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.106 [-0.977, 1.713], mean_best_reward: --\n",
      "   325/100000: episode: 22, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.520, 2.505], mean_best_reward: --\n",
      "   335/100000: episode: 23, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.151, 1.988], mean_best_reward: --\n",
      "   360/100000: episode: 24, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.320 [0.000, 1.000], mean observation: -0.017 [-1.790, 2.469], mean_best_reward: --\n",
      "   380/100000: episode: 25, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.073 [-1.611, 0.814], mean_best_reward: --\n",
      "   393/100000: episode: 26, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.794, 1.724], mean_best_reward: --\n",
      "   419/100000: episode: 27, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: 0.020 [-2.732, 2.136], mean_best_reward: --\n",
      "   433/100000: episode: 28, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.117 [-0.627, 1.113], mean_best_reward: --\n",
      "   443/100000: episode: 29, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-2.000, 3.112], mean_best_reward: --\n",
      "   459/100000: episode: 30, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.101 [-0.966, 1.865], mean_best_reward: --\n",
      "   473/100000: episode: 31, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.091 [-0.607, 1.222], mean_best_reward: --\n",
      "   486/100000: episode: 32, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.114 [-1.748, 2.781], mean_best_reward: --\n",
      "   506/100000: episode: 33, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.043 [-0.811, 1.296], mean_best_reward: --\n",
      "   528/100000: episode: 34, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.106 [-0.909, 0.541], mean_best_reward: --\n",
      "   538/100000: episode: 35, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.152 [-1.553, 2.622], mean_best_reward: --\n",
      "   552/100000: episode: 36, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.083 [-1.593, 2.484], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   571/100000: episode: 37, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.110 [-1.344, 2.415], mean_best_reward: --\n",
      "   585/100000: episode: 38, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.073 [-1.861, 1.221], mean_best_reward: --\n",
      "   598/100000: episode: 39, duration: 0.062s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.115 [-1.340, 2.237], mean_best_reward: --\n",
      "   608/100000: episode: 40, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.136 [-1.181, 2.058], mean_best_reward: --\n",
      "   621/100000: episode: 41, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.105 [-1.791, 0.960], mean_best_reward: --\n",
      "   631/100000: episode: 42, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.138 [-1.996, 1.197], mean_best_reward: --\n",
      "   644/100000: episode: 43, duration: 0.092s, episode steps: 13, steps per second: 142, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.092 [-0.769, 1.387], mean_best_reward: --\n",
      "   663/100000: episode: 44, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.073 [-1.000, 1.802], mean_best_reward: --\n",
      "   674/100000: episode: 45, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.114 [-1.725, 2.733], mean_best_reward: --\n",
      "   686/100000: episode: 46, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.128 [-2.170, 1.333], mean_best_reward: --\n",
      "   697/100000: episode: 47, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.138 [-2.318, 1.367], mean_best_reward: --\n",
      "   708/100000: episode: 48, duration: 0.076s, episode steps: 11, steps per second: 146, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.119 [-2.762, 1.752], mean_best_reward: --\n",
      "   726/100000: episode: 49, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.073 [-1.542, 0.810], mean_best_reward: --\n",
      "   756/100000: episode: 50, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.051 [-1.346, 0.642], mean_best_reward: --\n",
      "   804/100000: episode: 51, duration: 0.289s, episode steps: 48, steps per second: 166, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.119 [-2.438, 2.532], mean_best_reward: --\n",
      "   816/100000: episode: 52, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.108 [-1.945, 2.951], mean_best_reward: --\n",
      "   840/100000: episode: 53, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-1.158, 0.649], mean_best_reward: --\n",
      "   854/100000: episode: 54, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.102 [-1.459, 0.791], mean_best_reward: --\n",
      "   873/100000: episode: 55, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.083 [-0.780, 1.192], mean_best_reward: --\n",
      "   883/100000: episode: 56, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.623, 1.573], mean_best_reward: --\n",
      "   894/100000: episode: 57, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-1.915, 1.162], mean_best_reward: --\n",
      "   907/100000: episode: 58, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.092 [-1.572, 2.447], mean_best_reward: --\n",
      "   959/100000: episode: 59, duration: 0.315s, episode steps: 52, steps per second: 165, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.143 [-0.464, 1.086], mean_best_reward: --\n",
      "   971/100000: episode: 60, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.098 [-1.699, 1.184], mean_best_reward: --\n",
      "  1008/100000: episode: 61, duration: 0.262s, episode steps: 37, steps per second: 141, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.142 [-0.762, 1.902], mean_best_reward: --\n",
      "  1017/100000: episode: 62, duration: 0.066s, episode steps: 9, steps per second: 137, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.137 [-1.419, 2.360], mean_best_reward: --\n",
      "  1026/100000: episode: 63, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.810, 1.730], mean_best_reward: --\n",
      "  1040/100000: episode: 64, duration: 0.107s, episode steps: 14, steps per second: 131, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.087 [-1.609, 2.566], mean_best_reward: --\n",
      "  1050/100000: episode: 65, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.121 [-1.945, 1.144], mean_best_reward: --\n",
      "  1061/100000: episode: 66, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.137 [-1.008, 1.823], mean_best_reward: --\n",
      "  1072/100000: episode: 67, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.112 [-1.011, 1.782], mean_best_reward: --\n",
      "  1091/100000: episode: 68, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.072 [-1.155, 0.625], mean_best_reward: --\n",
      "  1102/100000: episode: 69, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.099 [-1.620, 0.989], mean_best_reward: --\n",
      "  1111/100000: episode: 70, duration: 0.075s, episode steps: 9, steps per second: 120, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.829, 1.770], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1126/100000: episode: 71, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.101 [-1.805, 2.847], mean_best_reward: --\n",
      "  1135/100000: episode: 72, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.287, 1.337], mean_best_reward: --\n",
      "  1144/100000: episode: 73, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.348, 2.229], mean_best_reward: --\n",
      "  1158/100000: episode: 74, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.077 [-2.972, 1.988], mean_best_reward: --\n",
      "  1171/100000: episode: 75, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.371, 2.293], mean_best_reward: --\n",
      "  1180/100000: episode: 76, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.152 [-1.972, 1.139], mean_best_reward: --\n",
      "  1199/100000: episode: 77, duration: 0.139s, episode steps: 19, steps per second: 137, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.077 [-1.424, 2.418], mean_best_reward: --\n",
      "  1214/100000: episode: 78, duration: 0.111s, episode steps: 15, steps per second: 136, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.084 [-1.749, 2.783], mean_best_reward: --\n",
      "  1225/100000: episode: 79, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.117 [-1.811, 2.795], mean_best_reward: --\n",
      "  1243/100000: episode: 80, duration: 0.132s, episode steps: 18, steps per second: 137, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.101 [-1.122, 0.582], mean_best_reward: --\n",
      "  1259/100000: episode: 81, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.099 [-0.820, 1.669], mean_best_reward: --\n",
      "  1283/100000: episode: 82, duration: 0.156s, episode steps: 24, steps per second: 154, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.076 [-1.299, 0.454], mean_best_reward: --\n",
      "  1292/100000: episode: 83, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-2.148, 1.372], mean_best_reward: --\n",
      "  1306/100000: episode: 84, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.090 [-2.127, 1.331], mean_best_reward: --\n",
      "  1322/100000: episode: 85, duration: 0.118s, episode steps: 16, steps per second: 136, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.068 [-1.030, 1.657], mean_best_reward: --\n",
      "  1333/100000: episode: 86, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.790, 1.778], mean_best_reward: --\n",
      "  1348/100000: episode: 87, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.114 [-1.352, 2.305], mean_best_reward: --\n",
      "  1360/100000: episode: 88, duration: 0.094s, episode steps: 12, steps per second: 128, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-2.102, 1.363], mean_best_reward: --\n",
      "  1370/100000: episode: 89, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.135 [-1.162, 2.029], mean_best_reward: --\n",
      "  1380/100000: episode: 90, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-3.120, 1.915], mean_best_reward: --\n",
      "  1424/100000: episode: 91, duration: 0.258s, episode steps: 44, steps per second: 170, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.048 [-0.593, 0.870], mean_best_reward: --\n",
      "  1436/100000: episode: 92, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.133 [-0.942, 1.726], mean_best_reward: --\n",
      "  1448/100000: episode: 93, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.072 [-2.081, 1.402], mean_best_reward: --\n",
      "  1461/100000: episode: 94, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.118 [-0.751, 1.439], mean_best_reward: --\n",
      "  1500/100000: episode: 95, duration: 0.245s, episode steps: 39, steps per second: 159, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.132 [-0.863, 0.447], mean_best_reward: --\n",
      "  1511/100000: episode: 96, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.137 [-1.823, 0.942], mean_best_reward: --\n",
      "  1525/100000: episode: 97, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.086 [-1.226, 2.016], mean_best_reward: --\n",
      "  1536/100000: episode: 98, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.108 [-1.770, 2.746], mean_best_reward: --\n",
      "  1548/100000: episode: 99, duration: 0.091s, episode steps: 12, steps per second: 131, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.941, 1.657], mean_best_reward: --\n",
      "  1560/100000: episode: 100, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.797, 1.236], mean_best_reward: --\n",
      "  1605/100000: episode: 101, duration: 0.255s, episode steps: 45, steps per second: 176, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.053 [-1.225, 0.441], mean_best_reward: --\n",
      "  1616/100000: episode: 102, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.795, 1.737], mean_best_reward: --\n",
      "  1636/100000: episode: 103, duration: 0.150s, episode steps: 20, steps per second: 134, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.045 [-2.458, 1.595], mean_best_reward: --\n",
      "  1653/100000: episode: 104, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.176 [0.000, 1.000], mean observation: 0.085 [-2.101, 3.219], mean_best_reward: --\n",
      "  1662/100000: episode: 105, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.353, 1.384], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1682/100000: episode: 106, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.069 [-1.567, 2.604], mean_best_reward: --\n",
      "  1696/100000: episode: 107, duration: 0.104s, episode steps: 14, steps per second: 134, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-2.152, 1.353], mean_best_reward: --\n",
      "  1711/100000: episode: 108, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-1.011, 1.756], mean_best_reward: --\n",
      "  1737/100000: episode: 109, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.117 [-0.353, 0.848], mean_best_reward: --\n",
      "  1753/100000: episode: 110, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.095 [-1.869, 1.133], mean_best_reward: --\n",
      "  1769/100000: episode: 111, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.100 [-2.296, 1.337], mean_best_reward: --\n",
      "  1782/100000: episode: 112, duration: 0.096s, episode steps: 13, steps per second: 135, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.084 [-2.239, 1.401], mean_best_reward: --\n",
      "  1800/100000: episode: 113, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.091 [-0.959, 1.736], mean_best_reward: --\n",
      "  1814/100000: episode: 114, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.102 [-2.592, 1.571], mean_best_reward: --\n",
      "  1825/100000: episode: 115, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.127 [-2.152, 1.356], mean_best_reward: --\n",
      "  1838/100000: episode: 116, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.111 [-0.994, 1.828], mean_best_reward: --\n",
      "  1858/100000: episode: 117, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.130, 0.599], mean_best_reward: --\n",
      "  1871/100000: episode: 118, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.137 [-2.292, 1.340], mean_best_reward: --\n",
      "  1886/100000: episode: 119, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.133 [0.000, 1.000], mean observation: 0.082 [-2.157, 3.212], mean_best_reward: --\n",
      "  1901/100000: episode: 120, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.965, 1.771], mean_best_reward: --\n",
      "  1913/100000: episode: 121, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.107 [-1.216, 2.082], mean_best_reward: --\n",
      "  1925/100000: episode: 122, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-1.997, 1.162], mean_best_reward: --\n",
      "  1945/100000: episode: 123, duration: 0.155s, episode steps: 20, steps per second: 129, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.086 [-0.589, 1.222], mean_best_reward: --\n",
      "  1957/100000: episode: 124, duration: 0.110s, episode steps: 12, steps per second: 109, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.135 [-1.708, 2.703], mean_best_reward: --\n",
      "  1979/100000: episode: 125, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.038 [-1.915, 2.877], mean_best_reward: --\n",
      "  1996/100000: episode: 126, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.056 [-1.356, 2.136], mean_best_reward: --\n",
      "  2007/100000: episode: 127, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-0.799, 1.345], mean_best_reward: --\n",
      "  2017/100000: episode: 128, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.339, 2.154], mean_best_reward: --\n",
      "  2033/100000: episode: 129, duration: 0.102s, episode steps: 16, steps per second: 157, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.079 [-1.302, 0.810], mean_best_reward: --\n",
      "  2051/100000: episode: 130, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.066 [-3.038, 1.971], mean_best_reward: --\n",
      "  2063/100000: episode: 131, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.605, 2.578], mean_best_reward: --\n",
      "  2077/100000: episode: 132, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.077 [-1.587, 1.019], mean_best_reward: --\n",
      "  2087/100000: episode: 133, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.116 [-2.017, 1.176], mean_best_reward: --\n",
      "  2098/100000: episode: 134, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.361, 2.342], mean_best_reward: --\n",
      "  2111/100000: episode: 135, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.114 [-1.454, 0.789], mean_best_reward: --\n",
      "  2124/100000: episode: 136, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.104 [-2.808, 1.782], mean_best_reward: --\n",
      "  2152/100000: episode: 137, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.065 [-0.821, 1.703], mean_best_reward: --\n",
      "  2168/100000: episode: 138, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.092 [-1.317, 0.804], mean_best_reward: --\n",
      "  2178/100000: episode: 139, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.031, 1.198], mean_best_reward: --\n",
      "  2187/100000: episode: 140, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.777, 1.752], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2210/100000: episode: 141, duration: 0.107s, episode steps: 23, steps per second: 214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.112 [-1.610, 0.623], mean_best_reward: --\n",
      "  2218/100000: episode: 142, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-2.510, 1.603], mean_best_reward: --\n",
      "  2228/100000: episode: 143, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.435, 1.517], mean_best_reward: --\n",
      "  2259/100000: episode: 144, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.111 [-0.412, 1.010], mean_best_reward: --\n",
      "  2269/100000: episode: 145, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.588, 2.545], mean_best_reward: --\n",
      "  2280/100000: episode: 146, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.096 [-1.023, 1.755], mean_best_reward: --\n",
      "  2294/100000: episode: 147, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.091 [-1.944, 2.964], mean_best_reward: --\n",
      "  2303/100000: episode: 148, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [-1.746, 2.898], mean_best_reward: --\n",
      "  2315/100000: episode: 149, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.200, 2.010], mean_best_reward: --\n",
      "  2327/100000: episode: 150, duration: 0.057s, episode steps: 12, steps per second: 210, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.123 [-2.066, 1.138], mean_best_reward: --\n",
      "  2335/100000: episode: 151, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.553, 1.618], mean_best_reward: 48.500000\n",
      "  2352/100000: episode: 152, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.093 [-1.728, 0.945], mean_best_reward: --\n",
      "  2361/100000: episode: 153, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-2.866, 1.736], mean_best_reward: --\n",
      "  2392/100000: episode: 154, duration: 0.155s, episode steps: 31, steps per second: 199, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.077 [-1.157, 0.771], mean_best_reward: --\n",
      "  2429/100000: episode: 155, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.060 [-0.788, 1.569], mean_best_reward: --\n",
      "  2438/100000: episode: 156, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-1.887, 1.156], mean_best_reward: --\n",
      "  2515/100000: episode: 157, duration: 0.414s, episode steps: 77, steps per second: 186, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.085 [-1.204, 0.824], mean_best_reward: --\n",
      "  2546/100000: episode: 158, duration: 0.142s, episode steps: 31, steps per second: 219, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.028 [-1.379, 2.146], mean_best_reward: --\n",
      "  2559/100000: episode: 159, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.098 [-1.349, 2.142], mean_best_reward: --\n",
      "  2587/100000: episode: 160, duration: 0.169s, episode steps: 28, steps per second: 166, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.057 [-1.754, 0.984], mean_best_reward: --\n",
      "  2609/100000: episode: 161, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.117 [-0.797, 1.185], mean_best_reward: --\n",
      "  2622/100000: episode: 162, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.084 [-0.647, 1.176], mean_best_reward: --\n",
      "  2636/100000: episode: 163, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.092 [-2.034, 1.355], mean_best_reward: --\n",
      "  2648/100000: episode: 164, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-0.954, 1.792], mean_best_reward: --\n",
      "  2666/100000: episode: 165, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.054 [-1.992, 3.070], mean_best_reward: --\n",
      "  2675/100000: episode: 166, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.418, 2.241], mean_best_reward: --\n",
      "  2686/100000: episode: 167, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-1.217, 1.878], mean_best_reward: --\n",
      "  2695/100000: episode: 168, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.159 [-2.313, 1.348], mean_best_reward: --\n",
      "  2710/100000: episode: 169, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.078 [-1.386, 2.222], mean_best_reward: --\n",
      "  2722/100000: episode: 170, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.094 [-2.002, 3.025], mean_best_reward: --\n",
      "  2740/100000: episode: 171, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.106 [-1.793, 0.940], mean_best_reward: --\n",
      "  2753/100000: episode: 172, duration: 0.097s, episode steps: 13, steps per second: 134, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.082 [-1.396, 2.200], mean_best_reward: --\n",
      "  2787/100000: episode: 173, duration: 0.206s, episode steps: 34, steps per second: 165, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: -0.027 [-1.742, 2.136], mean_best_reward: --\n",
      "  2800/100000: episode: 174, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.087 [-1.536, 2.330], mean_best_reward: --\n",
      "  2820/100000: episode: 175, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.094 [-0.836, 0.419], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2837/100000: episode: 176, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.096 [-1.473, 0.757], mean_best_reward: --\n",
      "  2851/100000: episode: 177, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.092 [-1.581, 2.529], mean_best_reward: --\n",
      "  2865/100000: episode: 178, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.087 [-2.080, 1.226], mean_best_reward: --\n",
      "  2887/100000: episode: 179, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.074 [-1.637, 0.976], mean_best_reward: --\n",
      "  2895/100000: episode: 180, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.508, 1.602], mean_best_reward: --\n",
      "  2910/100000: episode: 181, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.079 [-1.335, 2.147], mean_best_reward: --\n",
      "  2928/100000: episode: 182, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.059 [-2.523, 1.610], mean_best_reward: --\n",
      "  2959/100000: episode: 183, duration: 0.186s, episode steps: 31, steps per second: 166, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.677 [0.000, 1.000], mean observation: 0.003 [-3.006, 2.092], mean_best_reward: --\n",
      "  2978/100000: episode: 184, duration: 0.118s, episode steps: 19, steps per second: 162, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.057 [-1.029, 1.837], mean_best_reward: --\n",
      "  2998/100000: episode: 185, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.054 [-1.200, 1.718], mean_best_reward: --\n",
      "  3016/100000: episode: 186, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.093 [-0.617, 1.405], mean_best_reward: --\n",
      "  3030/100000: episode: 187, duration: 0.104s, episode steps: 14, steps per second: 135, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.089 [-2.017, 1.184], mean_best_reward: --\n",
      "  3043/100000: episode: 188, duration: 0.087s, episode steps: 13, steps per second: 150, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.098 [-1.766, 2.601], mean_best_reward: --\n",
      "  3057/100000: episode: 189, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.082 [-1.543, 2.514], mean_best_reward: --\n",
      "  3068/100000: episode: 190, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.125 [-1.932, 2.935], mean_best_reward: --\n",
      "  3077/100000: episode: 191, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.717, 2.790], mean_best_reward: --\n",
      "  3090/100000: episode: 192, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.113 [-1.820, 1.163], mean_best_reward: --\n",
      "  3103/100000: episode: 193, duration: 0.062s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.960, 1.166], mean_best_reward: --\n",
      "  3121/100000: episode: 194, duration: 0.083s, episode steps: 18, steps per second: 217, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.082 [-0.430, 1.156], mean_best_reward: --\n",
      "  3136/100000: episode: 195, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.091 [-1.544, 1.021], mean_best_reward: --\n",
      "  3158/100000: episode: 196, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.049 [-0.978, 1.649], mean_best_reward: --\n",
      "  3173/100000: episode: 197, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.083 [-1.611, 2.532], mean_best_reward: --\n",
      "  3184/100000: episode: 198, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.129 [-2.285, 1.337], mean_best_reward: --\n",
      "  3201/100000: episode: 199, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.092 [-1.789, 0.957], mean_best_reward: --\n",
      "  3226/100000: episode: 200, duration: 0.113s, episode steps: 25, steps per second: 222, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.993, 1.906], mean_best_reward: --\n",
      "  3238/100000: episode: 201, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.621, 1.565], mean_best_reward: 44.500000\n",
      "  3251/100000: episode: 202, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.080 [-1.200, 1.801], mean_best_reward: --\n",
      "  3267/100000: episode: 203, duration: 0.074s, episode steps: 16, steps per second: 216, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.088 [-0.806, 1.596], mean_best_reward: --\n",
      "  3304/100000: episode: 204, duration: 0.168s, episode steps: 37, steps per second: 221, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.108 [-0.739, 1.221], mean_best_reward: --\n",
      "  3343/100000: episode: 205, duration: 0.174s, episode steps: 39, steps per second: 225, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.054 [-0.646, 1.332], mean_best_reward: --\n",
      "  3366/100000: episode: 206, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.061 [-2.401, 1.395], mean_best_reward: --\n",
      "  3379/100000: episode: 207, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.105 [-1.404, 2.299], mean_best_reward: --\n",
      "  3390/100000: episode: 208, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.103 [-1.398, 2.274], mean_best_reward: --\n",
      "  3420/100000: episode: 209, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.064 [-0.843, 1.728], mean_best_reward: --\n",
      "  3442/100000: episode: 210, duration: 0.099s, episode steps: 22, steps per second: 222, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.067 [-1.150, 1.902], mean_best_reward: --\n",
      "  3461/100000: episode: 211, duration: 0.088s, episode steps: 19, steps per second: 216, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.056 [-1.742, 2.661], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3471/100000: episode: 212, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.993, 1.997], mean_best_reward: --\n",
      "  3486/100000: episode: 213, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.057 [-1.918, 1.220], mean_best_reward: --\n",
      "  3508/100000: episode: 214, duration: 0.102s, episode steps: 22, steps per second: 216, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.101 [-0.617, 0.874], mean_best_reward: --\n",
      "  3517/100000: episode: 215, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.131 [-1.601, 2.458], mean_best_reward: --\n",
      "  3537/100000: episode: 216, duration: 0.094s, episode steps: 20, steps per second: 212, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.066 [-2.067, 1.213], mean_best_reward: --\n",
      "  3549/100000: episode: 217, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.576, 2.367], mean_best_reward: --\n",
      "  3617/100000: episode: 218, duration: 0.305s, episode steps: 68, steps per second: 223, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.132 [-0.505, 1.176], mean_best_reward: --\n",
      "  3635/100000: episode: 219, duration: 0.084s, episode steps: 18, steps per second: 215, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.108 [-0.615, 1.151], mean_best_reward: --\n",
      "  3657/100000: episode: 220, duration: 0.100s, episode steps: 22, steps per second: 219, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.072 [-1.404, 2.292], mean_best_reward: --\n",
      "  3668/100000: episode: 221, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.397, 2.264], mean_best_reward: --\n",
      "  3688/100000: episode: 222, duration: 0.091s, episode steps: 20, steps per second: 219, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.074 [-2.095, 1.213], mean_best_reward: --\n",
      "  3700/100000: episode: 223, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.116 [-2.200, 1.369], mean_best_reward: --\n",
      "  3715/100000: episode: 224, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.076 [-1.438, 0.992], mean_best_reward: --\n",
      "  3727/100000: episode: 225, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.102 [-1.923, 3.007], mean_best_reward: --\n",
      "  3745/100000: episode: 226, duration: 0.083s, episode steps: 18, steps per second: 216, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.075 [-1.139, 1.991], mean_best_reward: --\n",
      "  3755/100000: episode: 227, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.521, 2.551], mean_best_reward: --\n",
      "  3778/100000: episode: 228, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.064 [-1.529, 0.784], mean_best_reward: --\n",
      "  3793/100000: episode: 229, duration: 0.070s, episode steps: 15, steps per second: 214, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.106 [-1.393, 0.586], mean_best_reward: --\n",
      "  3805/100000: episode: 230, duration: 0.057s, episode steps: 12, steps per second: 209, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.128 [-2.069, 1.162], mean_best_reward: --\n",
      "  3817/100000: episode: 231, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.119 [-2.147, 1.330], mean_best_reward: --\n",
      "  3831/100000: episode: 232, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.097 [-2.417, 1.549], mean_best_reward: --\n",
      "  3847/100000: episode: 233, duration: 0.075s, episode steps: 16, steps per second: 214, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.085 [-2.605, 1.581], mean_best_reward: --\n",
      "  3856/100000: episode: 234, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.119 [-1.917, 1.196], mean_best_reward: --\n",
      "  3865/100000: episode: 235, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-2.741, 1.803], mean_best_reward: --\n",
      "  3878/100000: episode: 236, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.102 [-1.329, 2.225], mean_best_reward: --\n",
      "  3901/100000: episode: 237, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.028 [-1.643, 1.213], mean_best_reward: --\n",
      "  3946/100000: episode: 238, duration: 0.207s, episode steps: 45, steps per second: 217, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.378 [0.000, 1.000], mean observation: -0.044 [-2.152, 2.789], mean_best_reward: --\n",
      "  4005/100000: episode: 239, duration: 0.265s, episode steps: 59, steps per second: 223, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.127 [-1.839, 0.734], mean_best_reward: --\n",
      "  4018/100000: episode: 240, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.121 [-1.266, 0.764], mean_best_reward: --\n",
      "  4027/100000: episode: 241, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.773, 2.744], mean_best_reward: --\n",
      "  4052/100000: episode: 242, duration: 0.115s, episode steps: 25, steps per second: 218, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.075 [-0.874, 0.411], mean_best_reward: --\n",
      "  4101/100000: episode: 243, duration: 0.216s, episode steps: 49, steps per second: 227, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.101 [-0.633, 0.956], mean_best_reward: --\n",
      "  4115/100000: episode: 244, duration: 0.066s, episode steps: 14, steps per second: 211, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-0.793, 1.573], mean_best_reward: --\n",
      "  4144/100000: episode: 245, duration: 0.132s, episode steps: 29, steps per second: 220, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.086 [-0.579, 1.479], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4155/100000: episode: 246, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.117 [-1.388, 2.264], mean_best_reward: --\n",
      "  4166/100000: episode: 247, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.379, 2.226], mean_best_reward: --\n",
      "  4177/100000: episode: 248, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.111 [-1.784, 2.697], mean_best_reward: --\n",
      "  4243/100000: episode: 249, duration: 0.302s, episode steps: 66, steps per second: 219, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.212, 0.696], mean_best_reward: --\n",
      "  4259/100000: episode: 250, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-1.000, 1.543], mean_best_reward: --\n",
      "  4272/100000: episode: 251, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.091 [-2.391, 1.518], mean_best_reward: 52.000000\n",
      "  4286/100000: episode: 252, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.085 [-1.545, 2.494], mean_best_reward: --\n",
      "  4299/100000: episode: 253, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.965, 1.517], mean_best_reward: --\n",
      "  4354/100000: episode: 254, duration: 0.250s, episode steps: 55, steps per second: 220, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.126 [-0.843, 1.150], mean_best_reward: --\n",
      "  4367/100000: episode: 255, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.116 [-0.965, 1.804], mean_best_reward: --\n",
      "  4379/100000: episode: 256, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.138 [-1.523, 2.628], mean_best_reward: --\n",
      "  4390/100000: episode: 257, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.129 [-1.695, 1.000], mean_best_reward: --\n",
      "  4416/100000: episode: 258, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.797, 1.231], mean_best_reward: --\n",
      "  4428/100000: episode: 259, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.115 [-2.536, 1.521], mean_best_reward: --\n",
      "  4438/100000: episode: 260, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.581, 1.564], mean_best_reward: --\n",
      "  4452/100000: episode: 261, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.100 [-2.290, 1.394], mean_best_reward: --\n",
      "  4467/100000: episode: 262, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.098 [-1.516, 2.453], mean_best_reward: --\n",
      "  4476/100000: episode: 263, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-1.817, 1.152], mean_best_reward: --\n",
      "  4490/100000: episode: 264, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.087 [-2.136, 1.362], mean_best_reward: --\n",
      "  4506/100000: episode: 265, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.098 [-1.706, 0.962], mean_best_reward: --\n",
      "  4517/100000: episode: 266, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.116 [-1.768, 2.712], mean_best_reward: --\n",
      "  4528/100000: episode: 267, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.107 [-2.462, 1.578], mean_best_reward: --\n",
      "  4541/100000: episode: 268, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.093 [-1.807, 1.161], mean_best_reward: --\n",
      "  4559/100000: episode: 269, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.054 [-2.458, 1.540], mean_best_reward: --\n",
      "  4600/100000: episode: 270, duration: 0.188s, episode steps: 41, steps per second: 218, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.059 [-1.362, 0.926], mean_best_reward: --\n",
      "  4618/100000: episode: 271, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.101 [-1.638, 0.775], mean_best_reward: --\n",
      "  4636/100000: episode: 272, duration: 0.085s, episode steps: 18, steps per second: 213, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.055 [-1.827, 1.151], mean_best_reward: --\n",
      "  4647/100000: episode: 273, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.137 [-1.710, 2.773], mean_best_reward: --\n",
      "  4662/100000: episode: 274, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-0.756, 1.412], mean_best_reward: --\n",
      "  4677/100000: episode: 275, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.079 [-2.764, 1.800], mean_best_reward: --\n",
      "  4690/100000: episode: 276, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.088 [-2.238, 1.411], mean_best_reward: --\n",
      "  4713/100000: episode: 277, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.044 [-2.197, 1.383], mean_best_reward: --\n",
      "  4730/100000: episode: 278, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.110 [-2.361, 1.348], mean_best_reward: --\n",
      "  4743/100000: episode: 279, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.781, 1.012], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4755/100000: episode: 280, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.115 [-1.552, 2.567], mean_best_reward: --\n",
      "  4815/100000: episode: 281, duration: 0.276s, episode steps: 60, steps per second: 217, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.068 [-2.742, 2.299], mean_best_reward: --\n",
      "  4832/100000: episode: 282, duration: 0.078s, episode steps: 17, steps per second: 218, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.072 [-1.407, 2.242], mean_best_reward: --\n",
      "  4847/100000: episode: 283, duration: 0.071s, episode steps: 15, steps per second: 212, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.087 [-1.010, 1.513], mean_best_reward: --\n",
      "  4869/100000: episode: 284, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.082 [-2.576, 1.532], mean_best_reward: --\n",
      "  4970/100000: episode: 285, duration: 0.442s, episode steps: 101, steps per second: 229, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.009 [-1.455, 1.261], mean_best_reward: --\n",
      "  4983/100000: episode: 286, duration: 0.062s, episode steps: 13, steps per second: 210, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.108 [-2.227, 1.384], mean_best_reward: --\n",
      "  4994/100000: episode: 287, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-1.868, 1.174], mean_best_reward: --\n",
      "  5010/100000: episode: 288, duration: 0.075s, episode steps: 16, steps per second: 214, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.091 [-0.611, 1.264], mean_best_reward: --\n",
      "  5034/100000: episode: 289, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.708 [0.000, 1.000], mean observation: -0.021 [-2.857, 2.088], mean_best_reward: --\n",
      "  5049/100000: episode: 290, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.505, 0.968], mean_best_reward: --\n",
      "  5063/100000: episode: 291, duration: 0.066s, episode steps: 14, steps per second: 211, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.102 [-1.573, 2.602], mean_best_reward: --\n",
      "  5081/100000: episode: 292, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-2.166, 1.164], mean_best_reward: --\n",
      "  5092/100000: episode: 293, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.093 [-1.586, 2.437], mean_best_reward: --\n",
      "  5115/100000: episode: 294, duration: 0.105s, episode steps: 23, steps per second: 219, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.042 [-1.403, 0.833], mean_best_reward: --\n",
      "  5125/100000: episode: 295, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.409, 2.218], mean_best_reward: --\n",
      "  5139/100000: episode: 296, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.205, 1.387], mean_best_reward: --\n",
      "  5149/100000: episode: 297, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.577, 2.482], mean_best_reward: --\n",
      "  5159/100000: episode: 298, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.116 [-1.406, 2.223], mean_best_reward: --\n",
      "  5170/100000: episode: 299, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.336, 1.421], mean_best_reward: --\n",
      "  5179/100000: episode: 300, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.554, 2.479], mean_best_reward: --\n",
      "  5190/100000: episode: 301, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.421, 2.322], mean_best_reward: 45.000000\n",
      "  5206/100000: episode: 302, duration: 0.074s, episode steps: 16, steps per second: 215, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.104 [-1.175, 2.044], mean_best_reward: --\n",
      "  5223/100000: episode: 303, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.091 [-1.032, 1.915], mean_best_reward: --\n",
      "  5244/100000: episode: 304, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.044 [-1.017, 1.505], mean_best_reward: --\n",
      "  5254/100000: episode: 305, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.141 [-1.935, 1.162], mean_best_reward: --\n",
      "  5275/100000: episode: 306, duration: 0.098s, episode steps: 21, steps per second: 214, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.039 [-1.729, 2.636], mean_best_reward: --\n",
      "  5293/100000: episode: 307, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.113 [-0.571, 1.020], mean_best_reward: --\n",
      "  5311/100000: episode: 308, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.095 [-1.113, 0.607], mean_best_reward: --\n",
      "  5324/100000: episode: 309, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.106 [-0.967, 1.799], mean_best_reward: --\n",
      "  5336/100000: episode: 310, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.097 [-0.836, 1.568], mean_best_reward: --\n",
      "  5346/100000: episode: 311, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.568, 2.563], mean_best_reward: --\n",
      "  5359/100000: episode: 312, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.069 [-1.789, 2.739], mean_best_reward: --\n",
      "  5418/100000: episode: 313, duration: 0.262s, episode steps: 59, steps per second: 225, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.135 [-0.665, 0.983], mean_best_reward: --\n",
      "  5438/100000: episode: 314, duration: 0.092s, episode steps: 20, steps per second: 218, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.099 [-1.153, 2.149], mean_best_reward: --\n",
      "  5459/100000: episode: 315, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.073 [-0.972, 1.777], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5469/100000: episode: 316, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.557, 2.601], mean_best_reward: --\n",
      "  5478/100000: episode: 317, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.162 [-1.355, 2.315], mean_best_reward: --\n",
      "  5497/100000: episode: 318, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.052 [-0.995, 1.398], mean_best_reward: --\n",
      "  5513/100000: episode: 319, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.093 [-1.556, 2.611], mean_best_reward: --\n",
      "  5534/100000: episode: 320, duration: 0.096s, episode steps: 21, steps per second: 218, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.089 [-1.011, 1.861], mean_best_reward: --\n",
      "  5548/100000: episode: 321, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.097 [-1.563, 2.502], mean_best_reward: --\n",
      "  5560/100000: episode: 322, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.128 [-1.494, 0.752], mean_best_reward: --\n",
      "  5578/100000: episode: 323, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-2.070, 1.169], mean_best_reward: --\n",
      "  5591/100000: episode: 324, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.088 [-0.838, 1.373], mean_best_reward: --\n",
      "  5602/100000: episode: 325, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-1.334, 2.046], mean_best_reward: --\n",
      "  5612/100000: episode: 326, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.605, 2.554], mean_best_reward: --\n",
      "  5631/100000: episode: 327, duration: 0.088s, episode steps: 19, steps per second: 216, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.060 [-1.399, 2.257], mean_best_reward: --\n",
      "  5642/100000: episode: 328, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.111 [-1.412, 2.214], mean_best_reward: --\n",
      "  5650/100000: episode: 329, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.533, 1.539], mean_best_reward: --\n",
      "  5669/100000: episode: 330, duration: 0.094s, episode steps: 19, steps per second: 203, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.064 [-1.898, 1.041], mean_best_reward: --\n",
      "  5678/100000: episode: 331, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.787, 2.758], mean_best_reward: --\n",
      "  5704/100000: episode: 332, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.023 [-1.514, 0.995], mean_best_reward: --\n",
      "  5717/100000: episode: 333, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.089 [-1.477, 0.831], mean_best_reward: --\n",
      "  5734/100000: episode: 334, duration: 0.080s, episode steps: 17, steps per second: 214, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.065 [-2.296, 1.414], mean_best_reward: --\n",
      "  5748/100000: episode: 335, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.107 [-2.078, 1.162], mean_best_reward: --\n",
      "  5759/100000: episode: 336, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.736, 2.837], mean_best_reward: --\n",
      "  5777/100000: episode: 337, duration: 0.083s, episode steps: 18, steps per second: 216, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.112 [-2.624, 1.522], mean_best_reward: --\n",
      "  5787/100000: episode: 338, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.117 [-1.398, 2.137], mean_best_reward: --\n",
      "  5824/100000: episode: 339, duration: 0.168s, episode steps: 37, steps per second: 220, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: 0.037 [-2.162, 1.770], mean_best_reward: --\n",
      "  5836/100000: episode: 340, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.097 [-2.171, 1.404], mean_best_reward: --\n",
      "  5845/100000: episode: 341, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.833, 1.792], mean_best_reward: --\n",
      "  5854/100000: episode: 342, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.161 [-2.475, 1.516], mean_best_reward: --\n",
      "  5875/100000: episode: 343, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.036 [-2.100, 1.385], mean_best_reward: --\n",
      "  5889/100000: episode: 344, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.098 [-1.197, 1.898], mean_best_reward: --\n",
      "  5906/100000: episode: 345, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.077 [-1.720, 2.728], mean_best_reward: --\n",
      "  5919/100000: episode: 346, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.088 [-0.834, 1.497], mean_best_reward: --\n",
      "  5930/100000: episode: 347, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.154 [-2.801, 1.715], mean_best_reward: --\n",
      "  5949/100000: episode: 348, duration: 0.087s, episode steps: 19, steps per second: 218, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.083 [-0.793, 1.346], mean_best_reward: --\n",
      "  5991/100000: episode: 349, duration: 0.196s, episode steps: 42, steps per second: 215, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.124, 0.678], mean_best_reward: --\n",
      "  6001/100000: episode: 350, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.911, 3.050], mean_best_reward: --\n",
      "  6012/100000: episode: 351, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.131 [-1.416, 0.746], mean_best_reward: 55.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6076/100000: episode: 352, duration: 0.287s, episode steps: 64, steps per second: 223, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.187 [-0.752, 1.329], mean_best_reward: --\n",
      "  6085/100000: episode: 353, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.811, 2.850], mean_best_reward: --\n",
      "  6104/100000: episode: 354, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.106 [-0.561, 1.293], mean_best_reward: --\n",
      "  6115/100000: episode: 355, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.105 [-2.355, 1.422], mean_best_reward: --\n",
      "  6126/100000: episode: 356, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.775, 1.745], mean_best_reward: --\n",
      "  6154/100000: episode: 357, duration: 0.130s, episode steps: 28, steps per second: 216, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.038 [-1.016, 1.617], mean_best_reward: --\n",
      "  6166/100000: episode: 358, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.213, 1.991], mean_best_reward: --\n",
      "  6179/100000: episode: 359, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.087 [-1.386, 0.818], mean_best_reward: --\n",
      "  6193/100000: episode: 360, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.108 [-0.794, 1.312], mean_best_reward: --\n",
      "  6220/100000: episode: 361, duration: 0.126s, episode steps: 27, steps per second: 215, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.033 [-2.730, 1.797], mean_best_reward: --\n",
      "  6233/100000: episode: 362, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.105 [-1.425, 2.372], mean_best_reward: --\n",
      "  6243/100000: episode: 363, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.116 [-1.864, 1.201], mean_best_reward: --\n",
      "  6262/100000: episode: 364, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.074 [-1.846, 1.009], mean_best_reward: --\n",
      "  6276/100000: episode: 365, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.119 [-2.098, 1.196], mean_best_reward: --\n",
      "  6286/100000: episode: 366, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.111 [-1.389, 2.152], mean_best_reward: --\n",
      "  6308/100000: episode: 367, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.244, 0.830], mean_best_reward: --\n",
      "  6320/100000: episode: 368, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.141 [-1.326, 2.270], mean_best_reward: --\n",
      "  6354/100000: episode: 369, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.134 [-1.072, 0.540], mean_best_reward: --\n",
      "  6365/100000: episode: 370, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-1.378, 2.134], mean_best_reward: --\n",
      "  6378/100000: episode: 371, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.096 [-0.978, 1.757], mean_best_reward: --\n",
      "  6393/100000: episode: 372, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.086 [-2.782, 1.736], mean_best_reward: --\n",
      "  6406/100000: episode: 373, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.101 [-0.975, 1.671], mean_best_reward: --\n",
      "  6432/100000: episode: 374, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.322, 0.943], mean_best_reward: --\n",
      "  6447/100000: episode: 375, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.061 [-2.185, 1.417], mean_best_reward: --\n",
      "  6463/100000: episode: 376, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.101 [-1.147, 1.997], mean_best_reward: --\n",
      "  6484/100000: episode: 377, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.080 [-1.474, 0.783], mean_best_reward: --\n",
      "  6497/100000: episode: 378, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.116 [-2.836, 1.769], mean_best_reward: --\n",
      "  6507/100000: episode: 379, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.967, 3.069], mean_best_reward: --\n",
      "  6525/100000: episode: 380, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.075 [-2.577, 1.574], mean_best_reward: --\n",
      "  6536/100000: episode: 381, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.095 [-1.607, 2.426], mean_best_reward: --\n",
      "  6545/100000: episode: 382, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.130 [-1.636, 0.944], mean_best_reward: --\n",
      "  6554/100000: episode: 383, duration: 0.047s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.151 [-0.933, 1.642], mean_best_reward: --\n",
      "  6594/100000: episode: 384, duration: 0.183s, episode steps: 40, steps per second: 219, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.179 [-1.466, 0.885], mean_best_reward: --\n",
      "  6608/100000: episode: 385, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-2.150, 1.340], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6638/100000: episode: 386, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.032 [-0.985, 1.493], mean_best_reward: --\n",
      "  6660/100000: episode: 387, duration: 0.103s, episode steps: 22, steps per second: 215, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.071 [-1.213, 0.741], mean_best_reward: --\n",
      "  6674/100000: episode: 388, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.119 [-0.557, 1.300], mean_best_reward: --\n",
      "  6706/100000: episode: 389, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.061 [-1.320, 0.610], mean_best_reward: --\n",
      "  6802/100000: episode: 390, duration: 0.426s, episode steps: 96, steps per second: 226, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.323 [-1.554, 2.485], mean_best_reward: --\n",
      "  6814/100000: episode: 391, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.118 [-1.342, 0.794], mean_best_reward: --\n",
      "  6824/100000: episode: 392, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.934, 3.118], mean_best_reward: --\n",
      "  6857/100000: episode: 393, duration: 0.153s, episode steps: 33, steps per second: 216, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.081 [-0.613, 0.895], mean_best_reward: --\n",
      "  6867/100000: episode: 394, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-1.021, 1.670], mean_best_reward: --\n",
      "  6878/100000: episode: 395, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-1.817, 1.028], mean_best_reward: --\n",
      "  6890/100000: episode: 396, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.074, 1.223], mean_best_reward: --\n",
      "  6904/100000: episode: 397, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.080 [-0.996, 1.579], mean_best_reward: --\n",
      "  6919/100000: episode: 398, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.909, 1.164], mean_best_reward: --\n",
      "  6933/100000: episode: 399, duration: 0.066s, episode steps: 14, steps per second: 211, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.118 [-1.589, 0.748], mean_best_reward: --\n",
      "  6963/100000: episode: 400, duration: 0.139s, episode steps: 30, steps per second: 217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-1.030, 0.894], mean_best_reward: --\n",
      "  6983/100000: episode: 401, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.061 [-0.779, 1.395], mean_best_reward: 57.500000\n",
      "  6996/100000: episode: 402, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.079 [-2.271, 1.422], mean_best_reward: --\n",
      "  7088/100000: episode: 403, duration: 0.407s, episode steps: 92, steps per second: 226, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.987, 0.705], mean_best_reward: --\n",
      "  7108/100000: episode: 404, duration: 0.092s, episode steps: 20, steps per second: 216, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.094 [-2.171, 1.170], mean_best_reward: --\n",
      "  7128/100000: episode: 405, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.120, 0.543], mean_best_reward: --\n",
      "  7141/100000: episode: 406, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.123 [-0.752, 1.287], mean_best_reward: --\n",
      "  7177/100000: episode: 407, duration: 0.164s, episode steps: 36, steps per second: 220, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.056 [-1.282, 0.607], mean_best_reward: --\n",
      "  7202/100000: episode: 408, duration: 0.116s, episode steps: 25, steps per second: 216, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.055 [-2.725, 1.764], mean_best_reward: --\n",
      "  7218/100000: episode: 409, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.081 [-1.892, 1.203], mean_best_reward: --\n",
      "  7279/100000: episode: 410, duration: 0.282s, episode steps: 61, steps per second: 216, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.207 [-0.305, 1.094], mean_best_reward: --\n",
      "  7291/100000: episode: 411, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.107 [-2.423, 1.550], mean_best_reward: --\n",
      "  7341/100000: episode: 412, duration: 0.241s, episode steps: 50, steps per second: 207, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.174 [-1.294, 0.402], mean_best_reward: --\n",
      "  7361/100000: episode: 413, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.105 [-1.277, 0.576], mean_best_reward: --\n",
      "  7373/100000: episode: 414, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.809, 1.213], mean_best_reward: --\n",
      "  7408/100000: episode: 415, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.050 [-1.280, 0.585], mean_best_reward: --\n",
      "  7420/100000: episode: 416, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.127 [-2.731, 1.720], mean_best_reward: --\n",
      "  7446/100000: episode: 417, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.079 [-1.012, 1.468], mean_best_reward: --\n",
      "  7483/100000: episode: 418, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.087 [-0.812, 0.429], mean_best_reward: --\n",
      "  7510/100000: episode: 419, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.129 [-0.751, 0.370], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7525/100000: episode: 420, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.098 [-2.372, 1.374], mean_best_reward: --\n",
      "  7548/100000: episode: 421, duration: 0.106s, episode steps: 23, steps per second: 217, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.045 [-2.205, 1.384], mean_best_reward: --\n",
      "  7618/100000: episode: 422, duration: 0.355s, episode steps: 70, steps per second: 197, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.099 [-0.793, 1.128], mean_best_reward: --\n",
      "  7645/100000: episode: 423, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.063 [-0.993, 0.600], mean_best_reward: --\n",
      "  7774/100000: episode: 424, duration: 0.616s, episode steps: 129, steps per second: 209, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.028 [-1.692, 0.761], mean_best_reward: --\n",
      "  7802/100000: episode: 425, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.119 [-0.731, 1.541], mean_best_reward: --\n",
      "  7825/100000: episode: 426, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.104 [-0.556, 1.045], mean_best_reward: --\n",
      "  7876/100000: episode: 427, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.125 [-0.578, 0.950], mean_best_reward: --\n",
      "  7895/100000: episode: 428, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.069 [-0.992, 1.616], mean_best_reward: --\n",
      "  7905/100000: episode: 429, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.119 [-2.502, 1.602], mean_best_reward: --\n",
      "  7959/100000: episode: 430, duration: 0.262s, episode steps: 54, steps per second: 206, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.648 [0.000, 1.000], mean observation: 0.170 [-3.120, 3.007], mean_best_reward: --\n",
      "  7972/100000: episode: 431, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.092 [-2.427, 1.555], mean_best_reward: --\n",
      "  8040/100000: episode: 432, duration: 0.336s, episode steps: 68, steps per second: 203, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.091 [-1.029, 0.787], mean_best_reward: --\n",
      "  8049/100000: episode: 433, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.776, 2.837], mean_best_reward: --\n",
      "  8079/100000: episode: 434, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.108 [-0.596, 0.919], mean_best_reward: --\n",
      "  8107/100000: episode: 435, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: 0.011 [-2.187, 1.752], mean_best_reward: --\n",
      "  8141/100000: episode: 436, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.026 [-2.204, 1.369], mean_best_reward: --\n",
      "  8162/100000: episode: 437, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.102 [-1.057, 0.551], mean_best_reward: --\n",
      "  8227/100000: episode: 438, duration: 0.301s, episode steps: 65, steps per second: 216, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.041 [-0.897, 1.129], mean_best_reward: --\n",
      "  8240/100000: episode: 439, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.124 [-2.470, 1.531], mean_best_reward: --\n",
      "  8265/100000: episode: 440, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.079 [-0.636, 0.892], mean_best_reward: --\n",
      "  8285/100000: episode: 441, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.051 [-2.941, 1.988], mean_best_reward: --\n",
      "  8341/100000: episode: 442, duration: 0.275s, episode steps: 56, steps per second: 204, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.048 [-1.108, 0.564], mean_best_reward: --\n",
      "  8364/100000: episode: 443, duration: 0.111s, episode steps: 23, steps per second: 206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.083 [-1.129, 0.602], mean_best_reward: --\n",
      "  8396/100000: episode: 444, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.098, 0.810], mean_best_reward: --\n",
      "  8415/100000: episode: 445, duration: 0.088s, episode steps: 19, steps per second: 216, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.115 [-0.950, 0.384], mean_best_reward: --\n",
      "  8451/100000: episode: 446, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.586, 0.884], mean_best_reward: --\n",
      "  8477/100000: episode: 447, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.731 [0.000, 1.000], mean observation: -0.011 [-3.359, 2.336], mean_best_reward: --\n",
      "  8508/100000: episode: 448, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.129 [-1.081, 0.628], mean_best_reward: --\n",
      "  8539/100000: episode: 449, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.039 [-0.587, 1.145], mean_best_reward: --\n",
      "  8550/100000: episode: 450, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.182, 1.929], mean_best_reward: --\n",
      "  8562/100000: episode: 451, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.097 [-1.292, 0.802], mean_best_reward: 96.500000\n",
      "  8579/100000: episode: 452, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.078 [-1.419, 0.800], mean_best_reward: --\n",
      "  8593/100000: episode: 453, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.097 [-1.394, 2.275], mean_best_reward: --\n",
      "  8610/100000: episode: 454, duration: 0.079s, episode steps: 17, steps per second: 216, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.112 [-0.564, 1.004], mean_best_reward: --\n",
      "  8624/100000: episode: 455, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-1.033, 1.770], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8654/100000: episode: 456, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.128 [-0.390, 1.471], mean_best_reward: --\n",
      "  8664/100000: episode: 457, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.561, 2.520], mean_best_reward: --\n",
      "  8677/100000: episode: 458, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.140 [-2.037, 1.151], mean_best_reward: --\n",
      "  8695/100000: episode: 459, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.058 [-2.000, 1.198], mean_best_reward: --\n",
      "  8704/100000: episode: 460, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.167 [-2.356, 1.348], mean_best_reward: --\n",
      "  8843/100000: episode: 461, duration: 0.693s, episode steps: 139, steps per second: 200, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.009 [-0.657, 1.044], mean_best_reward: --\n",
      "  8856/100000: episode: 462, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.128 [-2.342, 1.345], mean_best_reward: --\n",
      "  8866/100000: episode: 463, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.587, 2.648], mean_best_reward: --\n",
      "  8901/100000: episode: 464, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.072 [-0.575, 1.045], mean_best_reward: --\n",
      "  8937/100000: episode: 465, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.133 [-0.759, 0.414], mean_best_reward: --\n",
      "  8948/100000: episode: 466, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.113 [-0.976, 1.772], mean_best_reward: --\n",
      "  8958/100000: episode: 467, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.946, 2.993], mean_best_reward: --\n",
      "  8973/100000: episode: 468, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.087 [-1.331, 2.177], mean_best_reward: --\n",
      "  9015/100000: episode: 469, duration: 0.199s, episode steps: 42, steps per second: 211, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.556, 0.899], mean_best_reward: --\n",
      "  9033/100000: episode: 470, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.123 [-0.754, 1.685], mean_best_reward: --\n",
      "  9043/100000: episode: 471, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.746, 2.651], mean_best_reward: --\n",
      "  9059/100000: episode: 472, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.050 [-1.778, 2.596], mean_best_reward: --\n",
      "  9080/100000: episode: 473, duration: 0.097s, episode steps: 21, steps per second: 216, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.062 [-0.809, 1.144], mean_best_reward: --\n",
      "  9101/100000: episode: 474, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.238 [0.000, 1.000], mean observation: 0.051 [-2.091, 3.055], mean_best_reward: --\n",
      "  9110/100000: episode: 475, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.778, 2.862], mean_best_reward: --\n",
      "  9119/100000: episode: 476, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.158 [-1.858, 1.130], mean_best_reward: --\n",
      "  9142/100000: episode: 477, duration: 0.105s, episode steps: 23, steps per second: 218, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.261 [0.000, 1.000], mean observation: 0.036 [-2.154, 3.129], mean_best_reward: --\n",
      "  9186/100000: episode: 478, duration: 0.214s, episode steps: 44, steps per second: 206, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.048 [-0.800, 0.607], mean_best_reward: --\n",
      "  9204/100000: episode: 479, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.577, 0.968], mean_best_reward: --\n",
      "  9213/100000: episode: 480, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.144 [-1.752, 0.953], mean_best_reward: --\n",
      "  9231/100000: episode: 481, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.121 [-0.553, 1.011], mean_best_reward: --\n",
      "  9245/100000: episode: 482, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.112 [-1.572, 2.568], mean_best_reward: --\n",
      "  9258/100000: episode: 483, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.121 [-2.019, 1.153], mean_best_reward: --\n",
      "  9286/100000: episode: 484, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.089 [-1.693, 0.776], mean_best_reward: --\n",
      "  9302/100000: episode: 485, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.950, 1.566], mean_best_reward: --\n",
      "  9317/100000: episode: 486, duration: 0.070s, episode steps: 15, steps per second: 213, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-0.984, 1.846], mean_best_reward: --\n",
      "  9328/100000: episode: 487, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.295, 1.366], mean_best_reward: --\n",
      "  9342/100000: episode: 488, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.103 [-1.164, 1.992], mean_best_reward: --\n",
      "  9359/100000: episode: 489, duration: 0.079s, episode steps: 17, steps per second: 216, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.064 [-1.126, 1.837], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9379/100000: episode: 490, duration: 0.111s, episode steps: 20, steps per second: 179, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.068 [-1.006, 1.803], mean_best_reward: --\n",
      "  9391/100000: episode: 491, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.134 [-1.206, 2.092], mean_best_reward: --\n",
      "  9402/100000: episode: 492, duration: 0.078s, episode steps: 11, steps per second: 142, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.106 [-1.414, 2.301], mean_best_reward: --\n",
      "  9412/100000: episode: 493, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.111 [-1.584, 2.373], mean_best_reward: --\n",
      "  9455/100000: episode: 494, duration: 0.205s, episode steps: 43, steps per second: 210, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.036 [-0.549, 0.960], mean_best_reward: --\n",
      "  9486/100000: episode: 495, duration: 0.153s, episode steps: 31, steps per second: 202, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.043 [-1.157, 1.264], mean_best_reward: --\n",
      "  9498/100000: episode: 496, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-1.832, 1.210], mean_best_reward: --\n",
      "  9509/100000: episode: 497, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.128 [-0.937, 1.615], mean_best_reward: --\n",
      "  9520/100000: episode: 498, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.390, 2.303], mean_best_reward: --\n",
      "  9533/100000: episode: 499, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.138 [-1.731, 2.872], mean_best_reward: --\n",
      "  9545/100000: episode: 500, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.848, 1.196], mean_best_reward: --\n",
      "  9557/100000: episode: 501, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.897, 1.132], mean_best_reward: 99.500000\n",
      "  9627/100000: episode: 502, duration: 0.333s, episode steps: 70, steps per second: 210, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.095 [-0.778, 1.026], mean_best_reward: --\n",
      "  9649/100000: episode: 503, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.062 [-1.367, 0.752], mean_best_reward: --\n",
      "  9707/100000: episode: 504, duration: 0.275s, episode steps: 58, steps per second: 211, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.089 [-1.838, 0.533], mean_best_reward: --\n",
      "  9737/100000: episode: 505, duration: 0.144s, episode steps: 30, steps per second: 208, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.054 [-1.055, 0.759], mean_best_reward: --\n",
      "  9753/100000: episode: 506, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.044, 0.591], mean_best_reward: --\n",
      "  9771/100000: episode: 507, duration: 0.093s, episode steps: 18, steps per second: 195, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.931, 0.540], mean_best_reward: --\n",
      "  9782/100000: episode: 508, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.126 [-1.788, 0.986], mean_best_reward: --\n",
      "  9838/100000: episode: 509, duration: 0.279s, episode steps: 56, steps per second: 200, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.095 [-0.863, 0.569], mean_best_reward: --\n",
      "  9911/100000: episode: 510, duration: 0.356s, episode steps: 73, steps per second: 205, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.044 [-0.576, 0.977], mean_best_reward: --\n",
      "  9931/100000: episode: 511, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.058 [-1.447, 0.992], mean_best_reward: --\n",
      "  9953/100000: episode: 512, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.092 [-1.283, 0.627], mean_best_reward: --\n",
      " 10015/100000: episode: 513, duration: 0.315s, episode steps: 62, steps per second: 197, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.032 [-0.757, 1.125], mean_best_reward: --\n",
      " 10096/100000: episode: 514, duration: 0.384s, episode steps: 81, steps per second: 211, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.121 [-1.115, 0.601], mean_best_reward: --\n",
      " 10121/100000: episode: 515, duration: 0.114s, episode steps: 25, steps per second: 219, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.074 [-1.195, 0.792], mean_best_reward: --\n",
      " 10214/100000: episode: 516, duration: 0.448s, episode steps: 93, steps per second: 207, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.062 [-1.126, 0.731], mean_best_reward: --\n",
      " 10256/100000: episode: 517, duration: 0.214s, episode steps: 42, steps per second: 197, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.056 [-1.810, 1.385], mean_best_reward: --\n",
      " 10301/100000: episode: 518, duration: 0.229s, episode steps: 45, steps per second: 196, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.042 [-1.017, 0.631], mean_best_reward: --\n",
      " 10353/100000: episode: 519, duration: 0.246s, episode steps: 52, steps per second: 211, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.107 [-0.818, 1.867], mean_best_reward: --\n",
      " 10389/100000: episode: 520, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.083 [-0.876, 0.607], mean_best_reward: --\n",
      " 10405/100000: episode: 521, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.074 [-0.829, 1.414], mean_best_reward: --\n",
      " 10423/100000: episode: 522, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-2.056, 1.134], mean_best_reward: --\n",
      " 10459/100000: episode: 523, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.058, 0.623], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10508/100000: episode: 524, duration: 0.233s, episode steps: 49, steps per second: 210, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.136 [-0.957, 0.440], mean_best_reward: --\n",
      " 10531/100000: episode: 525, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.092 [-1.142, 0.612], mean_best_reward: --\n",
      " 10555/100000: episode: 526, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.057 [-1.589, 0.809], mean_best_reward: --\n",
      " 10599/100000: episode: 527, duration: 0.223s, episode steps: 44, steps per second: 197, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.086 [-0.536, 0.940], mean_best_reward: --\n",
      " 10632/100000: episode: 528, duration: 0.148s, episode steps: 33, steps per second: 222, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.117 [-1.143, 0.737], mean_best_reward: --\n",
      " 10658/100000: episode: 529, duration: 0.121s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.097 [-0.935, 0.367], mean_best_reward: --\n",
      " 10672/100000: episode: 530, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.114 [-1.989, 1.139], mean_best_reward: --\n",
      " 10694/100000: episode: 531, duration: 0.101s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.104 [-0.836, 0.385], mean_best_reward: --\n",
      " 10749/100000: episode: 532, duration: 0.280s, episode steps: 55, steps per second: 196, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.090 [-1.065, 0.730], mean_best_reward: --\n",
      " 10770/100000: episode: 533, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.122 [-1.014, 0.558], mean_best_reward: --\n",
      " 10810/100000: episode: 534, duration: 0.192s, episode steps: 40, steps per second: 208, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.108 [-0.881, 0.536], mean_best_reward: --\n",
      " 10865/100000: episode: 535, duration: 0.254s, episode steps: 55, steps per second: 216, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.025 [-0.609, 1.115], mean_best_reward: --\n",
      " 10924/100000: episode: 536, duration: 0.278s, episode steps: 59, steps per second: 212, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.182 [-0.632, 1.491], mean_best_reward: --\n",
      " 10961/100000: episode: 537, duration: 0.176s, episode steps: 37, steps per second: 210, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.085 [-1.225, 0.559], mean_best_reward: --\n",
      " 10971/100000: episode: 538, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.115 [-1.564, 1.022], mean_best_reward: --\n",
      " 10993/100000: episode: 539, duration: 0.101s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.092 [-1.351, 0.611], mean_best_reward: --\n",
      " 11028/100000: episode: 540, duration: 0.165s, episode steps: 35, steps per second: 212, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.124 [-0.966, 0.352], mean_best_reward: --\n",
      " 11055/100000: episode: 541, duration: 0.128s, episode steps: 27, steps per second: 212, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.079 [-2.045, 1.039], mean_best_reward: --\n",
      " 11088/100000: episode: 542, duration: 0.150s, episode steps: 33, steps per second: 219, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.077 [-0.299, 0.990], mean_best_reward: --\n",
      " 11137/100000: episode: 543, duration: 0.224s, episode steps: 49, steps per second: 219, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.039 [-1.303, 0.785], mean_best_reward: --\n",
      " 11168/100000: episode: 544, duration: 0.140s, episode steps: 31, steps per second: 222, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.133 [-0.861, 0.403], mean_best_reward: --\n",
      " 11201/100000: episode: 545, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.089 [-0.835, 0.543], mean_best_reward: --\n",
      " 11221/100000: episode: 546, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.116 [-1.370, 0.564], mean_best_reward: --\n",
      " 11268/100000: episode: 547, duration: 0.294s, episode steps: 47, steps per second: 160, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.037 [-0.626, 1.319], mean_best_reward: --\n",
      " 11304/100000: episode: 548, duration: 0.264s, episode steps: 36, steps per second: 137, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.100 [-0.799, 0.500], mean_best_reward: --\n",
      " 11353/100000: episode: 549, duration: 0.295s, episode steps: 49, steps per second: 166, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.080 [-1.277, 0.355], mean_best_reward: --\n",
      " 11396/100000: episode: 550, duration: 0.252s, episode steps: 43, steps per second: 171, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.067 [-1.092, 0.795], mean_best_reward: --\n",
      " 11435/100000: episode: 551, duration: 0.185s, episode steps: 39, steps per second: 211, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.032 [-1.420, 0.772], mean_best_reward: 100.000000\n",
      " 11460/100000: episode: 552, duration: 0.158s, episode steps: 25, steps per second: 158, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.088 [-0.733, 1.147], mean_best_reward: --\n",
      " 11495/100000: episode: 553, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.094 [-0.556, 1.062], mean_best_reward: --\n",
      " 11509/100000: episode: 554, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.088 [-1.349, 0.821], mean_best_reward: --\n",
      " 11524/100000: episode: 555, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.090 [-0.829, 1.465], mean_best_reward: --\n",
      " 11544/100000: episode: 556, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.081 [-1.676, 0.823], mean_best_reward: --\n",
      " 11559/100000: episode: 557, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-1.796, 1.015], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11616/100000: episode: 558, duration: 0.259s, episode steps: 57, steps per second: 220, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.062 [-0.499, 0.878], mean_best_reward: --\n",
      " 11664/100000: episode: 559, duration: 0.217s, episode steps: 48, steps per second: 221, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-0.391, 1.122], mean_best_reward: --\n",
      " 11724/100000: episode: 560, duration: 0.288s, episode steps: 60, steps per second: 209, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.126 [-0.780, 1.121], mean_best_reward: --\n",
      " 11735/100000: episode: 561, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.857, 1.783], mean_best_reward: --\n",
      " 11745/100000: episode: 562, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.063, 1.965], mean_best_reward: --\n",
      " 11754/100000: episode: 563, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.757, 2.829], mean_best_reward: --\n",
      " 11769/100000: episode: 564, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-0.977, 1.752], mean_best_reward: --\n",
      " 11786/100000: episode: 565, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.102 [-0.573, 1.244], mean_best_reward: --\n",
      " 11803/100000: episode: 566, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.058 [-2.239, 1.393], mean_best_reward: --\n",
      " 11838/100000: episode: 567, duration: 0.159s, episode steps: 35, steps per second: 220, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.314 [0.000, 1.000], mean observation: 0.005 [-2.542, 3.498], mean_best_reward: --\n",
      " 11866/100000: episode: 568, duration: 0.131s, episode steps: 28, steps per second: 214, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.453, 0.978], mean_best_reward: --\n",
      " 11911/100000: episode: 569, duration: 0.212s, episode steps: 45, steps per second: 212, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.046 [-0.397, 0.852], mean_best_reward: --\n",
      " 11971/100000: episode: 570, duration: 0.265s, episode steps: 60, steps per second: 226, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.171 [-2.016, 2.592], mean_best_reward: --\n",
      " 11996/100000: episode: 571, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.119 [-0.404, 0.872], mean_best_reward: --\n",
      " 12022/100000: episode: 572, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.044, 0.583], mean_best_reward: --\n",
      " 12037/100000: episode: 573, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-1.159, 2.113], mean_best_reward: --\n",
      " 12074/100000: episode: 574, duration: 0.168s, episode steps: 37, steps per second: 220, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: -0.024 [-2.074, 1.326], mean_best_reward: --\n",
      " 12098/100000: episode: 575, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.036 [-2.250, 1.525], mean_best_reward: --\n",
      " 12141/100000: episode: 576, duration: 0.202s, episode steps: 43, steps per second: 213, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.078 [-1.163, 0.578], mean_best_reward: --\n",
      " 12170/100000: episode: 577, duration: 0.131s, episode steps: 29, steps per second: 222, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.096 [-0.604, 1.681], mean_best_reward: --\n",
      " 12184/100000: episode: 578, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.003, 0.559], mean_best_reward: --\n",
      " 12208/100000: episode: 579, duration: 0.113s, episode steps: 24, steps per second: 213, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.121 [-0.562, 1.053], mean_best_reward: --\n",
      " 12341/100000: episode: 580, duration: 0.586s, episode steps: 133, steps per second: 227, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.104 [-1.025, 0.714], mean_best_reward: --\n",
      " 12354/100000: episode: 581, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-1.006, 1.676], mean_best_reward: --\n",
      " 12367/100000: episode: 582, duration: 0.062s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.110 [-1.159, 1.792], mean_best_reward: --\n",
      " 12383/100000: episode: 583, duration: 0.075s, episode steps: 16, steps per second: 215, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.103 [-2.176, 1.155], mean_best_reward: --\n",
      " 12414/100000: episode: 584, duration: 0.142s, episode steps: 31, steps per second: 219, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.082 [-0.428, 0.827], mean_best_reward: --\n",
      " 12452/100000: episode: 585, duration: 0.173s, episode steps: 38, steps per second: 220, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.605 [0.000, 1.000], mean observation: 0.058 [-1.872, 1.585], mean_best_reward: --\n",
      " 12463/100000: episode: 586, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-1.874, 1.186], mean_best_reward: --\n",
      " 12500/100000: episode: 587, duration: 0.169s, episode steps: 37, steps per second: 219, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.094 [-0.581, 1.235], mean_best_reward: --\n",
      " 12510/100000: episode: 588, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.116 [-2.500, 1.580], mean_best_reward: --\n",
      " 12521/100000: episode: 589, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.138 [-1.360, 2.372], mean_best_reward: --\n",
      " 12531/100000: episode: 590, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.144 [-1.602, 2.615], mean_best_reward: --\n",
      " 12554/100000: episode: 591, duration: 0.107s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.063 [-0.638, 1.031], mean_best_reward: --\n",
      " 12572/100000: episode: 592, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.092 [-0.803, 1.678], mean_best_reward: --\n",
      " 12581/100000: episode: 593, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.247, 1.400], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12593/100000: episode: 594, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.101 [-2.477, 1.586], mean_best_reward: --\n",
      " 12605/100000: episode: 595, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.123 [-1.895, 1.137], mean_best_reward: --\n",
      " 12643/100000: episode: 596, duration: 0.175s, episode steps: 38, steps per second: 217, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.432, 0.790], mean_best_reward: --\n",
      " 12685/100000: episode: 597, duration: 0.188s, episode steps: 42, steps per second: 223, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.451, 0.955], mean_best_reward: --\n",
      " 12724/100000: episode: 598, duration: 0.179s, episode steps: 39, steps per second: 217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.058 [-1.050, 0.562], mean_best_reward: --\n",
      " 12749/100000: episode: 599, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.092 [-0.619, 0.938], mean_best_reward: --\n",
      " 12772/100000: episode: 600, duration: 0.106s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.067 [-0.421, 1.079], mean_best_reward: --\n",
      " 12803/100000: episode: 601, duration: 0.140s, episode steps: 31, steps per second: 221, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.109 [-0.987, 0.400], mean_best_reward: 83.000000\n",
      " 12814/100000: episode: 602, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.146 [-2.894, 1.728], mean_best_reward: --\n",
      " 12833/100000: episode: 603, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.097 [-0.609, 1.167], mean_best_reward: --\n",
      " 12843/100000: episode: 604, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.575, 2.544], mean_best_reward: --\n",
      " 12859/100000: episode: 605, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.072 [-1.984, 3.048], mean_best_reward: --\n",
      " 12885/100000: episode: 606, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.046 [-0.804, 1.373], mean_best_reward: --\n",
      " 12899/100000: episode: 607, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-1.000, 1.541], mean_best_reward: --\n",
      " 12908/100000: episode: 608, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-1.787, 0.973], mean_best_reward: --\n",
      " 12936/100000: episode: 609, duration: 0.129s, episode steps: 28, steps per second: 217, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.082 [-1.184, 2.299], mean_best_reward: --\n",
      " 12947/100000: episode: 610, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.011, 1.712], mean_best_reward: --\n",
      " 12958/100000: episode: 611, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.086 [-1.373, 0.822], mean_best_reward: --\n",
      " 12969/100000: episode: 612, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.126 [-0.947, 1.654], mean_best_reward: --\n",
      " 12994/100000: episode: 613, duration: 0.118s, episode steps: 25, steps per second: 212, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.079 [-0.568, 1.091], mean_best_reward: --\n",
      " 13002/100000: episode: 614, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.553, 1.583], mean_best_reward: --\n",
      " 13024/100000: episode: 615, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.081 [-1.156, 0.556], mean_best_reward: --\n",
      " 13034/100000: episode: 616, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.041, 2.001], mean_best_reward: --\n",
      " 13049/100000: episode: 617, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.082 [-1.422, 2.322], mean_best_reward: --\n",
      " 13079/100000: episode: 618, duration: 0.140s, episode steps: 30, steps per second: 214, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.599, 1.001], mean_best_reward: --\n",
      " 13094/100000: episode: 619, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.092 [-1.363, 2.329], mean_best_reward: --\n",
      " 13103/100000: episode: 620, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.130 [-1.801, 1.157], mean_best_reward: --\n",
      " 13117/100000: episode: 621, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.093 [-1.540, 2.444], mean_best_reward: --\n",
      " 13127/100000: episode: 622, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.116 [-1.627, 1.025], mean_best_reward: --\n",
      " 13137/100000: episode: 623, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.140, 2.046], mean_best_reward: --\n",
      " 13242/100000: episode: 624, duration: 0.469s, episode steps: 105, steps per second: 224, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.158 [-0.820, 1.071], mean_best_reward: --\n",
      " 13259/100000: episode: 625, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.098 [-1.744, 2.831], mean_best_reward: --\n",
      " 13270/100000: episode: 626, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.104 [-1.392, 2.153], mean_best_reward: --\n",
      " 13387/100000: episode: 627, duration: 0.526s, episode steps: 117, steps per second: 222, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.079 [-1.325, 1.548], mean_best_reward: --\n",
      " 13398/100000: episode: 628, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.095 [-1.216, 1.885], mean_best_reward: --\n",
      " 13416/100000: episode: 629, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.065 [-0.999, 1.569], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13434/100000: episode: 630, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.087 [-1.681, 0.954], mean_best_reward: --\n",
      " 13463/100000: episode: 631, duration: 0.133s, episode steps: 29, steps per second: 218, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.055 [-1.057, 0.564], mean_best_reward: --\n",
      " 13474/100000: episode: 632, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.121 [-1.388, 2.259], mean_best_reward: --\n",
      " 13486/100000: episode: 633, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.111 [-1.549, 2.500], mean_best_reward: --\n",
      " 13497/100000: episode: 634, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.401, 1.561], mean_best_reward: --\n",
      " 13508/100000: episode: 635, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.103 [-1.930, 1.225], mean_best_reward: --\n",
      " 13528/100000: episode: 636, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.824, 1.671], mean_best_reward: --\n",
      " 13545/100000: episode: 637, duration: 0.079s, episode steps: 17, steps per second: 215, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.081 [-1.629, 1.003], mean_best_reward: --\n",
      " 13563/100000: episode: 638, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.090 [-0.980, 1.763], mean_best_reward: --\n",
      " 13592/100000: episode: 639, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.027 [-0.972, 0.625], mean_best_reward: --\n",
      " 13611/100000: episode: 640, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.086 [-0.593, 1.234], mean_best_reward: --\n",
      " 13644/100000: episode: 641, duration: 0.157s, episode steps: 33, steps per second: 211, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.095 [-0.611, 1.292], mean_best_reward: --\n",
      " 13656/100000: episode: 642, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.133 [-1.534, 2.561], mean_best_reward: --\n",
      " 13671/100000: episode: 643, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.964, 1.682], mean_best_reward: --\n",
      " 13699/100000: episode: 644, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.679 [0.000, 1.000], mean observation: -0.036 [-2.807, 1.914], mean_best_reward: --\n",
      " 13731/100000: episode: 645, duration: 0.147s, episode steps: 32, steps per second: 218, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.058 [-1.209, 0.640], mean_best_reward: --\n",
      " 13756/100000: episode: 646, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.055 [-0.995, 1.915], mean_best_reward: --\n",
      " 13770/100000: episode: 647, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.110 [-1.356, 2.209], mean_best_reward: --\n",
      " 13781/100000: episode: 648, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.105 [-2.234, 1.413], mean_best_reward: --\n",
      " 13796/100000: episode: 649, duration: 0.070s, episode steps: 15, steps per second: 213, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.088 [-2.289, 1.336], mean_best_reward: --\n",
      " 13818/100000: episode: 650, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.773 [0.000, 1.000], mean observation: -0.030 [-3.342, 2.319], mean_best_reward: --\n",
      " 13832/100000: episode: 651, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.958, 1.588], mean_best_reward: 52.000000\n",
      " 13860/100000: episode: 652, duration: 0.129s, episode steps: 28, steps per second: 218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.421, 0.952], mean_best_reward: --\n",
      " 13902/100000: episode: 653, duration: 0.204s, episode steps: 42, steps per second: 206, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.266, 0.782], mean_best_reward: --\n",
      " 13911/100000: episode: 654, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.125 [-1.178, 1.840], mean_best_reward: --\n",
      " 13932/100000: episode: 655, duration: 0.098s, episode steps: 21, steps per second: 214, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.092 [-1.116, 0.618], mean_best_reward: --\n",
      " 13955/100000: episode: 656, duration: 0.107s, episode steps: 23, steps per second: 214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.078 [-1.476, 0.624], mean_best_reward: --\n",
      " 13971/100000: episode: 657, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.092 [-1.747, 0.975], mean_best_reward: --\n",
      " 13990/100000: episode: 658, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.089 [-0.572, 1.309], mean_best_reward: --\n",
      " 14003/100000: episode: 659, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-1.130, 1.832], mean_best_reward: --\n",
      " 14034/100000: episode: 660, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.098 [-0.978, 0.635], mean_best_reward: --\n",
      " 14073/100000: episode: 661, duration: 0.179s, episode steps: 39, steps per second: 217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: -0.040 [-1.756, 0.954], mean_best_reward: --\n",
      " 14172/100000: episode: 662, duration: 0.489s, episode steps: 99, steps per second: 203, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.041 [-1.423, 0.850], mean_best_reward: --\n",
      " 14199/100000: episode: 663, duration: 0.124s, episode steps: 27, steps per second: 218, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.076 [-1.502, 0.957], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14220/100000: episode: 664, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.088 [-1.374, 0.613], mean_best_reward: --\n",
      " 14255/100000: episode: 665, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.079 [-1.118, 0.576], mean_best_reward: --\n",
      " 14298/100000: episode: 666, duration: 0.208s, episode steps: 43, steps per second: 206, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.115 [-1.186, 0.476], mean_best_reward: --\n",
      " 14329/100000: episode: 667, duration: 0.144s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.033 [-1.223, 1.864], mean_best_reward: --\n",
      " 14346/100000: episode: 668, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.093 [-1.014, 1.515], mean_best_reward: --\n",
      " 14394/100000: episode: 669, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.688, 1.041], mean_best_reward: --\n",
      " 14417/100000: episode: 670, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.090 [-1.312, 0.766], mean_best_reward: --\n",
      " 14447/100000: episode: 671, duration: 0.144s, episode steps: 30, steps per second: 209, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.571, 1.430], mean_best_reward: --\n",
      " 14485/100000: episode: 672, duration: 0.182s, episode steps: 38, steps per second: 209, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.077, 0.736], mean_best_reward: --\n",
      " 14499/100000: episode: 673, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.089 [-1.719, 1.002], mean_best_reward: --\n",
      " 14510/100000: episode: 674, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.092 [-2.213, 1.416], mean_best_reward: --\n",
      " 14525/100000: episode: 675, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.092 [-1.363, 0.803], mean_best_reward: --\n",
      " 14549/100000: episode: 676, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.982, 1.496], mean_best_reward: --\n",
      " 14586/100000: episode: 677, duration: 0.173s, episode steps: 37, steps per second: 214, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.108 [-1.234, 0.575], mean_best_reward: --\n",
      " 14604/100000: episode: 678, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.091 [-1.549, 2.435], mean_best_reward: --\n",
      " 14625/100000: episode: 679, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.085 [-1.792, 0.984], mean_best_reward: --\n",
      " 14639/100000: episode: 680, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.117 [-1.313, 0.613], mean_best_reward: --\n",
      " 14710/100000: episode: 681, duration: 0.339s, episode steps: 71, steps per second: 210, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.220 [-1.451, 0.795], mean_best_reward: --\n",
      " 14760/100000: episode: 682, duration: 0.230s, episode steps: 50, steps per second: 218, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.007 [-1.240, 0.793], mean_best_reward: --\n",
      " 14791/100000: episode: 683, duration: 0.151s, episode steps: 31, steps per second: 206, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.039 [-1.383, 0.946], mean_best_reward: --\n",
      " 14827/100000: episode: 684, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.007 [-1.576, 2.516], mean_best_reward: --\n",
      " 14841/100000: episode: 685, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.101 [-1.549, 0.837], mean_best_reward: --\n",
      " 14859/100000: episode: 686, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.131 [-0.934, 0.380], mean_best_reward: --\n",
      " 14905/100000: episode: 687, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.980, 0.354], mean_best_reward: --\n",
      " 14974/100000: episode: 688, duration: 0.329s, episode steps: 69, steps per second: 209, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.093 [-0.597, 1.109], mean_best_reward: --\n",
      " 14989/100000: episode: 689, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.085 [-1.244, 0.636], mean_best_reward: --\n",
      " 15014/100000: episode: 690, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.111 [-0.646, 1.676], mean_best_reward: --\n",
      " 15026/100000: episode: 691, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.106 [-3.016, 1.920], mean_best_reward: --\n",
      " 15045/100000: episode: 692, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.083 [-1.060, 0.589], mean_best_reward: --\n",
      " 15074/100000: episode: 693, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.070 [-1.360, 2.425], mean_best_reward: --\n",
      " 15090/100000: episode: 694, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.123 [-1.214, 0.744], mean_best_reward: --\n",
      " 15116/100000: episode: 695, duration: 0.121s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.084 [-0.431, 0.890], mean_best_reward: --\n",
      " 15134/100000: episode: 696, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.206, 0.579], mean_best_reward: --\n",
      " 15146/100000: episode: 697, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.584, 2.580], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15188/100000: episode: 698, duration: 0.197s, episode steps: 42, steps per second: 213, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.074 [-1.121, 0.810], mean_best_reward: --\n",
      " 15242/100000: episode: 699, duration: 0.244s, episode steps: 54, steps per second: 221, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.090 [-0.418, 0.729], mean_best_reward: --\n",
      " 15315/100000: episode: 700, duration: 0.326s, episode steps: 73, steps per second: 224, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.102 [-0.486, 1.697], mean_best_reward: --\n",
      " 15330/100000: episode: 701, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.093 [-2.211, 1.362], mean_best_reward: 113.000000\n",
      " 15354/100000: episode: 702, duration: 0.110s, episode steps: 24, steps per second: 218, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.629, 1.069], mean_best_reward: --\n",
      " 15472/100000: episode: 703, duration: 0.523s, episode steps: 118, steps per second: 226, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.116 [-0.551, 0.837], mean_best_reward: --\n",
      " 15504/100000: episode: 704, duration: 0.146s, episode steps: 32, steps per second: 219, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.062 [-0.599, 1.338], mean_best_reward: --\n",
      " 15518/100000: episode: 705, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.098 [-2.580, 1.536], mean_best_reward: --\n",
      " 15548/100000: episode: 706, duration: 0.137s, episode steps: 30, steps per second: 218, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.634, 1.061], mean_best_reward: --\n",
      " 15574/100000: episode: 707, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.378, 0.935], mean_best_reward: --\n",
      " 15608/100000: episode: 708, duration: 0.158s, episode steps: 34, steps per second: 215, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.154 [-0.774, 0.352], mean_best_reward: --\n",
      " 15643/100000: episode: 709, duration: 0.164s, episode steps: 35, steps per second: 214, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.155 [-0.380, 0.938], mean_best_reward: --\n",
      " 15654/100000: episode: 710, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-0.947, 1.749], mean_best_reward: --\n",
      " 15669/100000: episode: 711, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.080 [-1.208, 1.849], mean_best_reward: --\n",
      " 15724/100000: episode: 712, duration: 0.250s, episode steps: 55, steps per second: 220, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.164 [-1.312, 0.388], mean_best_reward: --\n",
      " 15755/100000: episode: 713, duration: 0.144s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.072 [-1.040, 0.614], mean_best_reward: --\n",
      " 15787/100000: episode: 714, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.084 [-0.959, 0.476], mean_best_reward: --\n",
      " 15822/100000: episode: 715, duration: 0.162s, episode steps: 35, steps per second: 216, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.080 [-1.459, 0.634], mean_best_reward: --\n",
      " 15868/100000: episode: 716, duration: 0.210s, episode steps: 46, steps per second: 219, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.045 [-1.104, 0.388], mean_best_reward: --\n",
      " 15901/100000: episode: 717, duration: 0.149s, episode steps: 33, steps per second: 221, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.096 [-0.427, 1.142], mean_best_reward: --\n",
      " 15923/100000: episode: 718, duration: 0.104s, episode steps: 22, steps per second: 211, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.033 [-1.518, 1.013], mean_best_reward: --\n",
      " 15944/100000: episode: 719, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.065 [-0.622, 1.067], mean_best_reward: --\n",
      " 15965/100000: episode: 720, duration: 0.096s, episode steps: 21, steps per second: 218, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.095 [-0.785, 1.561], mean_best_reward: --\n",
      " 16140/100000: episode: 721, duration: 0.767s, episode steps: 175, steps per second: 228, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.070 [-1.010, 1.959], mean_best_reward: --\n",
      " 16188/100000: episode: 722, duration: 0.219s, episode steps: 48, steps per second: 219, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.083 [-0.378, 1.040], mean_best_reward: --\n",
      " 16200/100000: episode: 723, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.942, 1.734], mean_best_reward: --\n",
      " 16255/100000: episode: 724, duration: 0.248s, episode steps: 55, steps per second: 222, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.082 [-0.454, 1.063], mean_best_reward: --\n",
      " 16289/100000: episode: 725, duration: 0.159s, episode steps: 34, steps per second: 214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.039 [-1.027, 1.631], mean_best_reward: --\n",
      " 16356/100000: episode: 726, duration: 0.296s, episode steps: 67, steps per second: 226, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.044 [-0.742, 1.728], mean_best_reward: --\n",
      " 16400/100000: episode: 727, duration: 0.213s, episode steps: 44, steps per second: 207, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.455, 1.126], mean_best_reward: --\n",
      " 16424/100000: episode: 728, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.633, 0.939], mean_best_reward: --\n",
      " 16452/100000: episode: 729, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.102 [-0.600, 1.490], mean_best_reward: --\n",
      " 16586/100000: episode: 730, duration: 0.670s, episode steps: 134, steps per second: 200, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.029 [-0.756, 1.406], mean_best_reward: --\n",
      " 16600/100000: episode: 731, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.056 [-2.304, 1.591], mean_best_reward: --\n",
      " 16624/100000: episode: 732, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.045 [-0.985, 1.677], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16635/100000: episode: 733, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.124 [-1.714, 0.967], mean_best_reward: --\n",
      " 16680/100000: episode: 734, duration: 0.204s, episode steps: 45, steps per second: 220, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.053 [-1.140, 1.532], mean_best_reward: --\n",
      " 16719/100000: episode: 735, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.083 [-0.765, 1.205], mean_best_reward: --\n",
      " 16767/100000: episode: 736, duration: 0.233s, episode steps: 48, steps per second: 206, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.045 [-0.630, 0.931], mean_best_reward: --\n",
      " 16807/100000: episode: 737, duration: 0.183s, episode steps: 40, steps per second: 219, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.128 [-0.347, 1.064], mean_best_reward: --\n",
      " 16924/100000: episode: 738, duration: 0.525s, episode steps: 117, steps per second: 223, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.118 [-0.637, 0.900], mean_best_reward: --\n",
      " 16948/100000: episode: 739, duration: 0.112s, episode steps: 24, steps per second: 215, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.394, 0.784], mean_best_reward: --\n",
      " 16969/100000: episode: 740, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.112 [-0.978, 0.564], mean_best_reward: --\n",
      " 16988/100000: episode: 741, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.080 [-0.783, 1.346], mean_best_reward: --\n",
      " 17022/100000: episode: 742, duration: 0.155s, episode steps: 34, steps per second: 219, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.115, 0.590], mean_best_reward: --\n",
      " 17067/100000: episode: 743, duration: 0.205s, episode steps: 45, steps per second: 219, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.117 [-0.543, 0.865], mean_best_reward: --\n",
      " 17085/100000: episode: 744, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.064 [-1.006, 1.709], mean_best_reward: --\n",
      " 17120/100000: episode: 745, duration: 0.247s, episode steps: 35, steps per second: 142, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.089 [-1.686, 0.614], mean_best_reward: --\n",
      " 17173/100000: episode: 746, duration: 0.268s, episode steps: 53, steps per second: 198, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.039 [-0.628, 1.650], mean_best_reward: --\n",
      " 17183/100000: episode: 747, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.120 [-2.497, 1.596], mean_best_reward: --\n",
      " 17226/100000: episode: 748, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.116 [-0.387, 0.996], mean_best_reward: --\n",
      " 17239/100000: episode: 749, duration: 0.070s, episode steps: 13, steps per second: 184, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.107 [-1.509, 0.836], mean_best_reward: --\n",
      " 17275/100000: episode: 750, duration: 0.166s, episode steps: 36, steps per second: 217, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.173, 1.111], mean_best_reward: --\n",
      " 17287/100000: episode: 751, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.101 [-0.971, 1.523], mean_best_reward: 105.000000\n",
      " 17314/100000: episode: 752, duration: 0.129s, episode steps: 27, steps per second: 209, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.113 [-1.321, 0.378], mean_best_reward: --\n",
      " 17434/100000: episode: 753, duration: 0.525s, episode steps: 120, steps per second: 229, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-1.378, 1.027], mean_best_reward: --\n",
      " 17465/100000: episode: 754, duration: 0.142s, episode steps: 31, steps per second: 218, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.013 [-1.876, 1.313], mean_best_reward: --\n",
      " 17500/100000: episode: 755, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.062 [-1.021, 0.423], mean_best_reward: --\n",
      " 17604/100000: episode: 756, duration: 0.510s, episode steps: 104, steps per second: 204, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-1.102, 1.102], mean_best_reward: --\n",
      " 17641/100000: episode: 757, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.091 [-1.390, 0.459], mean_best_reward: --\n",
      " 17680/100000: episode: 758, duration: 0.188s, episode steps: 39, steps per second: 208, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.137 [-1.288, 0.556], mean_best_reward: --\n",
      " 17735/100000: episode: 759, duration: 0.277s, episode steps: 55, steps per second: 199, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: 0.032 [-1.015, 1.670], mean_best_reward: --\n",
      " 17755/100000: episode: 760, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.091 [-0.779, 1.699], mean_best_reward: --\n",
      " 17795/100000: episode: 761, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.646, 0.912], mean_best_reward: --\n",
      " 17818/100000: episode: 762, duration: 0.108s, episode steps: 23, steps per second: 213, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.032 [-0.980, 1.487], mean_best_reward: --\n",
      " 17848/100000: episode: 763, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.113 [-0.597, 0.984], mean_best_reward: --\n",
      " 17871/100000: episode: 764, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.021 [-1.998, 1.381], mean_best_reward: --\n",
      " 17884/100000: episode: 765, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.086 [-2.202, 1.397], mean_best_reward: --\n",
      " 17909/100000: episode: 766, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.090 [-0.761, 1.299], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17964/100000: episode: 767, duration: 0.279s, episode steps: 55, steps per second: 197, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.158 [-0.502, 0.886], mean_best_reward: --\n",
      " 17984/100000: episode: 768, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.081 [-1.540, 0.822], mean_best_reward: --\n",
      " 18025/100000: episode: 769, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.067 [-1.708, 0.649], mean_best_reward: --\n",
      " 18063/100000: episode: 770, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-0.877, 0.579], mean_best_reward: --\n",
      " 18084/100000: episode: 771, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.030 [-2.584, 1.779], mean_best_reward: --\n",
      " 18093/100000: episode: 772, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.872, 1.794], mean_best_reward: --\n",
      " 18145/100000: episode: 773, duration: 0.247s, episode steps: 52, steps per second: 211, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.042 [-0.826, 1.224], mean_best_reward: --\n",
      " 18182/100000: episode: 774, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.057 [-1.109, 0.732], mean_best_reward: --\n",
      " 18243/100000: episode: 775, duration: 0.290s, episode steps: 61, steps per second: 210, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.130 [-1.036, 0.633], mean_best_reward: --\n",
      " 18302/100000: episode: 776, duration: 0.276s, episode steps: 59, steps per second: 214, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.127 [-0.915, 0.577], mean_best_reward: --\n",
      " 18369/100000: episode: 777, duration: 0.309s, episode steps: 67, steps per second: 217, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.082 [-0.631, 1.054], mean_best_reward: --\n",
      " 18405/100000: episode: 778, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.546, 0.897], mean_best_reward: --\n",
      " 18431/100000: episode: 779, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.066 [-0.632, 1.376], mean_best_reward: --\n",
      " 18447/100000: episode: 780, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.073 [-1.362, 0.804], mean_best_reward: --\n",
      " 18472/100000: episode: 781, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.110 [-0.944, 0.583], mean_best_reward: --\n",
      " 18504/100000: episode: 782, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.557, 0.925], mean_best_reward: --\n",
      " 18574/100000: episode: 783, duration: 0.331s, episode steps: 70, steps per second: 211, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.041 [-0.938, 1.550], mean_best_reward: --\n",
      " 18584/100000: episode: 784, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.150 [-2.538, 1.572], mean_best_reward: --\n",
      " 18607/100000: episode: 785, duration: 0.111s, episode steps: 23, steps per second: 206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.091 [-0.983, 0.410], mean_best_reward: --\n",
      " 18682/100000: episode: 786, duration: 0.374s, episode steps: 75, steps per second: 201, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.011 [-0.607, 1.038], mean_best_reward: --\n",
      " 18716/100000: episode: 787, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.117 [-0.393, 1.562], mean_best_reward: --\n",
      " 18779/100000: episode: 788, duration: 0.311s, episode steps: 63, steps per second: 203, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.069 [-0.924, 0.585], mean_best_reward: --\n",
      " 18816/100000: episode: 789, duration: 0.172s, episode steps: 37, steps per second: 215, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.083 [-0.416, 1.168], mean_best_reward: --\n",
      " 18840/100000: episode: 790, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.112 [-0.609, 1.487], mean_best_reward: --\n",
      " 18863/100000: episode: 791, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.089 [-0.777, 1.425], mean_best_reward: --\n",
      " 18893/100000: episode: 792, duration: 0.143s, episode steps: 30, steps per second: 209, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.200, 0.378], mean_best_reward: --\n",
      " 18909/100000: episode: 793, duration: 0.075s, episode steps: 16, steps per second: 213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.071 [-1.778, 1.184], mean_best_reward: --\n",
      " 18938/100000: episode: 794, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.099 [-1.045, 0.546], mean_best_reward: --\n",
      " 18950/100000: episode: 795, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.127 [-2.602, 1.540], mean_best_reward: --\n",
      " 18978/100000: episode: 796, duration: 0.132s, episode steps: 28, steps per second: 211, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.572, 1.078], mean_best_reward: --\n",
      " 18999/100000: episode: 797, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.100 [-2.080, 1.157], mean_best_reward: --\n",
      " 19062/100000: episode: 798, duration: 0.346s, episode steps: 63, steps per second: 182, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.131 [-1.587, 0.721], mean_best_reward: --\n",
      " 19135/100000: episode: 799, duration: 0.630s, episode steps: 73, steps per second: 116, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.039 [-0.994, 1.034], mean_best_reward: --\n",
      " 19155/100000: episode: 800, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.779, 1.103], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19176/100000: episode: 801, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.127 [-0.539, 0.961], mean_best_reward: 68.500000\n",
      " 19186/100000: episode: 802, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.142 [-1.993, 1.191], mean_best_reward: --\n",
      " 19220/100000: episode: 803, duration: 0.154s, episode steps: 34, steps per second: 221, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.542, 0.992], mean_best_reward: --\n",
      " 19247/100000: episode: 804, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.073 [-1.120, 0.585], mean_best_reward: --\n",
      " 19281/100000: episode: 805, duration: 0.209s, episode steps: 34, steps per second: 162, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.117 [-0.731, 0.360], mean_best_reward: --\n",
      " 19292/100000: episode: 806, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.795, 1.723], mean_best_reward: --\n",
      " 19306/100000: episode: 807, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.101 [-0.752, 1.326], mean_best_reward: --\n",
      " 19376/100000: episode: 808, duration: 0.403s, episode steps: 70, steps per second: 174, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.178 [-0.820, 1.265], mean_best_reward: --\n",
      " 19393/100000: episode: 809, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.075 [-2.814, 1.811], mean_best_reward: --\n",
      " 19448/100000: episode: 810, duration: 0.284s, episode steps: 55, steps per second: 194, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.095 [-1.216, 0.629], mean_best_reward: --\n",
      " 19463/100000: episode: 811, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.064 [-1.967, 1.375], mean_best_reward: --\n",
      " 19490/100000: episode: 812, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.079 [-0.936, 0.592], mean_best_reward: --\n",
      " 19537/100000: episode: 813, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.109 [-0.465, 1.338], mean_best_reward: --\n",
      " 19581/100000: episode: 814, duration: 0.214s, episode steps: 44, steps per second: 206, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.060 [-1.014, 0.581], mean_best_reward: --\n",
      " 19599/100000: episode: 815, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.051 [-2.508, 1.595], mean_best_reward: --\n",
      " 19634/100000: episode: 816, duration: 0.159s, episode steps: 35, steps per second: 220, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.103 [-0.961, 0.565], mean_best_reward: --\n",
      " 19654/100000: episode: 817, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.049 [-1.987, 1.210], mean_best_reward: --\n",
      " 19684/100000: episode: 818, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.052 [-1.226, 0.769], mean_best_reward: --\n",
      " 19719/100000: episode: 819, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.100 [-0.430, 1.146], mean_best_reward: --\n",
      " 19764/100000: episode: 820, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.099 [-0.362, 1.342], mean_best_reward: --\n",
      " 19808/100000: episode: 821, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.124 [-0.918, 0.468], mean_best_reward: --\n",
      " 19857/100000: episode: 822, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.157 [-1.253, 0.891], mean_best_reward: --\n",
      " 19893/100000: episode: 823, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.094 [-0.551, 0.886], mean_best_reward: --\n",
      " 19925/100000: episode: 824, duration: 0.144s, episode steps: 32, steps per second: 221, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.095 [-0.648, 1.606], mean_best_reward: --\n",
      " 19947/100000: episode: 825, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.837, 1.415], mean_best_reward: --\n",
      " 20011/100000: episode: 826, duration: 0.308s, episode steps: 64, steps per second: 208, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.150 [-1.625, 0.772], mean_best_reward: --\n",
      " 20115/100000: episode: 827, duration: 0.523s, episode steps: 104, steps per second: 199, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.168 [-1.154, 0.552], mean_best_reward: --\n",
      " 20135/100000: episode: 828, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.056 [-1.587, 1.005], mean_best_reward: --\n",
      " 20182/100000: episode: 829, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.161 [-0.442, 1.073], mean_best_reward: --\n",
      " 20209/100000: episode: 830, duration: 0.125s, episode steps: 27, steps per second: 216, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.103 [-1.199, 0.789], mean_best_reward: --\n",
      " 20218/100000: episode: 831, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.124 [-1.010, 1.636], mean_best_reward: --\n",
      " 20282/100000: episode: 832, duration: 0.326s, episode steps: 64, steps per second: 196, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.083 [-0.931, 0.382], mean_best_reward: --\n",
      " 20304/100000: episode: 833, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.050 [-1.096, 0.784], mean_best_reward: --\n",
      " 20318/100000: episode: 834, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.096 [-1.727, 0.973], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20331/100000: episode: 835, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.119 [-2.114, 1.337], mean_best_reward: --\n",
      " 20348/100000: episode: 836, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.073 [-2.740, 1.720], mean_best_reward: --\n",
      " 20396/100000: episode: 837, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.225, 0.547], mean_best_reward: --\n",
      " 20406/100000: episode: 838, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.130 [-0.995, 1.624], mean_best_reward: --\n",
      " 20427/100000: episode: 839, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.028 [-1.859, 1.335], mean_best_reward: --\n",
      " 20464/100000: episode: 840, duration: 0.168s, episode steps: 37, steps per second: 221, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.041 [-1.520, 1.725], mean_best_reward: --\n",
      " 20499/100000: episode: 841, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.011 [-1.250, 0.824], mean_best_reward: --\n",
      " 20572/100000: episode: 842, duration: 0.379s, episode steps: 73, steps per second: 192, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.045 [-1.267, 0.821], mean_best_reward: --\n",
      " 20595/100000: episode: 843, duration: 0.107s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.106 [-0.981, 0.425], mean_best_reward: --\n",
      " 20615/100000: episode: 844, duration: 0.092s, episode steps: 20, steps per second: 218, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.119 [-1.217, 0.762], mean_best_reward: --\n",
      " 20647/100000: episode: 845, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.009, 0.554], mean_best_reward: --\n",
      " 20714/100000: episode: 846, duration: 0.321s, episode steps: 67, steps per second: 209, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.062 [-0.784, 0.508], mean_best_reward: --\n",
      " 20745/100000: episode: 847, duration: 0.144s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.066 [-1.673, 0.866], mean_best_reward: --\n",
      " 20769/100000: episode: 848, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.632, 1.000], mean_best_reward: --\n",
      " 20783/100000: episode: 849, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.059 [-2.410, 1.603], mean_best_reward: --\n",
      " 20803/100000: episode: 850, duration: 0.092s, episode steps: 20, steps per second: 217, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.060 [-1.378, 0.840], mean_best_reward: --\n",
      " 20859/100000: episode: 851, duration: 0.258s, episode steps: 56, steps per second: 217, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.030 [-1.621, 1.162], mean_best_reward: 88.500000\n",
      " 20880/100000: episode: 852, duration: 0.096s, episode steps: 21, steps per second: 218, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.055 [-0.966, 1.546], mean_best_reward: --\n",
      " 20898/100000: episode: 853, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-2.135, 1.141], mean_best_reward: --\n",
      " 20920/100000: episode: 854, duration: 0.102s, episode steps: 22, steps per second: 216, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.086 [-0.795, 1.752], mean_best_reward: --\n",
      " 20934/100000: episode: 855, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.080 [-1.892, 1.214], mean_best_reward: --\n",
      " 20943/100000: episode: 856, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.308, 1.411], mean_best_reward: --\n",
      " 20957/100000: episode: 857, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.098 [-1.569, 2.593], mean_best_reward: --\n",
      " 20977/100000: episode: 858, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.102 [-0.629, 1.174], mean_best_reward: --\n",
      " 20988/100000: episode: 859, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.137 [-1.749, 2.770], mean_best_reward: --\n",
      " 21030/100000: episode: 860, duration: 0.188s, episode steps: 42, steps per second: 223, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.108 [-0.578, 1.140], mean_best_reward: --\n",
      " 21052/100000: episode: 861, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.070 [-0.456, 1.223], mean_best_reward: --\n",
      " 21062/100000: episode: 862, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.162, 2.039], mean_best_reward: --\n",
      " 21079/100000: episode: 863, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.053 [-0.827, 1.420], mean_best_reward: --\n",
      " 21091/100000: episode: 864, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.204, 2.007], mean_best_reward: --\n",
      " 21102/100000: episode: 865, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-1.024, 1.569], mean_best_reward: --\n",
      " 21114/100000: episode: 866, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.126 [-1.557, 2.506], mean_best_reward: --\n",
      " 21159/100000: episode: 867, duration: 0.203s, episode steps: 45, steps per second: 221, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.141 [-0.580, 1.804], mean_best_reward: --\n",
      " 21180/100000: episode: 868, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.079 [-0.757, 1.375], mean_best_reward: --\n",
      " 21193/100000: episode: 869, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.095 [-1.518, 0.977], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21224/100000: episode: 870, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.010 [-1.177, 1.470], mean_best_reward: --\n",
      " 21235/100000: episode: 871, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.112 [-1.754, 2.738], mean_best_reward: --\n",
      " 21246/100000: episode: 872, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.107 [-1.353, 2.194], mean_best_reward: --\n",
      " 21256/100000: episode: 873, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.027, 1.934], mean_best_reward: --\n",
      " 21300/100000: episode: 874, duration: 0.213s, episode steps: 44, steps per second: 207, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.681, 1.304], mean_best_reward: --\n",
      " 21316/100000: episode: 875, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.097 [-1.163, 2.078], mean_best_reward: --\n",
      " 21326/100000: episode: 876, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.577, 2.608], mean_best_reward: --\n",
      " 21346/100000: episode: 877, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.632, 1.173], mean_best_reward: --\n",
      " 21361/100000: episode: 878, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.048 [-2.804, 1.980], mean_best_reward: --\n",
      " 21377/100000: episode: 879, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.082 [-1.190, 1.837], mean_best_reward: --\n",
      " 21385/100000: episode: 880, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.555, 1.583], mean_best_reward: --\n",
      " 21394/100000: episode: 881, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.724, 2.826], mean_best_reward: --\n",
      " 21425/100000: episode: 882, duration: 0.141s, episode steps: 31, steps per second: 220, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.079 [-0.914, 0.561], mean_best_reward: --\n",
      " 21460/100000: episode: 883, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.044 [-0.579, 1.083], mean_best_reward: --\n",
      " 21471/100000: episode: 884, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.105 [-2.054, 1.373], mean_best_reward: --\n",
      " 21502/100000: episode: 885, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.100 [-1.268, 0.389], mean_best_reward: --\n",
      " 21522/100000: episode: 886, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.620, 1.023], mean_best_reward: --\n",
      " 21537/100000: episode: 887, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.094 [-1.366, 2.315], mean_best_reward: --\n",
      " 21555/100000: episode: 888, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.081 [-0.587, 1.261], mean_best_reward: --\n",
      " 21564/100000: episode: 889, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.144 [-1.395, 2.300], mean_best_reward: --\n",
      " 21577/100000: episode: 890, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.090 [-0.802, 1.231], mean_best_reward: --\n",
      " 21587/100000: episode: 891, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-0.959, 1.679], mean_best_reward: --\n",
      " 21621/100000: episode: 892, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.095 [-0.440, 0.719], mean_best_reward: --\n",
      " 21638/100000: episode: 893, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.113 [-0.575, 1.380], mean_best_reward: --\n",
      " 21655/100000: episode: 894, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.063 [-1.378, 2.119], mean_best_reward: --\n",
      " 21670/100000: episode: 895, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.107 [-2.391, 1.339], mean_best_reward: --\n",
      " 21685/100000: episode: 896, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.083 [-2.769, 1.796], mean_best_reward: --\n",
      " 21694/100000: episode: 897, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.750, 2.870], mean_best_reward: --\n",
      " 21708/100000: episode: 898, duration: 0.067s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.102 [-1.453, 0.949], mean_best_reward: --\n",
      " 21736/100000: episode: 899, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.054 [-0.824, 1.282], mean_best_reward: --\n",
      " 21762/100000: episode: 900, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.005 [-1.775, 2.493], mean_best_reward: --\n",
      " 21779/100000: episode: 901, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.119 [-1.256, 0.557], mean_best_reward: 84.500000\n",
      " 21791/100000: episode: 902, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.128 [-2.148, 1.174], mean_best_reward: --\n",
      " 21829/100000: episode: 903, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.106 [-0.774, 1.116], mean_best_reward: --\n",
      " 21859/100000: episode: 904, duration: 0.187s, episode steps: 30, steps per second: 160, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.084 [-0.771, 0.414], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21873/100000: episode: 905, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.117 [-1.145, 2.030], mean_best_reward: --\n",
      " 21888/100000: episode: 906, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.106 [-1.166, 0.614], mean_best_reward: --\n",
      " 21926/100000: episode: 907, duration: 0.184s, episode steps: 38, steps per second: 206, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.095 [-2.091, 1.008], mean_best_reward: --\n",
      " 21939/100000: episode: 908, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.104 [-2.213, 1.360], mean_best_reward: --\n",
      " 21950/100000: episode: 909, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.162 [-2.843, 1.720], mean_best_reward: --\n",
      " 21958/100000: episode: 910, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.546, 1.605], mean_best_reward: --\n",
      " 21994/100000: episode: 911, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.077 [-0.904, 0.618], mean_best_reward: --\n",
      " 22027/100000: episode: 912, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.055 [-0.984, 0.555], mean_best_reward: --\n",
      " 22153/100000: episode: 913, duration: 0.588s, episode steps: 126, steps per second: 214, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.261 [-1.086, 1.360], mean_best_reward: --\n",
      " 22173/100000: episode: 914, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.228, 0.820], mean_best_reward: --\n",
      " 22215/100000: episode: 915, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.188 [-0.454, 1.079], mean_best_reward: --\n",
      " 22274/100000: episode: 916, duration: 0.297s, episode steps: 59, steps per second: 199, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.020 [-1.221, 1.919], mean_best_reward: --\n",
      " 22296/100000: episode: 917, duration: 0.104s, episode steps: 22, steps per second: 211, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.558, 1.292], mean_best_reward: --\n",
      " 22312/100000: episode: 918, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.054 [-2.164, 1.405], mean_best_reward: --\n",
      " 22352/100000: episode: 919, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.130 [-0.949, 0.549], mean_best_reward: --\n",
      " 22370/100000: episode: 920, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.064 [-2.006, 1.176], mean_best_reward: --\n",
      " 22397/100000: episode: 921, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.119 [-1.036, 0.423], mean_best_reward: --\n",
      " 22460/100000: episode: 922, duration: 0.302s, episode steps: 63, steps per second: 209, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.020 [-1.746, 1.031], mean_best_reward: --\n",
      " 22496/100000: episode: 923, duration: 0.168s, episode steps: 36, steps per second: 215, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.097 [-0.895, 0.528], mean_best_reward: --\n",
      " 22531/100000: episode: 924, duration: 0.166s, episode steps: 35, steps per second: 211, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.097 [-0.837, 0.480], mean_best_reward: --\n",
      " 22542/100000: episode: 925, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.114 [-1.516, 0.961], mean_best_reward: --\n",
      " 22565/100000: episode: 926, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.084 [-0.438, 1.082], mean_best_reward: --\n",
      " 22620/100000: episode: 927, duration: 0.252s, episode steps: 55, steps per second: 218, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.118 [-1.213, 1.075], mean_best_reward: --\n",
      " 22648/100000: episode: 928, duration: 0.133s, episode steps: 28, steps per second: 210, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.105 [-0.941, 0.538], mean_best_reward: --\n",
      " 22712/100000: episode: 929, duration: 0.301s, episode steps: 64, steps per second: 213, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.245, 0.928], mean_best_reward: --\n",
      " 22751/100000: episode: 930, duration: 0.177s, episode steps: 39, steps per second: 220, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.034 [-0.983, 0.616], mean_best_reward: --\n",
      " 22766/100000: episode: 931, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.067 [-2.191, 1.347], mean_best_reward: --\n",
      " 22776/100000: episode: 932, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.525, 1.524], mean_best_reward: --\n",
      " 22786/100000: episode: 933, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-1.964, 1.150], mean_best_reward: --\n",
      " 22804/100000: episode: 934, duration: 0.085s, episode steps: 18, steps per second: 213, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.061 [-2.550, 1.583], mean_best_reward: --\n",
      " 22824/100000: episode: 935, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.101 [-1.278, 0.742], mean_best_reward: --\n",
      " 22842/100000: episode: 936, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.073 [-1.913, 1.152], mean_best_reward: --\n",
      " 22858/100000: episode: 937, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.979, 1.774], mean_best_reward: --\n",
      " 22875/100000: episode: 938, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.077 [-1.368, 0.778], mean_best_reward: --\n",
      " 22890/100000: episode: 939, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-2.115, 1.231], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22944/100000: episode: 940, duration: 0.242s, episode steps: 54, steps per second: 223, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.534, 1.081], mean_best_reward: --\n",
      " 22983/100000: episode: 941, duration: 0.177s, episode steps: 39, steps per second: 220, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.030 [-0.935, 1.393], mean_best_reward: --\n",
      " 22999/100000: episode: 942, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.096 [-1.396, 0.757], mean_best_reward: --\n",
      " 23012/100000: episode: 943, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.113 [-1.796, 1.011], mean_best_reward: --\n",
      " 23026/100000: episode: 944, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.305, 0.774], mean_best_reward: --\n",
      " 23038/100000: episode: 945, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.119 [-2.247, 1.337], mean_best_reward: --\n",
      " 23071/100000: episode: 946, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.046 [-1.752, 0.957], mean_best_reward: --\n",
      " 23084/100000: episode: 947, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-1.827, 0.980], mean_best_reward: --\n",
      " 23106/100000: episode: 948, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.092, 0.384], mean_best_reward: --\n",
      " 23147/100000: episode: 949, duration: 0.187s, episode steps: 41, steps per second: 219, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.089 [-0.858, 0.536], mean_best_reward: --\n",
      " 23158/100000: episode: 950, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.142 [-2.891, 1.786], mean_best_reward: --\n",
      " 23172/100000: episode: 951, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.103 [-2.165, 1.195], mean_best_reward: 92.000000\n",
      " 23202/100000: episode: 952, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.174, 0.584], mean_best_reward: --\n",
      " 23228/100000: episode: 953, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.074 [-0.580, 1.249], mean_best_reward: --\n",
      " 23248/100000: episode: 954, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.074 [-1.516, 2.315], mean_best_reward: --\n",
      " 23266/100000: episode: 955, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.093 [-1.517, 0.829], mean_best_reward: --\n",
      " 23305/100000: episode: 956, duration: 0.185s, episode steps: 39, steps per second: 210, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.113 [-0.610, 0.922], mean_best_reward: --\n",
      " 23324/100000: episode: 957, duration: 0.088s, episode steps: 19, steps per second: 215, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.078 [-2.313, 1.405], mean_best_reward: --\n",
      " 23335/100000: episode: 958, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.105 [-2.796, 1.795], mean_best_reward: --\n",
      " 23366/100000: episode: 959, duration: 0.146s, episode steps: 31, steps per second: 213, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.116 [-0.738, 1.486], mean_best_reward: --\n",
      " 23379/100000: episode: 960, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.089 [-1.023, 1.522], mean_best_reward: --\n",
      " 23397/100000: episode: 961, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.080 [-1.225, 2.080], mean_best_reward: --\n",
      " 23468/100000: episode: 962, duration: 0.399s, episode steps: 71, steps per second: 178, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.112 [-0.693, 1.103], mean_best_reward: --\n",
      " 23477/100000: episode: 963, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.153 [-2.284, 1.343], mean_best_reward: --\n",
      " 23559/100000: episode: 964, duration: 0.398s, episode steps: 82, steps per second: 206, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.045 [-0.811, 1.136], mean_best_reward: --\n",
      " 23623/100000: episode: 965, duration: 0.311s, episode steps: 64, steps per second: 206, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.765, 1.152], mean_best_reward: --\n",
      " 23650/100000: episode: 966, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: -0.010 [-2.292, 1.572], mean_best_reward: --\n",
      " 23720/100000: episode: 967, duration: 0.339s, episode steps: 70, steps per second: 207, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.029 [-1.161, 1.810], mean_best_reward: --\n",
      " 23746/100000: episode: 968, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.011 [-2.967, 2.176], mean_best_reward: --\n",
      " 23789/100000: episode: 969, duration: 0.210s, episode steps: 43, steps per second: 205, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.162 [-1.466, 0.982], mean_best_reward: --\n",
      " 23808/100000: episode: 970, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.115 [-0.398, 0.990], mean_best_reward: --\n",
      " 23891/100000: episode: 971, duration: 0.390s, episode steps: 83, steps per second: 213, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.058 [-0.763, 1.749], mean_best_reward: --\n",
      " 23903/100000: episode: 972, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.082 [-1.596, 2.413], mean_best_reward: --\n",
      " 23942/100000: episode: 973, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.069 [-0.644, 1.705], mean_best_reward: --\n",
      " 23954/100000: episode: 974, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.058, 1.211], mean_best_reward: --\n",
      " 23969/100000: episode: 975, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.094 [-2.345, 1.396], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23990/100000: episode: 976, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.080 [-0.582, 1.041], mean_best_reward: --\n",
      " 24054/100000: episode: 977, duration: 0.302s, episode steps: 64, steps per second: 212, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.136 [-0.906, 0.854], mean_best_reward: --\n",
      " 24108/100000: episode: 978, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.093 [-0.912, 0.813], mean_best_reward: --\n",
      " 24164/100000: episode: 979, duration: 0.257s, episode steps: 56, steps per second: 218, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.027 [-1.001, 0.733], mean_best_reward: --\n",
      " 24201/100000: episode: 980, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: 0.018 [-1.807, 1.326], mean_best_reward: --\n",
      " 24212/100000: episode: 981, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.159 [-2.386, 1.332], mean_best_reward: --\n",
      " 24229/100000: episode: 982, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.057 [-1.377, 2.110], mean_best_reward: --\n",
      " 24274/100000: episode: 983, duration: 0.211s, episode steps: 45, steps per second: 213, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.061 [-0.602, 1.192], mean_best_reward: --\n",
      " 24292/100000: episode: 984, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.623, 0.885], mean_best_reward: --\n",
      " 24318/100000: episode: 985, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.074 [-0.619, 1.197], mean_best_reward: --\n",
      " 24333/100000: episode: 986, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.077 [-1.382, 2.219], mean_best_reward: --\n",
      " 24345/100000: episode: 987, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.138 [-1.429, 0.749], mean_best_reward: --\n",
      " 24371/100000: episode: 988, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.029 [-0.646, 1.269], mean_best_reward: --\n",
      " 24396/100000: episode: 989, duration: 0.116s, episode steps: 25, steps per second: 215, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.059 [-2.305, 1.513], mean_best_reward: --\n",
      " 24445/100000: episode: 990, duration: 0.219s, episode steps: 49, steps per second: 223, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.048 [-0.639, 1.496], mean_best_reward: --\n",
      " 24480/100000: episode: 991, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.084 [-1.192, 0.492], mean_best_reward: --\n",
      " 24498/100000: episode: 992, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.064 [-1.545, 2.392], mean_best_reward: --\n",
      " 24516/100000: episode: 993, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.071 [-1.533, 2.472], mean_best_reward: --\n",
      " 24530/100000: episode: 994, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.098 [-2.226, 1.341], mean_best_reward: --\n",
      " 24543/100000: episode: 995, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.083 [-1.599, 0.986], mean_best_reward: --\n",
      " 24583/100000: episode: 996, duration: 0.188s, episode steps: 40, steps per second: 213, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.116 [-0.609, 1.050], mean_best_reward: --\n",
      " 24614/100000: episode: 997, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.105 [-0.599, 0.942], mean_best_reward: --\n",
      " 24672/100000: episode: 998, duration: 0.273s, episode steps: 58, steps per second: 212, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.483, 1.091], mean_best_reward: --\n",
      " 24695/100000: episode: 999, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.097 [-0.353, 0.798], mean_best_reward: --\n",
      " 24706/100000: episode: 1000, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-0.975, 1.790], mean_best_reward: --\n",
      " 24719/100000: episode: 1001, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.098 [-1.758, 2.736], mean_best_reward: 150.500000\n",
      " 24808/100000: episode: 1002, duration: 0.447s, episode steps: 89, steps per second: 199, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.126 [-1.042, 0.685], mean_best_reward: --\n",
      " 24878/100000: episode: 1003, duration: 0.324s, episode steps: 70, steps per second: 216, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.625, 1.021], mean_best_reward: --\n",
      " 24934/100000: episode: 1004, duration: 0.278s, episode steps: 56, steps per second: 201, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.052 [-0.738, 1.002], mean_best_reward: --\n",
      " 24978/100000: episode: 1005, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.153 [-0.436, 1.083], mean_best_reward: --\n",
      " 25035/100000: episode: 1006, duration: 0.290s, episode steps: 57, steps per second: 196, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.003 [-0.824, 0.973], mean_best_reward: --\n",
      " 25061/100000: episode: 1007, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.074 [-0.967, 1.645], mean_best_reward: --\n",
      " 25123/100000: episode: 1008, duration: 0.300s, episode steps: 62, steps per second: 206, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.529, 1.239], mean_best_reward: --\n",
      " 25161/100000: episode: 1009, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.120 [-0.952, 0.439], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25180/100000: episode: 1010, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.090 [-0.635, 1.098], mean_best_reward: --\n",
      " 25285/100000: episode: 1011, duration: 0.537s, episode steps: 105, steps per second: 195, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.218 [-2.104, 1.823], mean_best_reward: --\n",
      " 25299/100000: episode: 1012, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.561, 1.104], mean_best_reward: --\n",
      " 25312/100000: episode: 1013, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.103 [-0.579, 1.199], mean_best_reward: --\n",
      " 25399/100000: episode: 1014, duration: 0.438s, episode steps: 87, steps per second: 199, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.098 [-1.153, 0.824], mean_best_reward: --\n",
      " 25460/100000: episode: 1015, duration: 0.312s, episode steps: 61, steps per second: 195, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.079 [-1.042, 0.769], mean_best_reward: --\n",
      " 25481/100000: episode: 1016, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.104 [-0.567, 1.018], mean_best_reward: --\n",
      " 25502/100000: episode: 1017, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.066 [-1.344, 0.766], mean_best_reward: --\n",
      " 25540/100000: episode: 1018, duration: 0.185s, episode steps: 38, steps per second: 205, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.589, 0.872], mean_best_reward: --\n",
      " 25574/100000: episode: 1019, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.558, 1.139], mean_best_reward: --\n",
      " 25604/100000: episode: 1020, duration: 0.165s, episode steps: 30, steps per second: 181, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.078 [-0.609, 1.014], mean_best_reward: --\n",
      " 25665/100000: episode: 1021, duration: 0.276s, episode steps: 61, steps per second: 221, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.054 [-0.633, 1.053], mean_best_reward: --\n",
      " 25695/100000: episode: 1022, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-0.881, 0.384], mean_best_reward: --\n",
      " 25737/100000: episode: 1023, duration: 0.198s, episode steps: 42, steps per second: 212, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.182 [-1.512, 0.821], mean_best_reward: --\n",
      " 25831/100000: episode: 1024, duration: 0.445s, episode steps: 94, steps per second: 211, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.000 [-1.239, 0.988], mean_best_reward: --\n",
      " 25910/100000: episode: 1025, duration: 0.382s, episode steps: 79, steps per second: 207, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.157 [-0.879, 0.969], mean_best_reward: --\n",
      " 25933/100000: episode: 1026, duration: 0.109s, episode steps: 23, steps per second: 210, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.110 [-0.376, 0.968], mean_best_reward: --\n",
      " 26004/100000: episode: 1027, duration: 0.359s, episode steps: 71, steps per second: 198, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.006 [-0.634, 1.204], mean_best_reward: --\n",
      " 26098/100000: episode: 1028, duration: 0.468s, episode steps: 94, steps per second: 201, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.030 [-0.804, 1.012], mean_best_reward: --\n",
      " 26158/100000: episode: 1029, duration: 0.282s, episode steps: 60, steps per second: 213, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.098 [-1.621, 0.856], mean_best_reward: --\n",
      " 26183/100000: episode: 1030, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.083 [-0.641, 1.525], mean_best_reward: --\n",
      " 26206/100000: episode: 1031, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.077 [-0.800, 1.400], mean_best_reward: --\n",
      " 26274/100000: episode: 1032, duration: 0.336s, episode steps: 68, steps per second: 202, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.155 [-1.309, 0.497], mean_best_reward: --\n",
      " 26316/100000: episode: 1033, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.622, 1.159], mean_best_reward: --\n",
      " 26343/100000: episode: 1034, duration: 0.127s, episode steps: 27, steps per second: 213, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.040 [-0.775, 1.351], mean_best_reward: --\n",
      " 26368/100000: episode: 1035, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.113 [-0.375, 0.849], mean_best_reward: --\n",
      " 26417/100000: episode: 1036, duration: 0.227s, episode steps: 49, steps per second: 215, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.111 [-0.732, 1.907], mean_best_reward: --\n",
      " 26484/100000: episode: 1037, duration: 0.315s, episode steps: 67, steps per second: 213, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.204 [-0.505, 1.257], mean_best_reward: --\n",
      " 26532/100000: episode: 1038, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.096 [-1.053, 0.435], mean_best_reward: --\n",
      " 26584/100000: episode: 1039, duration: 0.254s, episode steps: 52, steps per second: 205, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.530, 1.024], mean_best_reward: --\n",
      " 26612/100000: episode: 1040, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.094 [-0.742, 0.417], mean_best_reward: --\n",
      " 26653/100000: episode: 1041, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.014 [-0.761, 0.975], mean_best_reward: --\n",
      " 26825/100000: episode: 1042, duration: 0.849s, episode steps: 172, steps per second: 202, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.058 [-0.620, 0.831], mean_best_reward: --\n",
      " 26851/100000: episode: 1043, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.123 [-0.866, 0.414], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26882/100000: episode: 1044, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.093 [-1.596, 0.779], mean_best_reward: --\n",
      " 26922/100000: episode: 1045, duration: 0.194s, episode steps: 40, steps per second: 207, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.528, 1.107], mean_best_reward: --\n",
      " 26951/100000: episode: 1046, duration: 0.137s, episode steps: 29, steps per second: 212, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.046 [-1.306, 0.806], mean_best_reward: --\n",
      " 27005/100000: episode: 1047, duration: 0.245s, episode steps: 54, steps per second: 220, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.156 [-0.432, 1.052], mean_best_reward: --\n",
      " 27019/100000: episode: 1048, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.098 [-0.820, 1.365], mean_best_reward: --\n",
      " 27070/100000: episode: 1049, duration: 0.232s, episode steps: 51, steps per second: 220, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.084 [-0.837, 0.741], mean_best_reward: --\n",
      " 27123/100000: episode: 1050, duration: 0.245s, episode steps: 53, steps per second: 216, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.130 [-0.347, 0.792], mean_best_reward: --\n",
      " 27272/100000: episode: 1051, duration: 0.657s, episode steps: 149, steps per second: 227, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.248 [-1.748, 1.347], mean_best_reward: 95.500000\n",
      " 27317/100000: episode: 1052, duration: 0.204s, episode steps: 45, steps per second: 221, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.124 [-0.543, 0.912], mean_best_reward: --\n",
      " 27356/100000: episode: 1053, duration: 0.181s, episode steps: 39, steps per second: 216, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.081 [-0.845, 0.606], mean_best_reward: --\n",
      " 27394/100000: episode: 1054, duration: 0.176s, episode steps: 38, steps per second: 216, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.423, 0.459], mean_best_reward: --\n",
      " 27422/100000: episode: 1055, duration: 0.129s, episode steps: 28, steps per second: 217, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.612, 0.882], mean_best_reward: --\n",
      " 27491/100000: episode: 1056, duration: 0.309s, episode steps: 69, steps per second: 223, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.046 [-0.943, 1.185], mean_best_reward: --\n",
      " 27523/100000: episode: 1057, duration: 0.146s, episode steps: 32, steps per second: 219, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.926, 0.355], mean_best_reward: --\n",
      " 27551/100000: episode: 1058, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.127 [-1.009, 0.581], mean_best_reward: --\n",
      " 27595/100000: episode: 1059, duration: 0.196s, episode steps: 44, steps per second: 224, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.129 [-0.980, 0.386], mean_best_reward: --\n",
      " 27664/100000: episode: 1060, duration: 0.321s, episode steps: 69, steps per second: 215, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.193 [-1.009, 1.285], mean_best_reward: --\n",
      " 27675/100000: episode: 1061, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.093 [-1.490, 1.001], mean_best_reward: --\n",
      " 27720/100000: episode: 1062, duration: 0.215s, episode steps: 45, steps per second: 209, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.105 [-0.543, 0.975], mean_best_reward: --\n",
      " 27765/100000: episode: 1063, duration: 0.212s, episode steps: 45, steps per second: 212, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: -0.006 [-2.107, 1.389], mean_best_reward: --\n",
      " 27794/100000: episode: 1064, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.029 [-0.837, 1.394], mean_best_reward: --\n",
      " 27819/100000: episode: 1065, duration: 0.125s, episode steps: 25, steps per second: 201, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.090 [-0.401, 0.875], mean_best_reward: --\n",
      " 27856/100000: episode: 1066, duration: 0.167s, episode steps: 37, steps per second: 222, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.062 [-0.927, 0.576], mean_best_reward: --\n",
      " 27878/100000: episode: 1067, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.073 [-1.471, 0.947], mean_best_reward: --\n",
      " 27913/100000: episode: 1068, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.085 [-0.415, 0.919], mean_best_reward: --\n",
      " 27989/100000: episode: 1069, duration: 0.353s, episode steps: 76, steps per second: 215, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.302, 0.933], mean_best_reward: --\n",
      " 28005/100000: episode: 1070, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.097 [-2.060, 1.184], mean_best_reward: --\n",
      " 28043/100000: episode: 1071, duration: 0.171s, episode steps: 38, steps per second: 222, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.109 [-0.353, 0.856], mean_best_reward: --\n",
      " 28071/100000: episode: 1072, duration: 0.129s, episode steps: 28, steps per second: 218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.092 [-0.613, 0.883], mean_best_reward: --\n",
      " 28125/100000: episode: 1073, duration: 0.243s, episode steps: 54, steps per second: 223, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.060 [-0.713, 1.481], mean_best_reward: --\n",
      " 28181/100000: episode: 1074, duration: 0.249s, episode steps: 56, steps per second: 225, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.102 [-0.827, 1.236], mean_best_reward: --\n",
      " 28211/100000: episode: 1075, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.089 [-0.918, 0.407], mean_best_reward: --\n",
      " 28257/100000: episode: 1076, duration: 0.209s, episode steps: 46, steps per second: 220, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.003, 0.417], mean_best_reward: --\n",
      " 28279/100000: episode: 1077, duration: 0.102s, episode steps: 22, steps per second: 215, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.548, 1.172], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28408/100000: episode: 1078, duration: 0.609s, episode steps: 129, steps per second: 212, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.027 [-1.541, 1.064], mean_best_reward: --\n",
      " 28418/100000: episode: 1079, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.114 [-1.688, 0.972], mean_best_reward: --\n",
      " 28509/100000: episode: 1080, duration: 0.409s, episode steps: 91, steps per second: 223, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.085 [-0.738, 0.907], mean_best_reward: --\n",
      " 28565/100000: episode: 1081, duration: 0.256s, episode steps: 56, steps per second: 219, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.522, 1.375], mean_best_reward: --\n",
      " 28626/100000: episode: 1082, duration: 0.282s, episode steps: 61, steps per second: 216, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.104 [-0.725, 0.583], mean_best_reward: --\n",
      " 28727/100000: episode: 1083, duration: 0.457s, episode steps: 101, steps per second: 221, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.028 [-0.618, 1.142], mean_best_reward: --\n",
      " 28798/100000: episode: 1084, duration: 0.322s, episode steps: 71, steps per second: 220, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.030 [-1.349, 0.584], mean_best_reward: --\n",
      " 28848/100000: episode: 1085, duration: 0.232s, episode steps: 50, steps per second: 216, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.110 [-0.740, 1.153], mean_best_reward: --\n",
      " 28890/100000: episode: 1086, duration: 0.193s, episode steps: 42, steps per second: 218, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.081 [-0.988, 0.690], mean_best_reward: --\n",
      " 28957/100000: episode: 1087, duration: 0.342s, episode steps: 67, steps per second: 196, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.078 [-0.375, 1.052], mean_best_reward: --\n",
      " 28984/100000: episode: 1088, duration: 0.186s, episode steps: 27, steps per second: 145, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.061 [-0.962, 1.763], mean_best_reward: --\n",
      " 29017/100000: episode: 1089, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.077 [-0.960, 0.581], mean_best_reward: --\n",
      " 29050/100000: episode: 1090, duration: 0.204s, episode steps: 33, steps per second: 162, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.090 [-2.100, 0.971], mean_best_reward: --\n",
      " 29074/100000: episode: 1091, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-1.047, 0.352], mean_best_reward: --\n",
      " 29118/100000: episode: 1092, duration: 0.210s, episode steps: 44, steps per second: 209, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.104 [-0.725, 0.391], mean_best_reward: --\n",
      " 29164/100000: episode: 1093, duration: 0.218s, episode steps: 46, steps per second: 211, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.183 [-1.104, 0.588], mean_best_reward: --\n",
      " 29212/100000: episode: 1094, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.293, 0.932], mean_best_reward: --\n",
      " 29245/100000: episode: 1095, duration: 0.209s, episode steps: 33, steps per second: 158, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.131 [-0.986, 0.620], mean_best_reward: --\n",
      " 29310/100000: episode: 1096, duration: 0.384s, episode steps: 65, steps per second: 169, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.144 [-0.909, 1.507], mean_best_reward: --\n",
      " 29354/100000: episode: 1097, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.066 [-0.779, 1.840], mean_best_reward: --\n",
      " 29424/100000: episode: 1098, duration: 0.314s, episode steps: 70, steps per second: 223, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.090 [-1.903, 0.738], mean_best_reward: --\n",
      " 29441/100000: episode: 1099, duration: 0.083s, episode steps: 17, steps per second: 206, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.092 [-1.651, 0.963], mean_best_reward: --\n",
      " 29468/100000: episode: 1100, duration: 0.123s, episode steps: 27, steps per second: 219, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.104 [-0.408, 1.115], mean_best_reward: --\n",
      " 29513/100000: episode: 1101, duration: 0.203s, episode steps: 45, steps per second: 222, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.087 [-0.407, 0.816], mean_best_reward: 97.500000\n",
      " 29556/100000: episode: 1102, duration: 0.196s, episode steps: 43, steps per second: 220, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.191 [-0.820, 1.277], mean_best_reward: --\n",
      " 29585/100000: episode: 1103, duration: 0.134s, episode steps: 29, steps per second: 217, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.110 [-0.352, 0.974], mean_best_reward: --\n",
      " 29605/100000: episode: 1104, duration: 0.093s, episode steps: 20, steps per second: 214, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.616, 0.962], mean_best_reward: --\n",
      " 29641/100000: episode: 1105, duration: 0.169s, episode steps: 36, steps per second: 212, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.018 [-1.193, 1.946], mean_best_reward: --\n",
      " 29674/100000: episode: 1106, duration: 0.149s, episode steps: 33, steps per second: 221, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.096 [-0.600, 1.363], mean_best_reward: --\n",
      " 29738/100000: episode: 1107, duration: 0.282s, episode steps: 64, steps per second: 227, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.214 [-1.142, 0.790], mean_best_reward: --\n",
      " 29775/100000: episode: 1108, duration: 0.166s, episode steps: 37, steps per second: 223, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.092 [-1.137, 0.771], mean_best_reward: --\n",
      " 29797/100000: episode: 1109, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.079 [-1.289, 0.772], mean_best_reward: --\n",
      " 29859/100000: episode: 1110, duration: 0.280s, episode steps: 62, steps per second: 221, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.206 [-1.354, 1.138], mean_best_reward: --\n",
      " 29910/100000: episode: 1111, duration: 0.230s, episode steps: 51, steps per second: 222, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.122 [-0.449, 0.827], mean_best_reward: --\n",
      " 29940/100000: episode: 1112, duration: 0.137s, episode steps: 30, steps per second: 219, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.417, 0.909], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29962/100000: episode: 1113, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-1.518, 0.976], mean_best_reward: --\n",
      " 30004/100000: episode: 1114, duration: 0.189s, episode steps: 42, steps per second: 222, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.155 [-0.956, 0.411], mean_best_reward: --\n",
      " 30071/100000: episode: 1115, duration: 0.304s, episode steps: 67, steps per second: 220, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.100 [-0.960, 0.351], mean_best_reward: --\n",
      " 30118/100000: episode: 1116, duration: 0.215s, episode steps: 47, steps per second: 218, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: 0.060 [-0.591, 1.520], mean_best_reward: --\n",
      " 30166/100000: episode: 1117, duration: 0.219s, episode steps: 48, steps per second: 220, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.055 [-0.573, 1.170], mean_best_reward: --\n",
      " 30239/100000: episode: 1118, duration: 0.326s, episode steps: 73, steps per second: 224, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.153 [-0.838, 1.336], mean_best_reward: --\n",
      " 30251/100000: episode: 1119, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.136, 0.600], mean_best_reward: --\n",
      " 30302/100000: episode: 1120, duration: 0.240s, episode steps: 51, steps per second: 213, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.134 [-1.118, 0.426], mean_best_reward: --\n",
      " 30335/100000: episode: 1121, duration: 0.153s, episode steps: 33, steps per second: 216, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.106 [-0.569, 1.281], mean_best_reward: --\n",
      " 30395/100000: episode: 1122, duration: 0.272s, episode steps: 60, steps per second: 221, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.135 [-1.085, 0.405], mean_best_reward: --\n",
      " 30432/100000: episode: 1123, duration: 0.187s, episode steps: 37, steps per second: 197, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.102 [-0.869, 0.608], mean_best_reward: --\n",
      " 30485/100000: episode: 1124, duration: 0.238s, episode steps: 53, steps per second: 223, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.108 [-0.323, 1.307], mean_best_reward: --\n",
      " 30517/100000: episode: 1125, duration: 0.152s, episode steps: 32, steps per second: 210, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.419, 0.972], mean_best_reward: --\n",
      " 30547/100000: episode: 1126, duration: 0.141s, episode steps: 30, steps per second: 212, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.020, 0.387], mean_best_reward: --\n",
      " 30559/100000: episode: 1127, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.137 [-0.931, 1.543], mean_best_reward: --\n",
      " 30596/100000: episode: 1128, duration: 0.216s, episode steps: 37, steps per second: 172, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.057 [-0.961, 1.669], mean_best_reward: --\n",
      " 30674/100000: episode: 1129, duration: 0.394s, episode steps: 78, steps per second: 198, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.143 [-1.133, 0.545], mean_best_reward: --\n",
      " 30726/100000: episode: 1130, duration: 0.242s, episode steps: 52, steps per second: 215, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.914, 0.864], mean_best_reward: --\n",
      " 30779/100000: episode: 1131, duration: 0.237s, episode steps: 53, steps per second: 224, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.161 [-1.158, 0.636], mean_best_reward: --\n",
      " 30822/100000: episode: 1132, duration: 0.192s, episode steps: 43, steps per second: 224, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.050 [-1.001, 0.626], mean_best_reward: --\n",
      " 30839/100000: episode: 1133, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.068 [-1.024, 1.782], mean_best_reward: --\n",
      " 30879/100000: episode: 1134, duration: 0.179s, episode steps: 40, steps per second: 224, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.119 [-0.878, 0.444], mean_best_reward: --\n",
      " 30905/100000: episode: 1135, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-0.993, 0.627], mean_best_reward: --\n",
      " 30943/100000: episode: 1136, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.089 [-0.939, 0.379], mean_best_reward: --\n",
      " 30995/100000: episode: 1137, duration: 0.302s, episode steps: 52, steps per second: 172, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.075 [-0.731, 1.107], mean_best_reward: --\n",
      " 31047/100000: episode: 1138, duration: 0.308s, episode steps: 52, steps per second: 169, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.103 [-0.958, 0.865], mean_best_reward: --\n",
      " 31094/100000: episode: 1139, duration: 0.232s, episode steps: 47, steps per second: 203, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.140 [-0.671, 1.320], mean_best_reward: --\n",
      " 31143/100000: episode: 1140, duration: 0.224s, episode steps: 49, steps per second: 219, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.108 [-1.100, 0.738], mean_best_reward: --\n",
      " 31173/100000: episode: 1141, duration: 0.136s, episode steps: 30, steps per second: 221, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.281, 0.376], mean_best_reward: --\n",
      " 31211/100000: episode: 1142, duration: 0.172s, episode steps: 38, steps per second: 221, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.028 [-0.768, 1.400], mean_best_reward: --\n",
      " 31273/100000: episode: 1143, duration: 0.277s, episode steps: 62, steps per second: 224, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.009 [-1.260, 0.643], mean_best_reward: --\n",
      " 31318/100000: episode: 1144, duration: 0.213s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.163 [-1.233, 0.548], mean_best_reward: --\n",
      " 31329/100000: episode: 1145, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.116 [-1.971, 1.172], mean_best_reward: --\n",
      " 31368/100000: episode: 1146, duration: 0.180s, episode steps: 39, steps per second: 217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.090 [-0.767, 1.263], mean_best_reward: --\n",
      " 31383/100000: episode: 1147, duration: 0.072s, episode steps: 15, steps per second: 210, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.080 [-2.222, 1.397], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31429/100000: episode: 1148, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-0.998, 0.637], mean_best_reward: --\n",
      " 31479/100000: episode: 1149, duration: 0.249s, episode steps: 50, steps per second: 201, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.109 [-0.538, 1.386], mean_best_reward: --\n",
      " 31501/100000: episode: 1150, duration: 0.101s, episode steps: 22, steps per second: 217, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.335, 0.572], mean_best_reward: --\n",
      " 31535/100000: episode: 1151, duration: 0.160s, episode steps: 34, steps per second: 212, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.142 [-0.713, 0.252], mean_best_reward: 173.500000\n",
      " 31617/100000: episode: 1152, duration: 0.366s, episode steps: 82, steps per second: 224, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.101 [-0.849, 0.854], mean_best_reward: --\n",
      " 31643/100000: episode: 1153, duration: 0.121s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.544, 0.925], mean_best_reward: --\n",
      " 31693/100000: episode: 1154, duration: 0.229s, episode steps: 50, steps per second: 219, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.109, 0.730], mean_best_reward: --\n",
      " 31735/100000: episode: 1155, duration: 0.189s, episode steps: 42, steps per second: 222, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.143 [-0.925, 0.603], mean_best_reward: --\n",
      " 31746/100000: episode: 1156, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.116 [-1.418, 0.792], mean_best_reward: --\n",
      " 31795/100000: episode: 1157, duration: 0.221s, episode steps: 49, steps per second: 221, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.078 [-1.011, 0.542], mean_best_reward: --\n",
      " 31836/100000: episode: 1158, duration: 0.183s, episode steps: 41, steps per second: 224, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.102 [-0.516, 1.218], mean_best_reward: --\n",
      " 31866/100000: episode: 1159, duration: 0.138s, episode steps: 30, steps per second: 218, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.086 [-0.897, 0.542], mean_best_reward: --\n",
      " 31888/100000: episode: 1160, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.072 [-1.541, 0.995], mean_best_reward: --\n",
      " 31913/100000: episode: 1161, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.094 [-0.248, 1.078], mean_best_reward: --\n",
      " 31941/100000: episode: 1162, duration: 0.127s, episode steps: 28, steps per second: 220, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.075 [-1.354, 0.465], mean_best_reward: --\n",
      " 31978/100000: episode: 1163, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.132 [-1.169, 0.554], mean_best_reward: --\n",
      " 31999/100000: episode: 1164, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.078 [-1.617, 0.839], mean_best_reward: --\n",
      " 32044/100000: episode: 1165, duration: 0.206s, episode steps: 45, steps per second: 219, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.113 [-0.467, 1.061], mean_best_reward: --\n",
      " 32086/100000: episode: 1166, duration: 0.194s, episode steps: 42, steps per second: 216, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.079 [-0.591, 0.867], mean_best_reward: --\n",
      " 32132/100000: episode: 1167, duration: 0.212s, episode steps: 46, steps per second: 217, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.099 [-0.459, 1.536], mean_best_reward: --\n",
      " 32158/100000: episode: 1168, duration: 0.120s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.431, 1.175], mean_best_reward: --\n",
      " 32197/100000: episode: 1169, duration: 0.188s, episode steps: 39, steps per second: 208, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.099 [-0.425, 0.981], mean_best_reward: --\n",
      " 32231/100000: episode: 1170, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.386, 0.937], mean_best_reward: --\n",
      " 32285/100000: episode: 1171, duration: 0.255s, episode steps: 54, steps per second: 212, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.011 [-1.511, 0.807], mean_best_reward: --\n",
      " 32297/100000: episode: 1172, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.126 [-1.007, 1.592], mean_best_reward: --\n",
      " 32346/100000: episode: 1173, duration: 0.243s, episode steps: 49, steps per second: 201, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.092 [-0.473, 1.470], mean_best_reward: --\n",
      " 32367/100000: episode: 1174, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.070 [-0.799, 1.346], mean_best_reward: --\n",
      " 32390/100000: episode: 1175, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.097 [-0.822, 1.729], mean_best_reward: --\n",
      " 32449/100000: episode: 1176, duration: 0.282s, episode steps: 59, steps per second: 209, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.072 [-0.427, 1.186], mean_best_reward: --\n",
      " 32495/100000: episode: 1177, duration: 0.214s, episode steps: 46, steps per second: 215, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.135 [-0.386, 1.084], mean_best_reward: --\n",
      " 32516/100000: episode: 1178, duration: 0.114s, episode steps: 21, steps per second: 183, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.110 [-1.259, 0.817], mean_best_reward: --\n",
      " 32550/100000: episode: 1179, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-1.243, 0.358], mean_best_reward: --\n",
      " 32587/100000: episode: 1180, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.068 [-1.004, 2.083], mean_best_reward: --\n",
      " 32643/100000: episode: 1181, duration: 0.267s, episode steps: 56, steps per second: 210, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.054 [-0.547, 0.886], mean_best_reward: --\n",
      " 32653/100000: episode: 1182, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.120 [-1.845, 1.187], mean_best_reward: --\n",
      " 32676/100000: episode: 1183, duration: 0.107s, episode steps: 23, steps per second: 215, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.104 [-1.179, 0.395], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32724/100000: episode: 1184, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.826, 1.131], mean_best_reward: --\n",
      " 32762/100000: episode: 1185, duration: 0.175s, episode steps: 38, steps per second: 217, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.042 [-0.602, 1.300], mean_best_reward: --\n",
      " 32815/100000: episode: 1186, duration: 0.255s, episode steps: 53, steps per second: 208, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.175 [-0.416, 1.099], mean_best_reward: --\n",
      " 32860/100000: episode: 1187, duration: 0.205s, episode steps: 45, steps per second: 220, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.090 [-0.491, 1.191], mean_best_reward: --\n",
      " 32905/100000: episode: 1188, duration: 0.210s, episode steps: 45, steps per second: 215, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.051 [-0.433, 0.923], mean_best_reward: --\n",
      " 33100/100000: episode: 1189, duration: 0.884s, episode steps: 195, steps per second: 221, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.366 [-2.445, 1.395], mean_best_reward: --\n",
      " 33113/100000: episode: 1190, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.086 [-1.963, 1.221], mean_best_reward: --\n",
      " 33146/100000: episode: 1191, duration: 0.151s, episode steps: 33, steps per second: 218, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.069 [-1.988, 1.011], mean_best_reward: --\n",
      " 33172/100000: episode: 1192, duration: 0.118s, episode steps: 26, steps per second: 220, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.114, 0.586], mean_best_reward: --\n",
      " 33238/100000: episode: 1193, duration: 0.310s, episode steps: 66, steps per second: 213, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.068 [-0.457, 1.055], mean_best_reward: --\n",
      " 33286/100000: episode: 1194, duration: 0.217s, episode steps: 48, steps per second: 222, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.583, 1.067], mean_best_reward: --\n",
      " 33360/100000: episode: 1195, duration: 0.328s, episode steps: 74, steps per second: 226, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.076 [-1.170, 0.808], mean_best_reward: --\n",
      " 33421/100000: episode: 1196, duration: 0.274s, episode steps: 61, steps per second: 222, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.031 [-0.785, 1.091], mean_best_reward: --\n",
      " 33471/100000: episode: 1197, duration: 0.230s, episode steps: 50, steps per second: 217, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.148 [-0.593, 0.902], mean_best_reward: --\n",
      " 33505/100000: episode: 1198, duration: 0.153s, episode steps: 34, steps per second: 222, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.088 [-0.636, 0.986], mean_best_reward: --\n",
      " 33524/100000: episode: 1199, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.080 [-0.955, 1.741], mean_best_reward: --\n",
      " 33581/100000: episode: 1200, duration: 0.254s, episode steps: 57, steps per second: 225, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.003 [-0.896, 0.839], mean_best_reward: --\n",
      " 33663/100000: episode: 1201, duration: 0.369s, episode steps: 82, steps per second: 222, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.140 [-0.728, 1.146], mean_best_reward: 124.000000\n",
      " 33726/100000: episode: 1202, duration: 0.280s, episode steps: 63, steps per second: 225, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.032 [-0.534, 1.033], mean_best_reward: --\n",
      " 33777/100000: episode: 1203, duration: 0.230s, episode steps: 51, steps per second: 222, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.106 [-0.562, 0.859], mean_best_reward: --\n",
      " 33792/100000: episode: 1204, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.169, 1.994], mean_best_reward: --\n",
      " 33874/100000: episode: 1205, duration: 0.362s, episode steps: 82, steps per second: 227, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.875, 1.346], mean_best_reward: --\n",
      " 33911/100000: episode: 1206, duration: 0.172s, episode steps: 37, steps per second: 215, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.092 [-0.432, 0.883], mean_best_reward: --\n",
      " 33933/100000: episode: 1207, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.295, 0.819], mean_best_reward: --\n",
      " 33983/100000: episode: 1208, duration: 0.223s, episode steps: 50, steps per second: 224, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.081 [-0.534, 0.923], mean_best_reward: --\n",
      " 34017/100000: episode: 1209, duration: 0.157s, episode steps: 34, steps per second: 217, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.073 [-0.624, 1.419], mean_best_reward: --\n",
      " 34045/100000: episode: 1210, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.042 [-1.007, 1.653], mean_best_reward: --\n",
      " 34060/100000: episode: 1211, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.134 [-0.559, 1.377], mean_best_reward: --\n",
      " 34084/100000: episode: 1212, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.094 [-0.608, 1.449], mean_best_reward: --\n",
      " 34111/100000: episode: 1213, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.088 [-0.387, 1.190], mean_best_reward: --\n",
      " 34132/100000: episode: 1214, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.098 [-0.667, 1.591], mean_best_reward: --\n",
      " 34166/100000: episode: 1215, duration: 0.158s, episode steps: 34, steps per second: 215, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.109 [-0.435, 1.160], mean_best_reward: --\n",
      " 34203/100000: episode: 1216, duration: 0.169s, episode steps: 37, steps per second: 219, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.125 [-1.806, 0.756], mean_best_reward: --\n",
      " 34217/100000: episode: 1217, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.642, 1.180], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34284/100000: episode: 1218, duration: 0.310s, episode steps: 67, steps per second: 216, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.123 [-0.677, 0.753], mean_best_reward: --\n",
      " 34309/100000: episode: 1219, duration: 0.122s, episode steps: 25, steps per second: 204, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.033 [-1.293, 0.999], mean_best_reward: --\n",
      " 34321/100000: episode: 1220, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.107 [-0.771, 1.426], mean_best_reward: --\n",
      " 34349/100000: episode: 1221, duration: 0.134s, episode steps: 28, steps per second: 210, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.605, 1.070], mean_best_reward: --\n",
      " 34391/100000: episode: 1222, duration: 0.192s, episode steps: 42, steps per second: 219, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.123, 0.633], mean_best_reward: --\n",
      " 34408/100000: episode: 1223, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.104 [-0.793, 1.538], mean_best_reward: --\n",
      " 34444/100000: episode: 1224, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.071, 0.566], mean_best_reward: --\n",
      " 34476/100000: episode: 1225, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.769, 1.143], mean_best_reward: --\n",
      " 34491/100000: episode: 1226, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.075 [-1.320, 0.837], mean_best_reward: --\n",
      " 34535/100000: episode: 1227, duration: 0.208s, episode steps: 44, steps per second: 211, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.118 [-0.343, 0.982], mean_best_reward: --\n",
      " 34592/100000: episode: 1228, duration: 0.260s, episode steps: 57, steps per second: 219, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.040 [-0.647, 0.867], mean_best_reward: --\n",
      " 34673/100000: episode: 1229, duration: 0.375s, episode steps: 81, steps per second: 216, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.000 [-0.652, 1.311], mean_best_reward: --\n",
      " 34788/100000: episode: 1230, duration: 0.511s, episode steps: 115, steps per second: 225, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.011 [-0.669, 1.501], mean_best_reward: --\n",
      " 34911/100000: episode: 1231, duration: 0.537s, episode steps: 123, steps per second: 229, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.138 [-1.045, 0.773], mean_best_reward: --\n",
      " 34961/100000: episode: 1232, duration: 0.231s, episode steps: 50, steps per second: 216, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.260 [-0.559, 1.503], mean_best_reward: --\n",
      " 35056/100000: episode: 1233, duration: 0.418s, episode steps: 95, steps per second: 227, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.006 [-1.035, 0.639], mean_best_reward: --\n",
      " 35103/100000: episode: 1234, duration: 0.213s, episode steps: 47, steps per second: 220, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.147 [-0.933, 0.592], mean_best_reward: --\n",
      " 35112/100000: episode: 1235, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.380, 2.270], mean_best_reward: --\n",
      " 35132/100000: episode: 1236, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.104 [-2.215, 1.161], mean_best_reward: --\n",
      " 35156/100000: episode: 1237, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.067 [-0.595, 1.154], mean_best_reward: --\n",
      " 35195/100000: episode: 1238, duration: 0.180s, episode steps: 39, steps per second: 217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.101 [-0.545, 0.922], mean_best_reward: --\n",
      " 35226/100000: episode: 1239, duration: 0.147s, episode steps: 31, steps per second: 211, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.104 [-1.168, 0.675], mean_best_reward: --\n",
      " 35246/100000: episode: 1240, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.079 [-1.164, 0.438], mean_best_reward: --\n",
      " 35277/100000: episode: 1241, duration: 0.144s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.074 [-0.355, 0.943], mean_best_reward: --\n",
      " 35376/100000: episode: 1242, duration: 0.439s, episode steps: 99, steps per second: 226, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.083 [-1.055, 0.779], mean_best_reward: --\n",
      " 35464/100000: episode: 1243, duration: 0.393s, episode steps: 88, steps per second: 224, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.030 [-0.777, 0.974], mean_best_reward: --\n",
      " 35482/100000: episode: 1244, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.065 [-1.769, 1.012], mean_best_reward: --\n",
      " 35518/100000: episode: 1245, duration: 0.162s, episode steps: 36, steps per second: 222, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.139 [-0.376, 0.711], mean_best_reward: --\n",
      " 35559/100000: episode: 1246, duration: 0.185s, episode steps: 41, steps per second: 222, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.013 [-0.934, 1.351], mean_best_reward: --\n",
      " 35607/100000: episode: 1247, duration: 0.215s, episode steps: 48, steps per second: 224, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.151 [-0.749, 0.276], mean_best_reward: --\n",
      " 35658/100000: episode: 1248, duration: 0.234s, episode steps: 51, steps per second: 218, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.112 [-0.439, 1.130], mean_best_reward: --\n",
      " 35679/100000: episode: 1249, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.104 [-0.927, 0.418], mean_best_reward: --\n",
      " 35727/100000: episode: 1250, duration: 0.214s, episode steps: 48, steps per second: 224, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.051 [-1.172, 1.467], mean_best_reward: --\n",
      " 35743/100000: episode: 1251, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.068 [-1.700, 1.011], mean_best_reward: 143.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35782/100000: episode: 1252, duration: 0.264s, episode steps: 39, steps per second: 147, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.110 [-0.409, 0.980], mean_best_reward: --\n",
      " 35824/100000: episode: 1253, duration: 0.258s, episode steps: 42, steps per second: 163, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.372, 1.102], mean_best_reward: --\n",
      " 35850/100000: episode: 1254, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.636, 0.965], mean_best_reward: --\n",
      " 35899/100000: episode: 1255, duration: 0.314s, episode steps: 49, steps per second: 156, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.112 [-0.427, 0.889], mean_best_reward: --\n",
      " 35953/100000: episode: 1256, duration: 0.334s, episode steps: 54, steps per second: 162, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.035 [-0.890, 1.104], mean_best_reward: --\n",
      " 35979/100000: episode: 1257, duration: 0.306s, episode steps: 26, steps per second: 85, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.113 [-1.112, 0.561], mean_best_reward: --\n",
      " 36004/100000: episode: 1258, duration: 0.171s, episode steps: 25, steps per second: 146, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.143 [-1.253, 0.456], mean_best_reward: --\n",
      " 36086/100000: episode: 1259, duration: 0.505s, episode steps: 82, steps per second: 162, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.100 [-1.195, 1.334], mean_best_reward: --\n",
      " 36102/100000: episode: 1260, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.119 [-1.072, 0.616], mean_best_reward: --\n",
      " 36125/100000: episode: 1261, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.080 [-0.936, 1.543], mean_best_reward: --\n",
      " 36175/100000: episode: 1262, duration: 0.248s, episode steps: 50, steps per second: 202, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.120 [-0.757, 0.355], mean_best_reward: --\n",
      " 36254/100000: episode: 1263, duration: 0.383s, episode steps: 79, steps per second: 206, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.082 [-1.296, 0.729], mean_best_reward: --\n",
      " 36348/100000: episode: 1264, duration: 0.470s, episode steps: 94, steps per second: 200, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-1.264, 0.996], mean_best_reward: --\n",
      " 36404/100000: episode: 1265, duration: 0.285s, episode steps: 56, steps per second: 196, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.109 [-1.123, 0.791], mean_best_reward: --\n",
      " 36439/100000: episode: 1266, duration: 0.303s, episode steps: 35, steps per second: 116, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.094 [-0.581, 0.908], mean_best_reward: --\n",
      " 36467/100000: episode: 1267, duration: 0.178s, episode steps: 28, steps per second: 157, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.148 [-0.570, 1.143], mean_best_reward: --\n",
      " 36506/100000: episode: 1268, duration: 0.312s, episode steps: 39, steps per second: 125, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.067 [-1.376, 1.016], mean_best_reward: --\n",
      " 36529/100000: episode: 1269, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.077 [-0.754, 1.499], mean_best_reward: --\n",
      " 36557/100000: episode: 1270, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.095 [-0.375, 0.921], mean_best_reward: --\n",
      " 36579/100000: episode: 1271, duration: 0.173s, episode steps: 22, steps per second: 127, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.084 [-1.196, 0.600], mean_best_reward: --\n",
      " 36621/100000: episode: 1272, duration: 0.256s, episode steps: 42, steps per second: 164, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.078 [-2.072, 1.140], mean_best_reward: --\n",
      " 36643/100000: episode: 1273, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.133 [-0.599, 1.045], mean_best_reward: --\n",
      " 36655/100000: episode: 1274, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.129 [-0.749, 1.397], mean_best_reward: --\n",
      " 36746/100000: episode: 1275, duration: 0.542s, episode steps: 91, steps per second: 168, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.057 [-0.570, 1.277], mean_best_reward: --\n",
      " 36861/100000: episode: 1276, duration: 0.615s, episode steps: 115, steps per second: 187, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.157 [-0.867, 1.178], mean_best_reward: --\n",
      " 36952/100000: episode: 1277, duration: 0.521s, episode steps: 91, steps per second: 175, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.004 [-1.436, 0.930], mean_best_reward: --\n",
      " 36979/100000: episode: 1278, duration: 0.185s, episode steps: 27, steps per second: 146, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.084 [-1.685, 0.636], mean_best_reward: --\n",
      " 37029/100000: episode: 1279, duration: 0.304s, episode steps: 50, steps per second: 164, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.092 [-1.610, 0.485], mean_best_reward: --\n",
      " 37154/100000: episode: 1280, duration: 0.721s, episode steps: 125, steps per second: 173, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.375 [-2.777, 1.539], mean_best_reward: --\n",
      " 37175/100000: episode: 1281, duration: 0.189s, episode steps: 21, steps per second: 111, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.077 [-0.631, 0.918], mean_best_reward: --\n",
      " 37206/100000: episode: 1282, duration: 0.159s, episode steps: 31, steps per second: 194, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.090 [-0.446, 1.230], mean_best_reward: --\n",
      " 37223/100000: episode: 1283, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.116 [-0.752, 1.199], mean_best_reward: --\n",
      " 37267/100000: episode: 1284, duration: 0.311s, episode steps: 44, steps per second: 142, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.105 [-0.544, 0.918], mean_best_reward: --\n",
      " 37286/100000: episode: 1285, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.101 [-0.825, 1.323], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37304/100000: episode: 1286, duration: 0.152s, episode steps: 18, steps per second: 119, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.120 [-1.254, 0.418], mean_best_reward: --\n",
      " 37330/100000: episode: 1287, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-2.266, 1.151], mean_best_reward: --\n",
      " 37370/100000: episode: 1288, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.141 [-0.815, 0.449], mean_best_reward: --\n",
      " 37396/100000: episode: 1289, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.602, 1.307], mean_best_reward: --\n",
      " 37416/100000: episode: 1290, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.094 [-1.386, 0.598], mean_best_reward: --\n",
      " 37486/100000: episode: 1291, duration: 0.481s, episode steps: 70, steps per second: 146, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.059 [-0.948, 0.742], mean_best_reward: --\n",
      " 37548/100000: episode: 1292, duration: 0.410s, episode steps: 62, steps per second: 151, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.034 [-1.022, 0.772], mean_best_reward: --\n",
      " 37595/100000: episode: 1293, duration: 0.303s, episode steps: 47, steps per second: 155, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.071 [-1.432, 0.603], mean_best_reward: --\n",
      " 37635/100000: episode: 1294, duration: 0.252s, episode steps: 40, steps per second: 159, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-1.329, 0.936], mean_best_reward: --\n",
      " 37672/100000: episode: 1295, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.108 [-1.341, 0.369], mean_best_reward: --\n",
      " 37712/100000: episode: 1296, duration: 0.236s, episode steps: 40, steps per second: 169, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.154 [-1.025, 0.683], mean_best_reward: --\n",
      " 37730/100000: episode: 1297, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.056 [-1.701, 1.185], mean_best_reward: --\n",
      " 37742/100000: episode: 1298, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.100 [-1.697, 1.026], mean_best_reward: --\n",
      " 37800/100000: episode: 1299, duration: 0.374s, episode steps: 58, steps per second: 155, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.185 [-0.543, 0.970], mean_best_reward: --\n",
      " 37844/100000: episode: 1300, duration: 0.488s, episode steps: 44, steps per second: 90, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.163 [-0.958, 0.361], mean_best_reward: --\n",
      " 37877/100000: episode: 1301, duration: 0.238s, episode steps: 33, steps per second: 138, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.106 [-0.500, 0.978], mean_best_reward: 84.000000\n",
      " 37896/100000: episode: 1302, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.065 [-1.665, 0.981], mean_best_reward: --\n",
      " 37945/100000: episode: 1303, duration: 0.236s, episode steps: 49, steps per second: 207, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.114 [-0.448, 0.980], mean_best_reward: --\n",
      " 37983/100000: episode: 1304, duration: 0.184s, episode steps: 38, steps per second: 207, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.062 [-1.361, 0.571], mean_best_reward: --\n",
      " 38061/100000: episode: 1305, duration: 0.374s, episode steps: 78, steps per second: 208, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.256 [-1.332, 0.827], mean_best_reward: --\n",
      " 38122/100000: episode: 1306, duration: 0.304s, episode steps: 61, steps per second: 200, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.019 [-0.596, 1.324], mean_best_reward: --\n",
      " 38149/100000: episode: 1307, duration: 0.130s, episode steps: 27, steps per second: 207, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.118 [-1.239, 0.615], mean_best_reward: --\n",
      " 38175/100000: episode: 1308, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.072 [-0.759, 1.387], mean_best_reward: --\n",
      " 38261/100000: episode: 1309, duration: 0.421s, episode steps: 86, steps per second: 204, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.098 [-0.837, 0.949], mean_best_reward: --\n",
      " 38305/100000: episode: 1310, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.346, 0.908], mean_best_reward: --\n",
      " 38335/100000: episode: 1311, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.093 [-0.754, 1.429], mean_best_reward: --\n",
      " 38388/100000: episode: 1312, duration: 0.255s, episode steps: 53, steps per second: 208, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.140 [-0.525, 1.388], mean_best_reward: --\n",
      " 38424/100000: episode: 1313, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.390, 1.211], mean_best_reward: --\n",
      " 38462/100000: episode: 1314, duration: 0.184s, episode steps: 38, steps per second: 207, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.120 [-0.456, 0.825], mean_best_reward: --\n",
      " 38492/100000: episode: 1315, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.100 [-0.384, 0.920], mean_best_reward: --\n",
      " 38543/100000: episode: 1316, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.095 [-1.188, 0.622], mean_best_reward: --\n",
      " 38567/100000: episode: 1317, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.574, 0.995], mean_best_reward: --\n",
      " 38599/100000: episode: 1318, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.079 [-0.578, 1.033], mean_best_reward: --\n",
      " 38662/100000: episode: 1319, duration: 0.307s, episode steps: 63, steps per second: 205, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.097 [-0.601, 0.748], mean_best_reward: --\n",
      " 38696/100000: episode: 1320, duration: 0.166s, episode steps: 34, steps per second: 204, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.059, 0.397], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38730/100000: episode: 1321, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.122 [-0.736, 1.244], mean_best_reward: --\n",
      " 38769/100000: episode: 1322, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.093 [-0.381, 1.260], mean_best_reward: --\n",
      " 38835/100000: episode: 1323, duration: 0.328s, episode steps: 66, steps per second: 201, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.081 [-0.625, 1.169], mean_best_reward: --\n",
      " 38849/100000: episode: 1324, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.112 [-1.609, 0.766], mean_best_reward: --\n",
      " 38870/100000: episode: 1325, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.936, 1.414], mean_best_reward: --\n",
      " 38933/100000: episode: 1326, duration: 0.319s, episode steps: 63, steps per second: 198, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.067 [-0.578, 1.146], mean_best_reward: --\n",
      " 38953/100000: episode: 1327, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.650, 1.273], mean_best_reward: --\n",
      " 38966/100000: episode: 1328, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-1.153, 1.818], mean_best_reward: --\n",
      " 39017/100000: episode: 1329, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.061 [-0.424, 0.824], mean_best_reward: --\n",
      " 39061/100000: episode: 1330, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.099 [-0.367, 0.872], mean_best_reward: --\n",
      " 39096/100000: episode: 1331, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.107 [-0.595, 0.896], mean_best_reward: --\n",
      " 39138/100000: episode: 1332, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.139 [-0.383, 1.096], mean_best_reward: --\n",
      " 39214/100000: episode: 1333, duration: 0.368s, episode steps: 76, steps per second: 207, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.096 [-0.757, 1.079], mean_best_reward: --\n",
      " 39249/100000: episode: 1334, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.078 [-0.813, 1.481], mean_best_reward: --\n",
      " 39267/100000: episode: 1335, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.619, 1.243], mean_best_reward: --\n",
      " 39303/100000: episode: 1336, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.130 [-0.312, 0.860], mean_best_reward: --\n",
      " 39351/100000: episode: 1337, duration: 0.233s, episode steps: 48, steps per second: 206, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.121 [-1.129, 0.385], mean_best_reward: --\n",
      " 39377/100000: episode: 1338, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.046 [-0.618, 1.303], mean_best_reward: --\n",
      " 39401/100000: episode: 1339, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.106 [-0.552, 1.057], mean_best_reward: --\n",
      " 39439/100000: episode: 1340, duration: 0.182s, episode steps: 38, steps per second: 209, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.005 [-0.775, 1.308], mean_best_reward: --\n",
      " 39540/100000: episode: 1341, duration: 0.492s, episode steps: 101, steps per second: 205, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.124 [-0.601, 1.201], mean_best_reward: --\n",
      " 39564/100000: episode: 1342, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.736, 1.313], mean_best_reward: --\n",
      " 39593/100000: episode: 1343, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.094 [-0.786, 1.311], mean_best_reward: --\n",
      " 39610/100000: episode: 1344, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.078 [-1.034, 1.744], mean_best_reward: --\n",
      " 39628/100000: episode: 1345, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.774, 1.374], mean_best_reward: --\n",
      " 39718/100000: episode: 1346, duration: 0.431s, episode steps: 90, steps per second: 209, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.040 [-0.559, 0.820], mean_best_reward: --\n",
      " 39767/100000: episode: 1347, duration: 0.239s, episode steps: 49, steps per second: 205, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.088 [-0.589, 1.756], mean_best_reward: --\n",
      " 39788/100000: episode: 1348, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.061 [-0.800, 1.172], mean_best_reward: --\n",
      " 39832/100000: episode: 1349, duration: 0.212s, episode steps: 44, steps per second: 208, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.152 [-0.379, 0.916], mean_best_reward: --\n",
      " 39866/100000: episode: 1350, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.117 [-0.411, 0.889], mean_best_reward: --\n",
      " 39910/100000: episode: 1351, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.183 [-0.949, 0.347], mean_best_reward: 86.500000\n",
      " 39929/100000: episode: 1352, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.075 [-0.739, 1.327], mean_best_reward: --\n",
      " 39944/100000: episode: 1353, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.071 [-0.996, 1.581], mean_best_reward: --\n",
      " 39995/100000: episode: 1354, duration: 0.244s, episode steps: 51, steps per second: 209, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.185 [-1.655, 1.157], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40048/100000: episode: 1355, duration: 0.254s, episode steps: 53, steps per second: 209, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.131 [-1.103, 0.560], mean_best_reward: --\n",
      " 40064/100000: episode: 1356, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.930, 1.456], mean_best_reward: --\n",
      " 40096/100000: episode: 1357, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.228, 0.541], mean_best_reward: --\n",
      " 40111/100000: episode: 1358, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.057 [-2.690, 1.804], mean_best_reward: --\n",
      " 40121/100000: episode: 1359, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.023, 1.189], mean_best_reward: --\n",
      " 40134/100000: episode: 1360, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.249, 1.338], mean_best_reward: --\n",
      " 40161/100000: episode: 1361, duration: 0.130s, episode steps: 27, steps per second: 207, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: -0.026 [-2.301, 1.415], mean_best_reward: --\n",
      " 40228/100000: episode: 1362, duration: 0.319s, episode steps: 67, steps per second: 210, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.297 [-1.700, 0.483], mean_best_reward: --\n",
      " 40245/100000: episode: 1363, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.079 [-1.798, 1.031], mean_best_reward: --\n",
      " 40255/100000: episode: 1364, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.548, 2.555], mean_best_reward: --\n",
      " 40264/100000: episode: 1365, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.291, 1.391], mean_best_reward: --\n",
      " 40281/100000: episode: 1366, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.077 [-1.203, 1.886], mean_best_reward: --\n",
      " 40297/100000: episode: 1367, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.090 [-2.015, 1.192], mean_best_reward: --\n",
      " 40308/100000: episode: 1368, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.093 [-2.226, 1.399], mean_best_reward: --\n",
      " 40341/100000: episode: 1369, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.007 [-1.878, 1.496], mean_best_reward: --\n",
      " 40360/100000: episode: 1370, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.083 [-2.293, 1.387], mean_best_reward: --\n",
      " 40376/100000: episode: 1371, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.373, 0.788], mean_best_reward: --\n",
      " 40399/100000: episode: 1372, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.105 [-0.381, 0.839], mean_best_reward: --\n",
      " 40420/100000: episode: 1373, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.088 [-0.616, 1.027], mean_best_reward: --\n",
      " 40437/100000: episode: 1374, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.101 [-1.009, 0.576], mean_best_reward: --\n",
      " 40464/100000: episode: 1375, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.051 [-1.700, 0.958], mean_best_reward: --\n",
      " 40480/100000: episode: 1376, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.999, 1.431], mean_best_reward: --\n",
      " 40499/100000: episode: 1377, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.065 [-0.796, 1.275], mean_best_reward: --\n",
      " 40515/100000: episode: 1378, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.092 [-1.932, 1.135], mean_best_reward: --\n",
      " 40532/100000: episode: 1379, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.059 [-2.664, 1.744], mean_best_reward: --\n",
      " 40548/100000: episode: 1380, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.633, 1.126], mean_best_reward: --\n",
      " 40571/100000: episode: 1381, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.074 [-0.815, 1.320], mean_best_reward: --\n",
      " 40594/100000: episode: 1382, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.050 [-0.764, 1.391], mean_best_reward: --\n",
      " 40605/100000: episode: 1383, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.113 [-1.195, 2.006], mean_best_reward: --\n",
      " 40617/100000: episode: 1384, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.084 [-1.002, 1.656], mean_best_reward: --\n",
      " 40638/100000: episode: 1385, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.285, 0.744], mean_best_reward: --\n",
      " 40653/100000: episode: 1386, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.113 [-2.363, 1.340], mean_best_reward: --\n",
      " 40678/100000: episode: 1387, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.028 [-1.783, 2.517], mean_best_reward: --\n",
      " 40688/100000: episode: 1388, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.112 [-1.029, 1.698], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40717/100000: episode: 1389, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.099 [-1.238, 0.396], mean_best_reward: --\n",
      " 40732/100000: episode: 1390, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.111 [-0.817, 1.532], mean_best_reward: --\n",
      " 40751/100000: episode: 1391, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.048 [-1.209, 1.735], mean_best_reward: --\n",
      " 40767/100000: episode: 1392, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.117 [-0.784, 1.618], mean_best_reward: --\n",
      " 40786/100000: episode: 1393, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.064 [-1.633, 1.007], mean_best_reward: --\n",
      " 40803/100000: episode: 1394, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.122 [-0.955, 1.894], mean_best_reward: --\n",
      " 40820/100000: episode: 1395, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.091 [-1.056, 0.551], mean_best_reward: --\n",
      " 40829/100000: episode: 1396, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.745, 2.824], mean_best_reward: --\n",
      " 40875/100000: episode: 1397, duration: 0.234s, episode steps: 46, steps per second: 196, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.019 [-0.641, 1.279], mean_best_reward: --\n",
      " 40896/100000: episode: 1398, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.112 [-0.858, 0.411], mean_best_reward: --\n",
      " 40908/100000: episode: 1399, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-1.128, 1.854], mean_best_reward: --\n",
      " 40924/100000: episode: 1400, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.107 [-1.913, 3.109], mean_best_reward: --\n",
      " 40956/100000: episode: 1401, duration: 0.150s, episode steps: 32, steps per second: 213, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.089 [-1.620, 0.656], mean_best_reward: 80.500000\n",
      " 41008/100000: episode: 1402, duration: 0.245s, episode steps: 52, steps per second: 212, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.107 [-0.975, 0.490], mean_best_reward: --\n",
      " 41046/100000: episode: 1403, duration: 0.178s, episode steps: 38, steps per second: 213, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.020 [-1.585, 0.981], mean_best_reward: --\n",
      " 41095/100000: episode: 1404, duration: 0.236s, episode steps: 49, steps per second: 208, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.083 [-0.965, 0.877], mean_best_reward: --\n",
      " 41168/100000: episode: 1405, duration: 0.337s, episode steps: 73, steps per second: 217, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.203 [-1.278, 1.110], mean_best_reward: --\n",
      " 41180/100000: episode: 1406, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.097 [-1.613, 0.944], mean_best_reward: --\n",
      " 41270/100000: episode: 1407, duration: 0.413s, episode steps: 90, steps per second: 218, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.196, 0.563], mean_best_reward: --\n",
      " 41308/100000: episode: 1408, duration: 0.188s, episode steps: 38, steps per second: 203, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.167 [-0.747, 0.389], mean_best_reward: --\n",
      " 41380/100000: episode: 1409, duration: 0.331s, episode steps: 72, steps per second: 217, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.510, 1.389], mean_best_reward: --\n",
      " 41514/100000: episode: 1410, duration: 0.616s, episode steps: 134, steps per second: 218, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.097 [-1.126, 1.190], mean_best_reward: --\n",
      " 41536/100000: episode: 1411, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.382, 0.926], mean_best_reward: --\n",
      " 41590/100000: episode: 1412, duration: 0.252s, episode steps: 54, steps per second: 215, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.096 [-0.364, 0.884], mean_best_reward: --\n",
      " 41623/100000: episode: 1413, duration: 0.156s, episode steps: 33, steps per second: 212, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.156 [-0.549, 0.926], mean_best_reward: --\n",
      " 41706/100000: episode: 1414, duration: 0.381s, episode steps: 83, steps per second: 218, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.050 [-0.784, 0.784], mean_best_reward: --\n",
      " 41767/100000: episode: 1415, duration: 0.293s, episode steps: 61, steps per second: 208, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.009 [-1.167, 0.749], mean_best_reward: --\n",
      " 41808/100000: episode: 1416, duration: 0.192s, episode steps: 41, steps per second: 214, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.113 [-0.917, 0.483], mean_best_reward: --\n",
      " 41848/100000: episode: 1417, duration: 0.189s, episode steps: 40, steps per second: 211, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.087 [-1.500, 0.462], mean_best_reward: --\n",
      " 41907/100000: episode: 1418, duration: 0.274s, episode steps: 59, steps per second: 215, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.150 [-0.964, 0.637], mean_best_reward: --\n",
      " 41969/100000: episode: 1419, duration: 0.296s, episode steps: 62, steps per second: 210, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.182, 0.815], mean_best_reward: --\n",
      " 42016/100000: episode: 1420, duration: 0.218s, episode steps: 47, steps per second: 216, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.082 [-0.570, 0.896], mean_best_reward: --\n",
      " 42084/100000: episode: 1421, duration: 0.319s, episode steps: 68, steps per second: 213, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.131 [-0.602, 0.989], mean_best_reward: --\n",
      " 42110/100000: episode: 1422, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.124 [-0.936, 0.354], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42164/100000: episode: 1423, duration: 0.265s, episode steps: 54, steps per second: 203, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.129 [-0.955, 0.430], mean_best_reward: --\n",
      " 42203/100000: episode: 1424, duration: 0.189s, episode steps: 39, steps per second: 207, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.103 [-1.168, 0.594], mean_best_reward: --\n",
      " 42235/100000: episode: 1425, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-1.152, 0.388], mean_best_reward: --\n",
      " 42288/100000: episode: 1426, duration: 0.267s, episode steps: 53, steps per second: 198, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.057 [-1.329, 0.828], mean_best_reward: --\n",
      " 42399/100000: episode: 1427, duration: 0.524s, episode steps: 111, steps per second: 212, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.036 [-1.127, 0.901], mean_best_reward: --\n",
      " 42440/100000: episode: 1428, duration: 0.196s, episode steps: 41, steps per second: 209, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.111 [-0.387, 0.718], mean_best_reward: --\n",
      " 42466/100000: episode: 1429, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.095 [-0.885, 0.359], mean_best_reward: --\n",
      " 42498/100000: episode: 1430, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.019 [-0.804, 1.315], mean_best_reward: --\n",
      " 42564/100000: episode: 1431, duration: 0.316s, episode steps: 66, steps per second: 209, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.075 [-0.957, 0.727], mean_best_reward: --\n",
      " 42647/100000: episode: 1432, duration: 0.387s, episode steps: 83, steps per second: 214, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.060 [-0.573, 0.761], mean_best_reward: --\n",
      " 42692/100000: episode: 1433, duration: 0.213s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.105 [-0.436, 0.885], mean_best_reward: --\n",
      " 42706/100000: episode: 1434, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.116 [-0.951, 1.832], mean_best_reward: --\n",
      " 42757/100000: episode: 1435, duration: 0.236s, episode steps: 51, steps per second: 216, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.066 [-0.556, 1.040], mean_best_reward: --\n",
      " 42834/100000: episode: 1436, duration: 0.377s, episode steps: 77, steps per second: 204, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.002 [-0.916, 0.874], mean_best_reward: --\n",
      " 42921/100000: episode: 1437, duration: 0.399s, episode steps: 87, steps per second: 218, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.039 [-0.868, 0.746], mean_best_reward: --\n",
      " 42949/100000: episode: 1438, duration: 0.131s, episode steps: 28, steps per second: 214, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.121 [-0.743, 0.380], mean_best_reward: --\n",
      " 42995/100000: episode: 1439, duration: 0.224s, episode steps: 46, steps per second: 206, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.121 [-0.960, 0.349], mean_best_reward: --\n",
      " 43043/100000: episode: 1440, duration: 0.225s, episode steps: 48, steps per second: 213, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.625, 1.278], mean_best_reward: --\n",
      " 43109/100000: episode: 1441, duration: 0.304s, episode steps: 66, steps per second: 217, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.398, 1.104], mean_best_reward: --\n",
      " 43142/100000: episode: 1442, duration: 0.156s, episode steps: 33, steps per second: 211, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.048 [-0.797, 1.471], mean_best_reward: --\n",
      " 43167/100000: episode: 1443, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.085 [-1.236, 0.770], mean_best_reward: --\n",
      " 43184/100000: episode: 1444, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.094 [-1.263, 0.754], mean_best_reward: --\n",
      " 43228/100000: episode: 1445, duration: 0.213s, episode steps: 44, steps per second: 207, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.167 [-0.343, 0.968], mean_best_reward: --\n",
      " 43319/100000: episode: 1446, duration: 0.418s, episode steps: 91, steps per second: 218, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.053 [-1.160, 0.739], mean_best_reward: --\n",
      " 43335/100000: episode: 1447, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.116 [-1.750, 0.955], mean_best_reward: --\n",
      " 43403/100000: episode: 1448, duration: 0.316s, episode steps: 68, steps per second: 215, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.153 [-0.528, 0.954], mean_best_reward: --\n",
      " 43468/100000: episode: 1449, duration: 0.304s, episode steps: 65, steps per second: 214, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.062 [-0.776, 1.095], mean_best_reward: --\n",
      " 43537/100000: episode: 1450, duration: 0.319s, episode steps: 69, steps per second: 217, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.028 [-1.107, 1.314], mean_best_reward: --\n",
      " 43580/100000: episode: 1451, duration: 0.200s, episode steps: 43, steps per second: 215, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.113 [-0.599, 1.793], mean_best_reward: 91.000000\n",
      " 43624/100000: episode: 1452, duration: 0.211s, episode steps: 44, steps per second: 208, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.059 [-0.809, 1.705], mean_best_reward: --\n",
      " 43655/100000: episode: 1453, duration: 0.148s, episode steps: 31, steps per second: 210, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.133 [-0.351, 1.061], mean_best_reward: --\n",
      " 43693/100000: episode: 1454, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.139 [-1.378, 0.647], mean_best_reward: --\n",
      " 43733/100000: episode: 1455, duration: 0.187s, episode steps: 40, steps per second: 214, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.150 [-0.917, 0.602], mean_best_reward: --\n",
      " 43801/100000: episode: 1456, duration: 0.316s, episode steps: 68, steps per second: 215, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.140 [-1.061, 0.855], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43856/100000: episode: 1457, duration: 0.267s, episode steps: 55, steps per second: 206, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.111 [-1.223, 0.783], mean_best_reward: --\n",
      " 43870/100000: episode: 1458, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.064 [-1.760, 1.194], mean_best_reward: --\n",
      " 43902/100000: episode: 1459, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.067 [-1.210, 0.630], mean_best_reward: --\n",
      " 43968/100000: episode: 1460, duration: 0.313s, episode steps: 66, steps per second: 211, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.890, 1.173], mean_best_reward: --\n",
      " 44027/100000: episode: 1461, duration: 0.282s, episode steps: 59, steps per second: 209, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.081 [-0.604, 1.093], mean_best_reward: --\n",
      " 44068/100000: episode: 1462, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.124 [-1.045, 0.454], mean_best_reward: --\n",
      " 44095/100000: episode: 1463, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.087 [-1.111, 0.633], mean_best_reward: --\n",
      " 44133/100000: episode: 1464, duration: 0.182s, episode steps: 38, steps per second: 208, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.575, 1.161], mean_best_reward: --\n",
      " 44165/100000: episode: 1465, duration: 0.154s, episode steps: 32, steps per second: 207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.584, 1.077], mean_best_reward: --\n",
      " 44236/100000: episode: 1466, duration: 0.333s, episode steps: 71, steps per second: 213, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.021 [-0.715, 0.625], mean_best_reward: --\n",
      " 44272/100000: episode: 1467, duration: 0.185s, episode steps: 36, steps per second: 195, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-0.797, 1.151], mean_best_reward: --\n",
      " 44336/100000: episode: 1468, duration: 0.302s, episode steps: 64, steps per second: 212, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.738, 0.983], mean_best_reward: --\n",
      " 44352/100000: episode: 1469, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.072 [-1.665, 1.021], mean_best_reward: --\n",
      " 44369/100000: episode: 1470, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.116 [-1.835, 1.003], mean_best_reward: --\n",
      " 44413/100000: episode: 1471, duration: 0.210s, episode steps: 44, steps per second: 210, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.146 [-0.911, 0.605], mean_best_reward: --\n",
      " 44431/100000: episode: 1472, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.057 [-2.979, 1.936], mean_best_reward: --\n",
      " 44497/100000: episode: 1473, duration: 0.314s, episode steps: 66, steps per second: 210, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.177 [-1.564, 0.667], mean_best_reward: --\n",
      " 44514/100000: episode: 1474, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.087 [-1.200, 0.776], mean_best_reward: --\n",
      " 44600/100000: episode: 1475, duration: 0.397s, episode steps: 86, steps per second: 217, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.055 [-0.771, 0.646], mean_best_reward: --\n",
      " 44634/100000: episode: 1476, duration: 0.161s, episode steps: 34, steps per second: 212, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.439, 1.126], mean_best_reward: --\n",
      " 44684/100000: episode: 1477, duration: 0.240s, episode steps: 50, steps per second: 209, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.625, 1.416], mean_best_reward: --\n",
      " 44740/100000: episode: 1478, duration: 0.261s, episode steps: 56, steps per second: 215, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.050 [-0.903, 0.313], mean_best_reward: --\n",
      " 44752/100000: episode: 1479, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.950, 1.615], mean_best_reward: --\n",
      " 44773/100000: episode: 1480, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.065 [-1.983, 1.215], mean_best_reward: --\n",
      " 44808/100000: episode: 1481, duration: 0.166s, episode steps: 35, steps per second: 211, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.025 [-1.348, 1.586], mean_best_reward: --\n",
      " 44842/100000: episode: 1482, duration: 0.159s, episode steps: 34, steps per second: 213, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.014 [-1.266, 0.806], mean_best_reward: --\n",
      " 44881/100000: episode: 1483, duration: 0.190s, episode steps: 39, steps per second: 205, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.082 [-1.131, 0.438], mean_best_reward: --\n",
      " 44890/100000: episode: 1484, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.277, 1.408], mean_best_reward: --\n",
      " 44909/100000: episode: 1485, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.085 [-1.921, 1.003], mean_best_reward: --\n",
      " 44939/100000: episode: 1486, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.082 [-0.629, 1.395], mean_best_reward: --\n",
      " 45077/100000: episode: 1487, duration: 0.637s, episode steps: 138, steps per second: 217, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.053 [-0.627, 1.185], mean_best_reward: --\n",
      " 45114/100000: episode: 1488, duration: 0.175s, episode steps: 37, steps per second: 212, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.123 [-1.318, 0.527], mean_best_reward: --\n",
      " 45138/100000: episode: 1489, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.081 [-1.387, 0.475], mean_best_reward: --\n",
      " 45155/100000: episode: 1490, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.101 [-0.549, 0.966], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45218/100000: episode: 1491, duration: 0.293s, episode steps: 63, steps per second: 215, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.172 [-0.430, 0.991], mean_best_reward: --\n",
      " 45290/100000: episode: 1492, duration: 0.339s, episode steps: 72, steps per second: 213, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.076 [-0.810, 1.123], mean_best_reward: --\n",
      " 45302/100000: episode: 1493, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.125 [-2.593, 1.615], mean_best_reward: --\n",
      " 45333/100000: episode: 1494, duration: 0.150s, episode steps: 31, steps per second: 207, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.031 [-1.508, 0.831], mean_best_reward: --\n",
      " 45372/100000: episode: 1495, duration: 0.182s, episode steps: 39, steps per second: 214, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.126 [-1.503, 0.615], mean_best_reward: --\n",
      " 45437/100000: episode: 1496, duration: 0.302s, episode steps: 65, steps per second: 215, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.107 [-0.538, 0.988], mean_best_reward: --\n",
      " 45495/100000: episode: 1497, duration: 0.274s, episode steps: 58, steps per second: 212, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.115 [-0.939, 0.453], mean_best_reward: --\n",
      " 45533/100000: episode: 1498, duration: 0.178s, episode steps: 38, steps per second: 214, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.086 [-0.708, 0.421], mean_best_reward: --\n",
      " 45562/100000: episode: 1499, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.120 [-0.913, 0.571], mean_best_reward: --\n",
      " 45599/100000: episode: 1500, duration: 0.177s, episode steps: 37, steps per second: 210, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.095 [-1.105, 0.574], mean_best_reward: --\n",
      " 45646/100000: episode: 1501, duration: 0.226s, episode steps: 47, steps per second: 208, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.014 [-0.950, 0.780], mean_best_reward: 131.000000\n",
      " 45693/100000: episode: 1502, duration: 0.222s, episode steps: 47, steps per second: 211, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.022 [-0.923, 0.840], mean_best_reward: --\n",
      " 45720/100000: episode: 1503, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.085 [-0.748, 1.184], mean_best_reward: --\n",
      " 45738/100000: episode: 1504, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.089 [-0.851, 1.684], mean_best_reward: --\n",
      " 45773/100000: episode: 1505, duration: 0.167s, episode steps: 35, steps per second: 209, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.052 [-0.433, 0.869], mean_best_reward: --\n",
      " 45814/100000: episode: 1506, duration: 0.198s, episode steps: 41, steps per second: 207, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.006 [-1.821, 1.219], mean_best_reward: --\n",
      " 45860/100000: episode: 1507, duration: 0.221s, episode steps: 46, steps per second: 209, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.044 [-0.999, 2.012], mean_best_reward: --\n",
      " 45913/100000: episode: 1508, duration: 0.257s, episode steps: 53, steps per second: 206, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.073 [-0.396, 1.101], mean_best_reward: --\n",
      " 45923/100000: episode: 1509, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.545, 1.565], mean_best_reward: --\n",
      " 45947/100000: episode: 1510, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.086 [-1.714, 0.759], mean_best_reward: --\n",
      " 45978/100000: episode: 1511, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.149 [-0.364, 0.894], mean_best_reward: --\n",
      " 45994/100000: episode: 1512, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.085 [-1.515, 0.942], mean_best_reward: --\n",
      " 46064/100000: episode: 1513, duration: 0.331s, episode steps: 70, steps per second: 211, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.056, 1.175], mean_best_reward: --\n",
      " 46074/100000: episode: 1514, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.113 [-1.997, 1.168], mean_best_reward: --\n",
      " 46097/100000: episode: 1515, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.102 [-0.794, 1.367], mean_best_reward: --\n",
      " 46154/100000: episode: 1516, duration: 0.280s, episode steps: 57, steps per second: 203, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.031 [-1.137, 0.937], mean_best_reward: --\n",
      " 46195/100000: episode: 1517, duration: 0.191s, episode steps: 41, steps per second: 215, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.115 [-0.402, 1.503], mean_best_reward: --\n",
      " 46237/100000: episode: 1518, duration: 0.200s, episode steps: 42, steps per second: 210, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.092 [-0.887, 0.565], mean_best_reward: --\n",
      " 46253/100000: episode: 1519, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.809, 1.521], mean_best_reward: --\n",
      " 46296/100000: episode: 1520, duration: 0.202s, episode steps: 43, steps per second: 213, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.125 [-1.077, 0.500], mean_best_reward: --\n",
      " 46376/100000: episode: 1521, duration: 0.376s, episode steps: 80, steps per second: 213, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.051 [-1.060, 0.605], mean_best_reward: --\n",
      " 46400/100000: episode: 1522, duration: 0.114s, episode steps: 24, steps per second: 211, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.061 [-1.394, 0.794], mean_best_reward: --\n",
      " 46436/100000: episode: 1523, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.408, 0.957], mean_best_reward: --\n",
      " 46475/100000: episode: 1524, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.020 [-1.710, 1.184], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46519/100000: episode: 1525, duration: 0.206s, episode steps: 44, steps per second: 214, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.328, 0.345], mean_best_reward: --\n",
      " 46544/100000: episode: 1526, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.086 [-1.204, 0.619], mean_best_reward: --\n",
      " 46565/100000: episode: 1527, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.098 [-0.561, 1.290], mean_best_reward: --\n",
      " 46619/100000: episode: 1528, duration: 0.252s, episode steps: 54, steps per second: 215, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.303, 0.570], mean_best_reward: --\n",
      " 46636/100000: episode: 1529, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.094 [-2.368, 1.359], mean_best_reward: --\n",
      " 46676/100000: episode: 1530, duration: 0.188s, episode steps: 40, steps per second: 213, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: -0.101 [-2.318, 1.163], mean_best_reward: --\n",
      " 46709/100000: episode: 1531, duration: 0.155s, episode steps: 33, steps per second: 213, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.058 [-0.555, 0.914], mean_best_reward: --\n",
      " 46754/100000: episode: 1532, duration: 0.217s, episode steps: 45, steps per second: 208, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.014 [-1.328, 0.939], mean_best_reward: --\n",
      " 46806/100000: episode: 1533, duration: 0.244s, episode steps: 52, steps per second: 213, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.069 [-1.147, 0.579], mean_best_reward: --\n",
      " 46825/100000: episode: 1534, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.057 [-2.848, 1.793], mean_best_reward: --\n",
      " 46875/100000: episode: 1535, duration: 0.235s, episode steps: 50, steps per second: 213, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.104 [-0.433, 0.884], mean_best_reward: --\n",
      " 46888/100000: episode: 1536, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.105 [-2.714, 1.741], mean_best_reward: --\n",
      " 46943/100000: episode: 1537, duration: 0.255s, episode steps: 55, steps per second: 216, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.118 [-0.706, 0.926], mean_best_reward: --\n",
      " 46972/100000: episode: 1538, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.107 [-1.182, 0.607], mean_best_reward: --\n",
      " 46993/100000: episode: 1539, duration: 0.107s, episode steps: 21, steps per second: 195, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.050 [-1.219, 1.991], mean_best_reward: --\n",
      " 47037/100000: episode: 1540, duration: 0.217s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.091 [-0.926, 0.416], mean_best_reward: --\n",
      " 47101/100000: episode: 1541, duration: 0.297s, episode steps: 64, steps per second: 215, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.117 [-0.977, 0.588], mean_best_reward: --\n",
      " 47148/100000: episode: 1542, duration: 0.219s, episode steps: 47, steps per second: 214, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.010 [-2.026, 1.367], mean_best_reward: --\n",
      " 47163/100000: episode: 1543, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.080 [-2.740, 1.783], mean_best_reward: --\n",
      " 47209/100000: episode: 1544, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.297, 0.549], mean_best_reward: --\n",
      " 47239/100000: episode: 1545, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-0.962, 0.597], mean_best_reward: --\n",
      " 47263/100000: episode: 1546, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.135 [-0.552, 1.032], mean_best_reward: --\n",
      " 47295/100000: episode: 1547, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-1.119, 0.374], mean_best_reward: --\n",
      " 47314/100000: episode: 1548, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.082 [-1.918, 1.010], mean_best_reward: --\n",
      " 47370/100000: episode: 1549, duration: 0.271s, episode steps: 56, steps per second: 207, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.007 [-0.988, 1.204], mean_best_reward: --\n",
      " 47479/100000: episode: 1550, duration: 0.506s, episode steps: 109, steps per second: 215, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.059 [-0.729, 1.471], mean_best_reward: --\n",
      " 47520/100000: episode: 1551, duration: 0.195s, episode steps: 41, steps per second: 210, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.116 [-0.760, 1.152], mean_best_reward: 118.000000\n",
      " 47593/100000: episode: 1552, duration: 0.349s, episode steps: 73, steps per second: 209, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.013 [-0.769, 1.005], mean_best_reward: --\n",
      " 47630/100000: episode: 1553, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.106 [-0.355, 0.845], mean_best_reward: --\n",
      " 47680/100000: episode: 1554, duration: 0.237s, episode steps: 50, steps per second: 211, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.359, 0.761], mean_best_reward: --\n",
      " 47712/100000: episode: 1555, duration: 0.153s, episode steps: 32, steps per second: 209, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.084 [-0.369, 0.727], mean_best_reward: --\n",
      " 47740/100000: episode: 1556, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.602, 0.945], mean_best_reward: --\n",
      " 47775/100000: episode: 1557, duration: 0.168s, episode steps: 35, steps per second: 208, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.052 [-0.400, 0.862], mean_best_reward: --\n",
      " 47845/100000: episode: 1558, duration: 0.333s, episode steps: 70, steps per second: 210, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-1.129, 1.366], mean_best_reward: --\n",
      " 47880/100000: episode: 1559, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.057 [-0.422, 1.141], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47918/100000: episode: 1560, duration: 0.181s, episode steps: 38, steps per second: 209, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.072 [-0.561, 1.011], mean_best_reward: --\n",
      " 47949/100000: episode: 1561, duration: 0.146s, episode steps: 31, steps per second: 212, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.142 [-0.748, 1.240], mean_best_reward: --\n",
      " 47982/100000: episode: 1562, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.071 [-0.570, 1.339], mean_best_reward: --\n",
      " 48011/100000: episode: 1563, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.123 [-1.527, 0.935], mean_best_reward: --\n",
      " 48044/100000: episode: 1564, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.124 [-1.169, 0.550], mean_best_reward: --\n",
      " 48081/100000: episode: 1565, duration: 0.173s, episode steps: 37, steps per second: 213, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.015 [-0.825, 1.224], mean_best_reward: --\n",
      " 48129/100000: episode: 1566, duration: 0.227s, episode steps: 48, steps per second: 212, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.133 [-1.385, 0.391], mean_best_reward: --\n",
      " 48138/100000: episode: 1567, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.156 [-2.444, 1.531], mean_best_reward: --\n",
      " 48180/100000: episode: 1568, duration: 0.196s, episode steps: 42, steps per second: 214, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.136 [-0.221, 0.711], mean_best_reward: --\n",
      " 48291/100000: episode: 1569, duration: 0.514s, episode steps: 111, steps per second: 216, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.017 [-0.769, 1.045], mean_best_reward: --\n",
      " 48308/100000: episode: 1570, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.103 [-1.363, 2.444], mean_best_reward: --\n",
      " 48368/100000: episode: 1571, duration: 0.278s, episode steps: 60, steps per second: 216, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.034 [-0.810, 1.189], mean_best_reward: --\n",
      " 48424/100000: episode: 1572, duration: 0.265s, episode steps: 56, steps per second: 211, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.147 [-1.290, 0.766], mean_best_reward: --\n",
      " 48466/100000: episode: 1573, duration: 0.197s, episode steps: 42, steps per second: 214, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.210 [-0.632, 1.316], mean_best_reward: --\n",
      " 48500/100000: episode: 1574, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.094 [-0.576, 1.644], mean_best_reward: --\n",
      " 48557/100000: episode: 1575, duration: 0.281s, episode steps: 57, steps per second: 203, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.002 [-1.295, 0.743], mean_best_reward: --\n",
      " 48621/100000: episode: 1576, duration: 0.302s, episode steps: 64, steps per second: 212, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.078 [-1.107, 0.564], mean_best_reward: --\n",
      " 48663/100000: episode: 1577, duration: 0.195s, episode steps: 42, steps per second: 215, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.067, 0.763], mean_best_reward: --\n",
      " 48695/100000: episode: 1578, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.805, 1.259], mean_best_reward: --\n",
      " 48734/100000: episode: 1579, duration: 0.183s, episode steps: 39, steps per second: 213, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.103 [-0.738, 0.988], mean_best_reward: --\n",
      " 48774/100000: episode: 1580, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.402, 1.060], mean_best_reward: --\n",
      " 48821/100000: episode: 1581, duration: 0.218s, episode steps: 47, steps per second: 215, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.112 [-0.455, 1.013], mean_best_reward: --\n",
      " 48843/100000: episode: 1582, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.090 [-0.460, 1.113], mean_best_reward: --\n",
      " 48866/100000: episode: 1583, duration: 0.112s, episode steps: 23, steps per second: 206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.090 [-0.794, 1.230], mean_best_reward: --\n",
      " 48901/100000: episode: 1584, duration: 0.165s, episode steps: 35, steps per second: 213, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.131 [-0.474, 0.849], mean_best_reward: --\n",
      " 48917/100000: episode: 1585, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.276], mean_best_reward: --\n",
      " 48956/100000: episode: 1586, duration: 0.183s, episode steps: 39, steps per second: 213, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.092 [-0.537, 0.976], mean_best_reward: --\n",
      " 49015/100000: episode: 1587, duration: 0.275s, episode steps: 59, steps per second: 215, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.067 [-0.938, 1.445], mean_best_reward: --\n",
      " 49062/100000: episode: 1588, duration: 0.230s, episode steps: 47, steps per second: 205, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.003 [-0.981, 1.368], mean_best_reward: --\n",
      " 49099/100000: episode: 1589, duration: 0.177s, episode steps: 37, steps per second: 209, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.043 [-0.576, 1.150], mean_best_reward: --\n",
      " 49126/100000: episode: 1590, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.097 [-0.560, 1.152], mean_best_reward: --\n",
      " 49150/100000: episode: 1591, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.099 [-0.628, 0.964], mean_best_reward: --\n",
      " 49169/100000: episode: 1592, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.080 [-0.640, 1.408], mean_best_reward: --\n",
      " 49231/100000: episode: 1593, duration: 0.292s, episode steps: 62, steps per second: 212, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.098 [-0.670, 0.864], mean_best_reward: --\n",
      " 49264/100000: episode: 1594, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.137 [-0.545, 1.457], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49335/100000: episode: 1595, duration: 0.334s, episode steps: 71, steps per second: 213, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.004 [-1.457, 1.313], mean_best_reward: --\n",
      " 49365/100000: episode: 1596, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.116 [-0.439, 1.428], mean_best_reward: --\n",
      " 49465/100000: episode: 1597, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.104 [-1.542, 0.611], mean_best_reward: --\n",
      " 49524/100000: episode: 1598, duration: 0.280s, episode steps: 59, steps per second: 211, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.051 [-0.593, 1.185], mean_best_reward: --\n",
      " 49595/100000: episode: 1599, duration: 0.337s, episode steps: 71, steps per second: 211, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.074 [-0.915, 1.297], mean_best_reward: --\n",
      " 49620/100000: episode: 1600, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.087 [-0.794, 1.574], mean_best_reward: --\n",
      " 49636/100000: episode: 1601, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.080 [-1.265, 0.813], mean_best_reward: 130.000000\n",
      " 49660/100000: episode: 1602, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.126 [-0.803, 1.211], mean_best_reward: --\n",
      " 49728/100000: episode: 1603, duration: 0.319s, episode steps: 68, steps per second: 213, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.004 [-1.289, 0.725], mean_best_reward: --\n",
      " 49785/100000: episode: 1604, duration: 0.268s, episode steps: 57, steps per second: 213, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.092 [-0.917, 0.454], mean_best_reward: --\n",
      " 49820/100000: episode: 1605, duration: 0.167s, episode steps: 35, steps per second: 209, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.115 [-0.532, 1.206], mean_best_reward: --\n",
      " 49863/100000: episode: 1606, duration: 0.202s, episode steps: 43, steps per second: 213, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.027 [-0.595, 1.112], mean_best_reward: --\n",
      " 49907/100000: episode: 1607, duration: 0.213s, episode steps: 44, steps per second: 207, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.027 [-1.179, 2.009], mean_best_reward: --\n",
      " 49916/100000: episode: 1608, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.166 [-2.507, 1.543], mean_best_reward: --\n",
      " 49943/100000: episode: 1609, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.119 [-0.589, 1.726], mean_best_reward: --\n",
      " 49975/100000: episode: 1610, duration: 0.152s, episode steps: 32, steps per second: 210, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.139 [-1.004, 0.362], mean_best_reward: --\n",
      " 50023/100000: episode: 1611, duration: 0.225s, episode steps: 48, steps per second: 214, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.573, 1.020], mean_best_reward: --\n",
      " 50059/100000: episode: 1612, duration: 0.169s, episode steps: 36, steps per second: 212, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.159 [-0.742, 1.198], mean_best_reward: --\n",
      " 50101/100000: episode: 1613, duration: 0.204s, episode steps: 42, steps per second: 206, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.079 [-0.791, 1.549], mean_best_reward: --\n",
      " 50151/100000: episode: 1614, duration: 0.235s, episode steps: 50, steps per second: 213, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.103 [-0.679, 1.303], mean_best_reward: --\n",
      " 50171/100000: episode: 1615, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.058 [-2.992, 1.951], mean_best_reward: --\n",
      " 50203/100000: episode: 1616, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.051 [-1.198, 0.814], mean_best_reward: --\n",
      " 50240/100000: episode: 1617, duration: 0.175s, episode steps: 37, steps per second: 211, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.064 [-1.214, 0.426], mean_best_reward: --\n",
      " 50284/100000: episode: 1618, duration: 0.214s, episode steps: 44, steps per second: 206, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.814, 1.081], mean_best_reward: --\n",
      " 50410/100000: episode: 1619, duration: 0.580s, episode steps: 126, steps per second: 217, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.068 [-1.596, 0.957], mean_best_reward: --\n",
      " 50446/100000: episode: 1620, duration: 0.170s, episode steps: 36, steps per second: 211, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-1.539, 1.149], mean_best_reward: --\n",
      " 50495/100000: episode: 1621, duration: 0.232s, episode steps: 49, steps per second: 211, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.096 [-0.601, 1.010], mean_best_reward: --\n",
      " 50519/100000: episode: 1622, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.097 [-0.832, 1.298], mean_best_reward: --\n",
      " 50550/100000: episode: 1623, duration: 0.148s, episode steps: 31, steps per second: 210, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.082 [-1.661, 0.764], mean_best_reward: --\n",
      " 50628/100000: episode: 1624, duration: 0.372s, episode steps: 78, steps per second: 210, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.062 [-0.739, 0.824], mean_best_reward: --\n",
      " 50702/100000: episode: 1625, duration: 0.346s, episode steps: 74, steps per second: 214, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.043 [-1.013, 0.812], mean_best_reward: --\n",
      " 50743/100000: episode: 1626, duration: 0.199s, episode steps: 41, steps per second: 206, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.111 [-1.790, 0.628], mean_best_reward: --\n",
      " 50768/100000: episode: 1627, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.106 [-2.129, 0.994], mean_best_reward: --\n",
      " 50806/100000: episode: 1628, duration: 0.177s, episode steps: 38, steps per second: 215, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.074 [-1.248, 0.578], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50834/100000: episode: 1629, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.064 [-0.949, 0.634], mean_best_reward: --\n",
      " 50855/100000: episode: 1630, duration: 0.140s, episode steps: 21, steps per second: 150, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.107 [-0.644, 1.429], mean_best_reward: --\n",
      " 50884/100000: episode: 1631, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.035 [-2.137, 1.335], mean_best_reward: --\n",
      " 50940/100000: episode: 1632, duration: 0.277s, episode steps: 56, steps per second: 202, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.589 [0.000, 1.000], mean observation: 0.129 [-2.058, 1.942], mean_best_reward: --\n",
      " 50994/100000: episode: 1633, duration: 0.262s, episode steps: 54, steps per second: 206, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.056 [-0.947, 0.612], mean_best_reward: --\n",
      " 51041/100000: episode: 1634, duration: 0.227s, episode steps: 47, steps per second: 207, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.113 [-0.366, 0.822], mean_best_reward: --\n",
      " 51064/100000: episode: 1635, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.113 [-0.610, 0.981], mean_best_reward: --\n",
      " 51122/100000: episode: 1636, duration: 0.281s, episode steps: 58, steps per second: 207, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.073 [-1.109, 0.978], mean_best_reward: --\n",
      " 51279/100000: episode: 1637, duration: 0.847s, episode steps: 157, steps per second: 185, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.133 [-2.472, 2.990], mean_best_reward: --\n",
      " 51340/100000: episode: 1638, duration: 0.320s, episode steps: 61, steps per second: 190, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.018 [-1.384, 0.950], mean_best_reward: --\n",
      " 51371/100000: episode: 1639, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.066 [-1.173, 0.792], mean_best_reward: --\n",
      " 51435/100000: episode: 1640, duration: 0.326s, episode steps: 64, steps per second: 196, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.092 [-0.755, 1.404], mean_best_reward: --\n",
      " 51484/100000: episode: 1641, duration: 0.243s, episode steps: 49, steps per second: 202, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.136 [-0.783, 0.362], mean_best_reward: --\n",
      " 51524/100000: episode: 1642, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.147 [-1.051, 0.634], mean_best_reward: --\n",
      " 51561/100000: episode: 1643, duration: 0.221s, episode steps: 37, steps per second: 167, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.063 [-0.790, 1.068], mean_best_reward: --\n",
      " 51615/100000: episode: 1644, duration: 0.294s, episode steps: 54, steps per second: 184, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.044 [-0.618, 0.915], mean_best_reward: --\n",
      " 51627/100000: episode: 1645, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.129 [-1.619, 0.976], mean_best_reward: --\n",
      " 51675/100000: episode: 1646, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.147 [-1.147, 0.809], mean_best_reward: --\n",
      " 51698/100000: episode: 1647, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.103 [-0.905, 0.430], mean_best_reward: --\n",
      " 51787/100000: episode: 1648, duration: 0.454s, episode steps: 89, steps per second: 196, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.239 [-0.485, 1.831], mean_best_reward: --\n",
      " 51852/100000: episode: 1649, duration: 0.341s, episode steps: 65, steps per second: 191, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.023 [-1.098, 0.597], mean_best_reward: --\n",
      " 51903/100000: episode: 1650, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.154 [-1.624, 1.203], mean_best_reward: --\n",
      " 51978/100000: episode: 1651, duration: 0.364s, episode steps: 75, steps per second: 206, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.079 [-1.949, 2.138], mean_best_reward: 92.500000\n",
      " 52018/100000: episode: 1652, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.097 [-1.026, 0.564], mean_best_reward: --\n",
      " 52061/100000: episode: 1653, duration: 0.217s, episode steps: 43, steps per second: 199, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.080 [-0.425, 1.247], mean_best_reward: --\n",
      " 52116/100000: episode: 1654, duration: 0.302s, episode steps: 55, steps per second: 182, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.132 [-0.574, 0.922], mean_best_reward: --\n",
      " 52130/100000: episode: 1655, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.117 [-1.466, 0.794], mean_best_reward: --\n",
      " 52159/100000: episode: 1656, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.109 [-1.125, 0.539], mean_best_reward: --\n",
      " 52186/100000: episode: 1657, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.426, 0.748], mean_best_reward: --\n",
      " 52206/100000: episode: 1658, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.897, 0.447], mean_best_reward: --\n",
      " 52271/100000: episode: 1659, duration: 0.314s, episode steps: 65, steps per second: 207, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.026 [-0.886, 0.755], mean_best_reward: --\n",
      " 52295/100000: episode: 1660, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.339, 0.618], mean_best_reward: --\n",
      " 52362/100000: episode: 1661, duration: 0.318s, episode steps: 67, steps per second: 211, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.121 [-0.511, 0.908], mean_best_reward: --\n",
      " 52402/100000: episode: 1662, duration: 0.185s, episode steps: 40, steps per second: 216, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.125 [-0.784, 1.221], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52503/100000: episode: 1663, duration: 0.465s, episode steps: 101, steps per second: 217, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.308 [-0.835, 1.596], mean_best_reward: --\n",
      " 52600/100000: episode: 1664, duration: 0.455s, episode steps: 97, steps per second: 213, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.030 [-1.263, 0.746], mean_best_reward: --\n",
      " 52630/100000: episode: 1665, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-0.920, 0.413], mean_best_reward: --\n",
      " 52652/100000: episode: 1666, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-1.355, 0.832], mean_best_reward: --\n",
      " 52676/100000: episode: 1667, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.040 [-1.177, 1.760], mean_best_reward: --\n",
      " 52698/100000: episode: 1668, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.789, 1.207], mean_best_reward: --\n",
      " 52779/100000: episode: 1669, duration: 0.382s, episode steps: 81, steps per second: 212, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.093 [-1.270, 1.357], mean_best_reward: --\n",
      " 52820/100000: episode: 1670, duration: 0.196s, episode steps: 41, steps per second: 209, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.151 [-1.128, 0.442], mean_best_reward: --\n",
      " 52851/100000: episode: 1671, duration: 0.152s, episode steps: 31, steps per second: 203, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.026 [-0.936, 1.551], mean_best_reward: --\n",
      " 52873/100000: episode: 1672, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.159, 0.811], mean_best_reward: --\n",
      " 52900/100000: episode: 1673, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.079 [-0.878, 0.554], mean_best_reward: --\n",
      " 52912/100000: episode: 1674, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.116 [-1.668, 1.007], mean_best_reward: --\n",
      " 52937/100000: episode: 1675, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.098 [-1.212, 0.557], mean_best_reward: --\n",
      " 52980/100000: episode: 1676, duration: 0.218s, episode steps: 43, steps per second: 198, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.047 [-1.152, 0.778], mean_best_reward: --\n",
      " 53014/100000: episode: 1677, duration: 0.170s, episode steps: 34, steps per second: 199, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.060 [-0.756, 0.442], mean_best_reward: --\n",
      " 53114/100000: episode: 1678, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.063 [-0.592, 1.125], mean_best_reward: --\n",
      " 53159/100000: episode: 1679, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.049 [-1.022, 0.779], mean_best_reward: --\n",
      " 53187/100000: episode: 1680, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.183, 0.575], mean_best_reward: --\n",
      " 53239/100000: episode: 1681, duration: 0.248s, episode steps: 52, steps per second: 209, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.094 [-0.754, 1.288], mean_best_reward: --\n",
      " 53257/100000: episode: 1682, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.095 [-1.399, 0.613], mean_best_reward: --\n",
      " 53292/100000: episode: 1683, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.078 [-1.080, 0.746], mean_best_reward: --\n",
      " 53325/100000: episode: 1684, duration: 0.248s, episode steps: 33, steps per second: 133, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.093 [-0.940, 0.431], mean_best_reward: --\n",
      " 53348/100000: episode: 1685, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.060 [-1.636, 0.960], mean_best_reward: --\n",
      " 53445/100000: episode: 1686, duration: 0.545s, episode steps: 97, steps per second: 178, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.069 [-0.671, 1.405], mean_best_reward: --\n",
      " 53480/100000: episode: 1687, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.049 [-1.910, 0.975], mean_best_reward: --\n",
      " 53502/100000: episode: 1688, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.627, 1.033], mean_best_reward: --\n",
      " 53520/100000: episode: 1689, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.106 [-1.366, 0.588], mean_best_reward: --\n",
      " 53562/100000: episode: 1690, duration: 0.255s, episode steps: 42, steps per second: 165, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.011 [-1.840, 1.314], mean_best_reward: --\n",
      " 53573/100000: episode: 1691, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.120 [-1.579, 1.007], mean_best_reward: --\n",
      " 53609/100000: episode: 1692, duration: 0.171s, episode steps: 36, steps per second: 211, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.057 [-0.806, 1.540], mean_best_reward: --\n",
      " 53629/100000: episode: 1693, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.209, 0.820], mean_best_reward: --\n",
      " 53678/100000: episode: 1694, duration: 0.236s, episode steps: 49, steps per second: 208, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.074 [-0.696, 0.437], mean_best_reward: --\n",
      " 53715/100000: episode: 1695, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.073 [-1.237, 0.827], mean_best_reward: --\n",
      " 53759/100000: episode: 1696, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.134 [-0.994, 0.331], mean_best_reward: --\n",
      " 53783/100000: episode: 1697, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.359, 0.761], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53830/100000: episode: 1698, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.087 [-1.254, 0.433], mean_best_reward: --\n",
      " 53862/100000: episode: 1699, duration: 0.151s, episode steps: 32, steps per second: 211, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.140, 0.626], mean_best_reward: --\n",
      " 53884/100000: episode: 1700, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.057 [-1.263, 0.808], mean_best_reward: --\n",
      " 53914/100000: episode: 1701, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.064, 0.355], mean_best_reward: 93.500000\n",
      " 53985/100000: episode: 1702, duration: 0.367s, episode steps: 71, steps per second: 194, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.081 [-0.819, 1.177], mean_best_reward: --\n",
      " 54069/100000: episode: 1703, duration: 0.420s, episode steps: 84, steps per second: 200, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.039 [-0.933, 0.886], mean_best_reward: --\n",
      " 54120/100000: episode: 1704, duration: 0.238s, episode steps: 51, steps per second: 214, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.122 [-1.018, 0.422], mean_best_reward: --\n",
      " 54169/100000: episode: 1705, duration: 0.266s, episode steps: 49, steps per second: 185, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.097 [-0.601, 0.871], mean_best_reward: --\n",
      " 54208/100000: episode: 1706, duration: 0.186s, episode steps: 39, steps per second: 209, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.063 [-1.326, 0.643], mean_best_reward: --\n",
      " 54245/100000: episode: 1707, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.128 [-0.927, 0.268], mean_best_reward: --\n",
      " 54336/100000: episode: 1708, duration: 0.460s, episode steps: 91, steps per second: 198, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.055 [-0.903, 0.966], mean_best_reward: --\n",
      " 54352/100000: episode: 1709, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.354, 0.793], mean_best_reward: --\n",
      " 54406/100000: episode: 1710, duration: 0.281s, episode steps: 54, steps per second: 192, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.079 [-0.942, 1.305], mean_best_reward: --\n",
      " 54437/100000: episode: 1711, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.105 [-1.412, 0.415], mean_best_reward: --\n",
      " 54473/100000: episode: 1712, duration: 0.185s, episode steps: 36, steps per second: 194, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.068 [-1.481, 0.571], mean_best_reward: --\n",
      " 54513/100000: episode: 1713, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.092 [-0.592, 1.027], mean_best_reward: --\n",
      " 54527/100000: episode: 1714, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.567, 1.113], mean_best_reward: --\n",
      " 54570/100000: episode: 1715, duration: 0.222s, episode steps: 43, steps per second: 193, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.062 [-0.983, 0.546], mean_best_reward: --\n",
      " 54642/100000: episode: 1716, duration: 0.352s, episode steps: 72, steps per second: 205, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.026 [-1.020, 0.621], mean_best_reward: --\n",
      " 54730/100000: episode: 1717, duration: 0.425s, episode steps: 88, steps per second: 207, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.083 [-1.010, 0.530], mean_best_reward: --\n",
      " 54775/100000: episode: 1718, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.124 [-0.430, 1.157], mean_best_reward: --\n",
      " 54814/100000: episode: 1719, duration: 0.190s, episode steps: 39, steps per second: 205, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.137 [-0.972, 0.387], mean_best_reward: --\n",
      " 54900/100000: episode: 1720, duration: 0.419s, episode steps: 86, steps per second: 205, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.744, 1.031], mean_best_reward: --\n",
      " 54946/100000: episode: 1721, duration: 0.222s, episode steps: 46, steps per second: 207, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.608, 0.448], mean_best_reward: --\n",
      " 54981/100000: episode: 1722, duration: 0.171s, episode steps: 35, steps per second: 204, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.099 [-0.591, 0.973], mean_best_reward: --\n",
      " 54993/100000: episode: 1723, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.102 [-1.774, 1.139], mean_best_reward: --\n",
      " 55003/100000: episode: 1724, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.130 [-1.373, 0.772], mean_best_reward: --\n",
      " 55055/100000: episode: 1725, duration: 0.254s, episode steps: 52, steps per second: 205, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.093 [-0.759, 1.302], mean_best_reward: --\n",
      " 55090/100000: episode: 1726, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.073 [-1.131, 0.753], mean_best_reward: --\n",
      " 55218/100000: episode: 1727, duration: 0.620s, episode steps: 128, steps per second: 207, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.084 [-0.936, 0.638], mean_best_reward: --\n",
      " 55254/100000: episode: 1728, duration: 0.182s, episode steps: 36, steps per second: 197, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.012 [-1.370, 0.979], mean_best_reward: --\n",
      " 55310/100000: episode: 1729, duration: 0.280s, episode steps: 56, steps per second: 200, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.125 [-0.518, 0.894], mean_best_reward: --\n",
      " 55439/100000: episode: 1730, duration: 0.642s, episode steps: 129, steps per second: 201, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.257 [-2.201, 1.128], mean_best_reward: --\n",
      " 55461/100000: episode: 1731, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.108 [-0.901, 0.594], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55514/100000: episode: 1732, duration: 0.258s, episode steps: 53, steps per second: 206, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.036 [-1.114, 0.794], mean_best_reward: --\n",
      " 55536/100000: episode: 1733, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.089 [-1.670, 0.758], mean_best_reward: --\n",
      " 55570/100000: episode: 1734, duration: 0.168s, episode steps: 34, steps per second: 203, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.150, 0.578], mean_best_reward: --\n",
      " 55633/100000: episode: 1735, duration: 0.311s, episode steps: 63, steps per second: 203, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.215 [-1.689, 0.369], mean_best_reward: --\n",
      " 55673/100000: episode: 1736, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.129 [-0.939, 0.584], mean_best_reward: --\n",
      " 55720/100000: episode: 1737, duration: 0.232s, episode steps: 47, steps per second: 202, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.150 [-0.642, 1.532], mean_best_reward: --\n",
      " 55758/100000: episode: 1738, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.077 [-0.548, 1.413], mean_best_reward: --\n",
      " 55825/100000: episode: 1739, duration: 0.333s, episode steps: 67, steps per second: 201, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.102 [-1.038, 0.814], mean_best_reward: --\n",
      " 55842/100000: episode: 1740, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.090 [-1.235, 0.643], mean_best_reward: --\n",
      " 55892/100000: episode: 1741, duration: 0.295s, episode steps: 50, steps per second: 170, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.084 [-0.807, 1.589], mean_best_reward: --\n",
      " 55931/100000: episode: 1742, duration: 0.236s, episode steps: 39, steps per second: 166, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.142 [-0.631, 1.104], mean_best_reward: --\n",
      " 55982/100000: episode: 1743, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.070 [-0.753, 0.403], mean_best_reward: --\n",
      " 56035/100000: episode: 1744, duration: 0.349s, episode steps: 53, steps per second: 152, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.090 [-0.555, 0.945], mean_best_reward: --\n",
      " 56078/100000: episode: 1745, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.107 [-0.669, 0.862], mean_best_reward: --\n",
      " 56121/100000: episode: 1746, duration: 0.245s, episode steps: 43, steps per second: 175, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.104 [-0.364, 0.957], mean_best_reward: --\n",
      " 56145/100000: episode: 1747, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.117 [-1.066, 0.571], mean_best_reward: --\n",
      " 56194/100000: episode: 1748, duration: 0.293s, episode steps: 49, steps per second: 167, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.115 [-1.044, 0.647], mean_best_reward: --\n",
      " 56263/100000: episode: 1749, duration: 0.392s, episode steps: 69, steps per second: 176, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.038 [-1.711, 0.784], mean_best_reward: --\n",
      " 56313/100000: episode: 1750, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.074 [-1.014, 0.439], mean_best_reward: --\n",
      " 56336/100000: episode: 1751, duration: 0.163s, episode steps: 23, steps per second: 141, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.083 [-1.603, 0.758], mean_best_reward: 107.500000\n",
      " 56424/100000: episode: 1752, duration: 0.410s, episode steps: 88, steps per second: 214, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.098 [-0.652, 0.832], mean_best_reward: --\n",
      " 56467/100000: episode: 1753, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.051 [-1.037, 0.802], mean_best_reward: --\n",
      " 56510/100000: episode: 1754, duration: 0.197s, episode steps: 43, steps per second: 218, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.139 [-1.068, 0.401], mean_best_reward: --\n",
      " 56546/100000: episode: 1755, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.144 [-0.402, 1.687], mean_best_reward: --\n",
      " 56572/100000: episode: 1756, duration: 0.122s, episode steps: 26, steps per second: 214, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.002 [-1.372, 1.921], mean_best_reward: --\n",
      " 56621/100000: episode: 1757, duration: 0.230s, episode steps: 49, steps per second: 213, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.108 [-1.286, 0.717], mean_best_reward: --\n",
      " 56672/100000: episode: 1758, duration: 0.234s, episode steps: 51, steps per second: 218, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.091 [-1.037, 0.364], mean_best_reward: --\n",
      " 56716/100000: episode: 1759, duration: 0.204s, episode steps: 44, steps per second: 215, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.289, 0.381], mean_best_reward: --\n",
      " 56770/100000: episode: 1760, duration: 0.251s, episode steps: 54, steps per second: 215, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.074 [-0.525, 1.102], mean_best_reward: --\n",
      " 56795/100000: episode: 1761, duration: 0.116s, episode steps: 25, steps per second: 215, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.011 [-1.702, 1.207], mean_best_reward: --\n",
      " 56811/100000: episode: 1762, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.064 [-1.496, 0.980], mean_best_reward: --\n",
      " 56834/100000: episode: 1763, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.086 [-0.569, 1.058], mean_best_reward: --\n",
      " 56908/100000: episode: 1764, duration: 0.343s, episode steps: 74, steps per second: 216, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.101 [-1.296, 0.735], mean_best_reward: --\n",
      " 56926/100000: episode: 1765, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.100 [-1.292, 0.744], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56953/100000: episode: 1766, duration: 0.128s, episode steps: 27, steps per second: 212, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.096 [-0.614, 0.902], mean_best_reward: --\n",
      " 57005/100000: episode: 1767, duration: 0.242s, episode steps: 52, steps per second: 214, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.114 [-1.003, 0.678], mean_best_reward: --\n",
      " 57020/100000: episode: 1768, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-2.166, 1.338], mean_best_reward: --\n",
      " 57063/100000: episode: 1769, duration: 0.200s, episode steps: 43, steps per second: 215, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.076 [-1.069, 0.994], mean_best_reward: --\n",
      " 57089/100000: episode: 1770, duration: 0.122s, episode steps: 26, steps per second: 212, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.433, 0.655], mean_best_reward: --\n",
      " 57115/100000: episode: 1771, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.082 [-1.398, 0.755], mean_best_reward: --\n",
      " 57142/100000: episode: 1772, duration: 0.127s, episode steps: 27, steps per second: 213, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.100 [-1.179, 0.744], mean_best_reward: --\n",
      " 57174/100000: episode: 1773, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.121 [-0.646, 0.244], mean_best_reward: --\n",
      " 57221/100000: episode: 1774, duration: 0.217s, episode steps: 47, steps per second: 216, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.146 [-0.594, 0.983], mean_best_reward: --\n",
      " 57272/100000: episode: 1775, duration: 0.234s, episode steps: 51, steps per second: 218, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.130 [-0.509, 1.246], mean_best_reward: --\n",
      " 57292/100000: episode: 1776, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.300, 0.792], mean_best_reward: --\n",
      " 57331/100000: episode: 1777, duration: 0.181s, episode steps: 39, steps per second: 215, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.165 [-1.018, 1.150], mean_best_reward: --\n",
      " 57431/100000: episode: 1778, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.095 [-1.668, 0.937], mean_best_reward: --\n",
      " 57476/100000: episode: 1779, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.048 [-2.114, 0.960], mean_best_reward: --\n",
      " 57498/100000: episode: 1780, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.108 [-1.885, 0.951], mean_best_reward: --\n",
      " 57544/100000: episode: 1781, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.069 [-0.907, 0.583], mean_best_reward: --\n",
      " 57596/100000: episode: 1782, duration: 0.267s, episode steps: 52, steps per second: 194, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.314, 0.437], mean_best_reward: --\n",
      " 57695/100000: episode: 1783, duration: 0.505s, episode steps: 99, steps per second: 196, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.118 [-1.352, 0.957], mean_best_reward: --\n",
      " 57739/100000: episode: 1784, duration: 0.253s, episode steps: 44, steps per second: 174, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.096 [-0.463, 1.179], mean_best_reward: --\n",
      " 57771/100000: episode: 1785, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.156 [-1.476, 0.925], mean_best_reward: --\n",
      " 57823/100000: episode: 1786, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.145 [-0.463, 1.086], mean_best_reward: --\n",
      " 57848/100000: episode: 1787, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.097 [-1.102, 0.572], mean_best_reward: --\n",
      " 57939/100000: episode: 1788, duration: 0.472s, episode steps: 91, steps per second: 193, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.077 [-1.318, 1.145], mean_best_reward: --\n",
      " 57990/100000: episode: 1789, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.118 [-0.405, 0.758], mean_best_reward: --\n",
      " 58010/100000: episode: 1790, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.091 [-1.029, 0.569], mean_best_reward: --\n",
      " 58053/100000: episode: 1791, duration: 0.215s, episode steps: 43, steps per second: 200, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.143 [-0.638, 1.206], mean_best_reward: --\n",
      " 58081/100000: episode: 1792, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.107 [-0.903, 0.214], mean_best_reward: --\n",
      " 58109/100000: episode: 1793, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.116 [-0.781, 0.418], mean_best_reward: --\n",
      " 58235/100000: episode: 1794, duration: 0.662s, episode steps: 126, steps per second: 190, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.475 [-2.401, 1.035], mean_best_reward: --\n",
      " 58321/100000: episode: 1795, duration: 0.422s, episode steps: 86, steps per second: 204, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.280, 1.384], mean_best_reward: --\n",
      " 58360/100000: episode: 1796, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.103 [-1.153, 0.581], mean_best_reward: --\n",
      " 58426/100000: episode: 1797, duration: 0.340s, episode steps: 66, steps per second: 194, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.086 [-0.980, 0.512], mean_best_reward: --\n",
      " 58471/100000: episode: 1798, duration: 0.228s, episode steps: 45, steps per second: 198, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.098 [-0.784, 0.575], mean_best_reward: --\n",
      " 58496/100000: episode: 1799, duration: 0.147s, episode steps: 25, steps per second: 171, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.121 [-0.995, 0.580], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58525/100000: episode: 1800, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.063 [-1.890, 1.028], mean_best_reward: --\n",
      " 58575/100000: episode: 1801, duration: 0.306s, episode steps: 50, steps per second: 163, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.123 [-0.250, 0.975], mean_best_reward: 101.000000\n",
      " 58630/100000: episode: 1802, duration: 0.274s, episode steps: 55, steps per second: 201, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.089 [-0.925, 0.682], mean_best_reward: --\n",
      " 58664/100000: episode: 1803, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.086 [-0.951, 1.940], mean_best_reward: --\n",
      " 58699/100000: episode: 1804, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.085 [-1.528, 0.635], mean_best_reward: --\n",
      " 58760/100000: episode: 1805, duration: 0.403s, episode steps: 61, steps per second: 151, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.147 [-1.299, 0.477], mean_best_reward: --\n",
      " 58793/100000: episode: 1806, duration: 0.208s, episode steps: 33, steps per second: 158, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.093 [-1.424, 0.637], mean_best_reward: --\n",
      " 58838/100000: episode: 1807, duration: 0.226s, episode steps: 45, steps per second: 200, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.044 [-1.221, 0.848], mean_best_reward: --\n",
      " 58959/100000: episode: 1808, duration: 0.694s, episode steps: 121, steps per second: 174, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.017 [-0.982, 1.171], mean_best_reward: --\n",
      " 58989/100000: episode: 1809, duration: 0.144s, episode steps: 30, steps per second: 208, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.105 [-0.426, 0.924], mean_best_reward: --\n",
      " 59024/100000: episode: 1810, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.014 [-1.001, 1.427], mean_best_reward: --\n",
      " 59097/100000: episode: 1811, duration: 0.473s, episode steps: 73, steps per second: 154, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.067 [-0.910, 0.612], mean_best_reward: --\n",
      " 59157/100000: episode: 1812, duration: 0.343s, episode steps: 60, steps per second: 175, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.054 [-0.778, 1.533], mean_best_reward: --\n",
      " 59205/100000: episode: 1813, duration: 0.261s, episode steps: 48, steps per second: 184, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.075 [-1.300, 0.801], mean_best_reward: --\n",
      " 59223/100000: episode: 1814, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.083 [-1.422, 0.764], mean_best_reward: --\n",
      " 59304/100000: episode: 1815, duration: 0.465s, episode steps: 81, steps per second: 174, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.025 [-0.912, 1.119], mean_best_reward: --\n",
      " 59337/100000: episode: 1816, duration: 0.153s, episode steps: 33, steps per second: 216, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.078 [-1.288, 0.631], mean_best_reward: --\n",
      " 59396/100000: episode: 1817, duration: 0.282s, episode steps: 59, steps per second: 209, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.043 [-0.824, 0.888], mean_best_reward: --\n",
      " 59528/100000: episode: 1818, duration: 0.621s, episode steps: 132, steps per second: 213, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.059 [-0.617, 0.843], mean_best_reward: --\n",
      " 59558/100000: episode: 1819, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.061 [-0.596, 1.293], mean_best_reward: --\n",
      " 59596/100000: episode: 1820, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.283, 0.449], mean_best_reward: --\n",
      " 59664/100000: episode: 1821, duration: 0.330s, episode steps: 68, steps per second: 206, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.093 [-0.862, 0.421], mean_best_reward: --\n",
      " 59689/100000: episode: 1822, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.130 [-0.996, 0.550], mean_best_reward: --\n",
      " 59722/100000: episode: 1823, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.124 [-1.189, 0.756], mean_best_reward: --\n",
      " 59770/100000: episode: 1824, duration: 0.228s, episode steps: 48, steps per second: 211, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-1.336, 0.942], mean_best_reward: --\n",
      " 59851/100000: episode: 1825, duration: 0.375s, episode steps: 81, steps per second: 216, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.129 [-0.551, 0.841], mean_best_reward: --\n",
      " 59884/100000: episode: 1826, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.078 [-0.619, 1.284], mean_best_reward: --\n",
      " 59933/100000: episode: 1827, duration: 0.234s, episode steps: 49, steps per second: 210, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.074 [-1.095, 0.576], mean_best_reward: --\n",
      " 59970/100000: episode: 1828, duration: 0.172s, episode steps: 37, steps per second: 215, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.004 [-1.184, 0.928], mean_best_reward: --\n",
      " 60013/100000: episode: 1829, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: 0.067 [-0.973, 2.114], mean_best_reward: --\n",
      " 60057/100000: episode: 1830, duration: 0.207s, episode steps: 44, steps per second: 213, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.099 [-1.156, 0.529], mean_best_reward: --\n",
      " 60100/100000: episode: 1831, duration: 0.213s, episode steps: 43, steps per second: 202, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.131 [-1.173, 0.944], mean_best_reward: --\n",
      " 60167/100000: episode: 1832, duration: 0.312s, episode steps: 67, steps per second: 215, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.107 [-0.599, 0.915], mean_best_reward: --\n",
      " 60215/100000: episode: 1833, duration: 0.223s, episode steps: 48, steps per second: 215, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.082, 0.547], mean_best_reward: --\n",
      " 60242/100000: episode: 1834, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.125 [-0.599, 0.925], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60266/100000: episode: 1835, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.931, 1.405], mean_best_reward: --\n",
      " 60304/100000: episode: 1836, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.107 [-0.732, 1.170], mean_best_reward: --\n",
      " 60336/100000: episode: 1837, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.138 [-1.323, 0.686], mean_best_reward: --\n",
      " 60368/100000: episode: 1838, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.099 [-0.587, 1.491], mean_best_reward: --\n",
      " 60402/100000: episode: 1839, duration: 0.196s, episode steps: 34, steps per second: 173, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.115 [-0.389, 0.812], mean_best_reward: --\n",
      " 60454/100000: episode: 1840, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.120 [-0.478, 0.974], mean_best_reward: --\n",
      " 60495/100000: episode: 1841, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.070 [-1.173, 0.404], mean_best_reward: --\n",
      " 60544/100000: episode: 1842, duration: 0.242s, episode steps: 49, steps per second: 202, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.109 [-0.978, 0.464], mean_best_reward: --\n",
      " 60562/100000: episode: 1843, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.126 [-0.570, 1.097], mean_best_reward: --\n",
      " 60593/100000: episode: 1844, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.120 [-0.540, 0.891], mean_best_reward: --\n",
      " 60626/100000: episode: 1845, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.041 [-0.943, 0.583], mean_best_reward: --\n",
      " 60661/100000: episode: 1846, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.074 [-0.624, 1.097], mean_best_reward: --\n",
      " 60731/100000: episode: 1847, duration: 0.344s, episode steps: 70, steps per second: 204, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.056 [-0.813, 1.184], mean_best_reward: --\n",
      " 60761/100000: episode: 1848, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.110 [-0.374, 1.121], mean_best_reward: --\n",
      " 60797/100000: episode: 1849, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.074 [-0.603, 0.987], mean_best_reward: --\n",
      " 60830/100000: episode: 1850, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.107 [-0.364, 0.882], mean_best_reward: --\n",
      " 60889/100000: episode: 1851, duration: 0.325s, episode steps: 59, steps per second: 182, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.027 [-1.778, 1.191], mean_best_reward: 127.500000\n",
      " 60976/100000: episode: 1852, duration: 0.483s, episode steps: 87, steps per second: 180, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.052 [-1.777, 0.924], mean_best_reward: --\n",
      " 61008/100000: episode: 1853, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.083 [-1.259, 0.569], mean_best_reward: --\n",
      " 61037/100000: episode: 1854, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.112 [-0.557, 1.122], mean_best_reward: --\n",
      " 61104/100000: episode: 1855, duration: 0.349s, episode steps: 67, steps per second: 192, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.040 [-0.895, 0.407], mean_best_reward: --\n",
      " 61135/100000: episode: 1856, duration: 0.150s, episode steps: 31, steps per second: 207, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.079 [-0.948, 1.824], mean_best_reward: --\n",
      " 61154/100000: episode: 1857, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.083 [-0.940, 1.609], mean_best_reward: --\n",
      " 61199/100000: episode: 1858, duration: 0.220s, episode steps: 45, steps per second: 205, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.093 [-1.706, 0.661], mean_best_reward: --\n",
      " 61273/100000: episode: 1859, duration: 0.355s, episode steps: 74, steps per second: 209, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.102 [-0.968, 0.587], mean_best_reward: --\n",
      " 61316/100000: episode: 1860, duration: 0.203s, episode steps: 43, steps per second: 211, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.106 [-0.577, 0.960], mean_best_reward: --\n",
      " 61366/100000: episode: 1861, duration: 0.239s, episode steps: 50, steps per second: 210, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.151 [-1.322, 0.800], mean_best_reward: --\n",
      " 61398/100000: episode: 1862, duration: 0.151s, episode steps: 32, steps per second: 211, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.102 [-0.430, 0.925], mean_best_reward: --\n",
      " 61435/100000: episode: 1863, duration: 0.175s, episode steps: 37, steps per second: 211, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.087 [-1.282, 0.415], mean_best_reward: --\n",
      " 61470/100000: episode: 1864, duration: 0.188s, episode steps: 35, steps per second: 187, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.131 [-0.784, 0.375], mean_best_reward: --\n",
      " 61500/100000: episode: 1865, duration: 0.143s, episode steps: 30, steps per second: 210, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.093 [-0.769, 1.145], mean_best_reward: --\n",
      " 61525/100000: episode: 1866, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.039 [-1.298, 0.969], mean_best_reward: --\n",
      " 61559/100000: episode: 1867, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.407, 1.057], mean_best_reward: --\n",
      " 61583/100000: episode: 1868, duration: 0.114s, episode steps: 24, steps per second: 210, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-1.011, 0.605], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61629/100000: episode: 1869, duration: 0.229s, episode steps: 46, steps per second: 201, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.137 [-1.052, 1.175], mean_best_reward: --\n",
      " 61646/100000: episode: 1870, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.076 [-1.491, 0.819], mean_best_reward: --\n",
      " 61703/100000: episode: 1871, duration: 0.370s, episode steps: 57, steps per second: 154, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.166 [-1.083, 0.715], mean_best_reward: --\n",
      " 61790/100000: episode: 1872, duration: 0.429s, episode steps: 87, steps per second: 203, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.073 [-0.832, 0.916], mean_best_reward: --\n",
      " 61883/100000: episode: 1873, duration: 0.462s, episode steps: 93, steps per second: 201, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.117 [-1.495, 1.437], mean_best_reward: --\n",
      " 61903/100000: episode: 1874, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.391, 0.840], mean_best_reward: --\n",
      " 61949/100000: episode: 1875, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.632, 1.309], mean_best_reward: --\n",
      " 61995/100000: episode: 1876, duration: 0.233s, episode steps: 46, steps per second: 198, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.073 [-1.265, 0.507], mean_best_reward: --\n",
      " 62032/100000: episode: 1877, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.085 [-0.836, 0.534], mean_best_reward: --\n",
      " 62050/100000: episode: 1878, duration: 0.093s, episode steps: 18, steps per second: 195, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.094 [-0.983, 1.814], mean_best_reward: --\n",
      " 62087/100000: episode: 1879, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.113 [-0.371, 0.894], mean_best_reward: --\n",
      " 62105/100000: episode: 1880, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.078 [-0.760, 1.508], mean_best_reward: --\n",
      " 62125/100000: episode: 1881, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.069 [-0.943, 1.540], mean_best_reward: --\n",
      " 62179/100000: episode: 1882, duration: 0.264s, episode steps: 54, steps per second: 205, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.124 [-1.005, 0.451], mean_best_reward: --\n",
      " 62201/100000: episode: 1883, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.120, 0.621], mean_best_reward: --\n",
      " 62257/100000: episode: 1884, duration: 0.286s, episode steps: 56, steps per second: 195, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.070 [-0.782, 1.232], mean_best_reward: --\n",
      " 62287/100000: episode: 1885, duration: 0.148s, episode steps: 30, steps per second: 202, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.080 [-1.370, 0.758], mean_best_reward: --\n",
      " 62312/100000: episode: 1886, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.053 [-1.777, 1.163], mean_best_reward: --\n",
      " 62342/100000: episode: 1887, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.099 [-1.254, 0.389], mean_best_reward: --\n",
      " 62399/100000: episode: 1888, duration: 0.287s, episode steps: 57, steps per second: 198, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.143 [-1.674, 0.891], mean_best_reward: --\n",
      " 62457/100000: episode: 1889, duration: 0.307s, episode steps: 58, steps per second: 189, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.146 [-0.478, 1.025], mean_best_reward: --\n",
      " 62511/100000: episode: 1890, duration: 0.278s, episode steps: 54, steps per second: 195, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.091 [-0.731, 0.444], mean_best_reward: --\n",
      " 62588/100000: episode: 1891, duration: 0.391s, episode steps: 77, steps per second: 197, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.122 [-0.919, 0.468], mean_best_reward: --\n",
      " 62620/100000: episode: 1892, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.120 [-0.862, 0.366], mean_best_reward: --\n",
      " 62650/100000: episode: 1893, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.081 [-0.840, 0.436], mean_best_reward: --\n",
      " 62715/100000: episode: 1894, duration: 0.320s, episode steps: 65, steps per second: 203, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.147 [-0.538, 1.121], mean_best_reward: --\n",
      " 62837/100000: episode: 1895, duration: 0.595s, episode steps: 122, steps per second: 205, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.015 [-1.043, 1.013], mean_best_reward: --\n",
      " 62914/100000: episode: 1896, duration: 0.365s, episode steps: 77, steps per second: 211, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.017 [-1.699, 0.656], mean_best_reward: --\n",
      " 62936/100000: episode: 1897, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.497, 0.935], mean_best_reward: --\n",
      " 62969/100000: episode: 1898, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.093 [-0.586, 1.265], mean_best_reward: --\n",
      " 62996/100000: episode: 1899, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.091 [-1.160, 0.418], mean_best_reward: --\n",
      " 63068/100000: episode: 1900, duration: 0.357s, episode steps: 72, steps per second: 202, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.105 [-1.600, 0.641], mean_best_reward: --\n",
      " 63114/100000: episode: 1901, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.104 [-0.961, 0.442], mean_best_reward: 104.000000\n",
      " 63162/100000: episode: 1902, duration: 0.224s, episode steps: 48, steps per second: 214, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.176 [-1.915, 0.825], mean_best_reward: --\n",
      " 63192/100000: episode: 1903, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.117 [-0.402, 1.452], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63224/100000: episode: 1904, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.072 [-0.456, 1.104], mean_best_reward: --\n",
      " 63274/100000: episode: 1905, duration: 0.246s, episode steps: 50, steps per second: 203, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.144 [-0.404, 0.949], mean_best_reward: --\n",
      " 63315/100000: episode: 1906, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.104 [-1.398, 0.571], mean_best_reward: --\n",
      " 63357/100000: episode: 1907, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.051 [-1.545, 0.814], mean_best_reward: --\n",
      " 63416/100000: episode: 1908, duration: 0.294s, episode steps: 59, steps per second: 201, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.031 [-2.459, 1.562], mean_best_reward: --\n",
      " 63469/100000: episode: 1909, duration: 0.258s, episode steps: 53, steps per second: 206, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.055 [-0.742, 1.322], mean_best_reward: --\n",
      " 63512/100000: episode: 1910, duration: 0.211s, episode steps: 43, steps per second: 204, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.072 [-1.241, 0.803], mean_best_reward: --\n",
      " 63536/100000: episode: 1911, duration: 0.114s, episode steps: 24, steps per second: 211, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.107 [-1.351, 0.591], mean_best_reward: --\n",
      " 63560/100000: episode: 1912, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.075 [-1.554, 0.758], mean_best_reward: --\n",
      " 63579/100000: episode: 1913, duration: 0.126s, episode steps: 19, steps per second: 151, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.055 [-1.453, 0.836], mean_best_reward: --\n",
      " 63630/100000: episode: 1914, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.087 [-0.780, 0.457], mean_best_reward: --\n",
      " 63726/100000: episode: 1915, duration: 0.491s, episode steps: 96, steps per second: 196, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.207 [-1.019, 1.476], mean_best_reward: --\n",
      " 63786/100000: episode: 1916, duration: 0.319s, episode steps: 60, steps per second: 188, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.192 [-1.644, 0.577], mean_best_reward: --\n",
      " 63835/100000: episode: 1917, duration: 0.242s, episode steps: 49, steps per second: 203, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.085 [-1.204, 0.354], mean_best_reward: --\n",
      " 63849/100000: episode: 1918, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.086 [-0.992, 1.464], mean_best_reward: --\n",
      " 63873/100000: episode: 1919, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.051 [-1.385, 0.989], mean_best_reward: --\n",
      " 63934/100000: episode: 1920, duration: 0.309s, episode steps: 61, steps per second: 197, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.128 [-0.683, 1.179], mean_best_reward: --\n",
      " 63978/100000: episode: 1921, duration: 0.244s, episode steps: 44, steps per second: 181, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.873, 0.844], mean_best_reward: --\n",
      " 64006/100000: episode: 1922, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.107 [-1.225, 0.778], mean_best_reward: --\n",
      " 64037/100000: episode: 1923, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.136 [-1.009, 0.616], mean_best_reward: --\n",
      " 64060/100000: episode: 1924, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.094 [-1.219, 0.619], mean_best_reward: --\n",
      " 64073/100000: episode: 1925, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.079 [-0.645, 1.161], mean_best_reward: --\n",
      " 64127/100000: episode: 1926, duration: 0.271s, episode steps: 54, steps per second: 199, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.062 [-1.075, 0.808], mean_best_reward: --\n",
      " 64160/100000: episode: 1927, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.079 [-0.406, 1.025], mean_best_reward: --\n",
      " 64215/100000: episode: 1928, duration: 0.268s, episode steps: 55, steps per second: 205, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.155 [-0.570, 1.138], mean_best_reward: --\n",
      " 64226/100000: episode: 1929, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.122 [-1.852, 1.148], mean_best_reward: --\n",
      " 64302/100000: episode: 1930, duration: 0.411s, episode steps: 76, steps per second: 185, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.110 [-1.187, 0.564], mean_best_reward: --\n",
      " 64326/100000: episode: 1931, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.098 [-1.306, 0.434], mean_best_reward: --\n",
      " 64410/100000: episode: 1932, duration: 0.443s, episode steps: 84, steps per second: 190, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.042 [-0.979, 1.247], mean_best_reward: --\n",
      " 64470/100000: episode: 1933, duration: 0.300s, episode steps: 60, steps per second: 200, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.016 [-1.037, 0.637], mean_best_reward: --\n",
      " 64587/100000: episode: 1934, duration: 0.688s, episode steps: 117, steps per second: 170, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.039 [-0.785, 1.028], mean_best_reward: --\n",
      " 64678/100000: episode: 1935, duration: 0.491s, episode steps: 91, steps per second: 185, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.050 [-2.238, 0.986], mean_best_reward: --\n",
      " 64780/100000: episode: 1936, duration: 0.541s, episode steps: 102, steps per second: 189, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.209 [-1.433, 0.700], mean_best_reward: --\n",
      " 64825/100000: episode: 1937, duration: 0.249s, episode steps: 45, steps per second: 181, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.114 [-0.387, 0.992], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64916/100000: episode: 1938, duration: 0.441s, episode steps: 91, steps per second: 206, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.082 [-0.896, 1.406], mean_best_reward: --\n",
      " 64941/100000: episode: 1939, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.104 [-0.582, 1.032], mean_best_reward: --\n",
      " 64958/100000: episode: 1940, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.103 [-1.391, 0.606], mean_best_reward: --\n",
      " 64996/100000: episode: 1941, duration: 0.215s, episode steps: 38, steps per second: 176, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.100 [-0.970, 0.274], mean_best_reward: --\n",
      " 65034/100000: episode: 1942, duration: 0.182s, episode steps: 38, steps per second: 208, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.100 [-0.914, 0.427], mean_best_reward: --\n",
      " 65089/100000: episode: 1943, duration: 0.313s, episode steps: 55, steps per second: 176, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.058 [-1.142, 0.635], mean_best_reward: --\n",
      " 65122/100000: episode: 1944, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.145 [-0.396, 0.728], mean_best_reward: --\n",
      " 65203/100000: episode: 1945, duration: 0.404s, episode steps: 81, steps per second: 200, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.009 [-0.612, 0.891], mean_best_reward: --\n",
      " 65238/100000: episode: 1946, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.079 [-0.988, 0.431], mean_best_reward: --\n",
      " 65357/100000: episode: 1947, duration: 0.556s, episode steps: 119, steps per second: 214, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.143 [-1.086, 1.156], mean_best_reward: --\n",
      " 65387/100000: episode: 1948, duration: 0.144s, episode steps: 30, steps per second: 209, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.050 [-1.144, 1.870], mean_best_reward: --\n",
      " 65457/100000: episode: 1949, duration: 0.330s, episode steps: 70, steps per second: 212, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.007 [-1.039, 0.739], mean_best_reward: --\n",
      " 65488/100000: episode: 1950, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.084 [-0.906, 0.415], mean_best_reward: --\n",
      " 65517/100000: episode: 1951, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.105 [-0.974, 0.410], mean_best_reward: 94.000000\n",
      " 65534/100000: episode: 1952, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.096 [-1.120, 0.620], mean_best_reward: --\n",
      " 65569/100000: episode: 1953, duration: 0.168s, episode steps: 35, steps per second: 209, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.092 [-1.712, 0.663], mean_best_reward: --\n",
      " 65595/100000: episode: 1954, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.007 [-1.934, 1.396], mean_best_reward: --\n",
      " 65685/100000: episode: 1955, duration: 0.418s, episode steps: 90, steps per second: 215, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.010 [-1.030, 0.747], mean_best_reward: --\n",
      " 65761/100000: episode: 1956, duration: 0.359s, episode steps: 76, steps per second: 212, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.098 [-1.111, 1.150], mean_best_reward: --\n",
      " 65791/100000: episode: 1957, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-0.971, 0.364], mean_best_reward: --\n",
      " 65846/100000: episode: 1958, duration: 0.261s, episode steps: 55, steps per second: 211, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.158 [-1.455, 0.606], mean_best_reward: --\n",
      " 65866/100000: episode: 1959, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-0.985, 0.553], mean_best_reward: --\n",
      " 65889/100000: episode: 1960, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.068 [-0.586, 1.344], mean_best_reward: --\n",
      " 65908/100000: episode: 1961, duration: 0.094s, episode steps: 19, steps per second: 203, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.092 [-1.248, 0.796], mean_best_reward: --\n",
      " 65946/100000: episode: 1962, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.247, 0.397], mean_best_reward: --\n",
      " 66019/100000: episode: 1963, duration: 0.358s, episode steps: 73, steps per second: 204, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.041 [-1.189, 0.839], mean_best_reward: --\n",
      " 66042/100000: episode: 1964, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.111 [-0.818, 1.806], mean_best_reward: --\n",
      " 66084/100000: episode: 1965, duration: 0.202s, episode steps: 42, steps per second: 208, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.134 [-1.199, 0.457], mean_best_reward: --\n",
      " 66132/100000: episode: 1966, duration: 0.233s, episode steps: 48, steps per second: 206, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.165, 0.724], mean_best_reward: --\n",
      " 66179/100000: episode: 1967, duration: 0.226s, episode steps: 47, steps per second: 208, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.054 [-1.193, 0.624], mean_best_reward: --\n",
      " 66214/100000: episode: 1968, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.142 [-0.389, 1.528], mean_best_reward: --\n",
      " 66237/100000: episode: 1969, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.120 [-0.396, 1.163], mean_best_reward: --\n",
      " 66260/100000: episode: 1970, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.070 [-1.323, 0.783], mean_best_reward: --\n",
      " 66286/100000: episode: 1971, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.085 [-0.949, 0.566], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66344/100000: episode: 1972, duration: 0.284s, episode steps: 58, steps per second: 204, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.140 [-1.021, 0.683], mean_best_reward: --\n",
      " 66424/100000: episode: 1973, duration: 0.377s, episode steps: 80, steps per second: 212, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.148 [-0.647, 1.434], mean_best_reward: --\n",
      " 66461/100000: episode: 1974, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.038 [-0.961, 0.584], mean_best_reward: --\n",
      " 66548/100000: episode: 1975, duration: 0.403s, episode steps: 87, steps per second: 216, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.108 [-0.743, 1.104], mean_best_reward: --\n",
      " 66612/100000: episode: 1976, duration: 0.307s, episode steps: 64, steps per second: 208, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-0.998, 1.238], mean_best_reward: --\n",
      " 66646/100000: episode: 1977, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.083 [-1.657, 0.778], mean_best_reward: --\n",
      " 66669/100000: episode: 1978, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.063 [-2.339, 1.404], mean_best_reward: --\n",
      " 66714/100000: episode: 1979, duration: 0.212s, episode steps: 45, steps per second: 212, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.085 [-0.878, 0.550], mean_best_reward: --\n",
      " 66758/100000: episode: 1980, duration: 0.211s, episode steps: 44, steps per second: 209, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.170 [-0.764, 1.298], mean_best_reward: --\n",
      " 66793/100000: episode: 1981, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.081 [-1.377, 0.575], mean_best_reward: --\n",
      " 66852/100000: episode: 1982, duration: 0.286s, episode steps: 59, steps per second: 206, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.135 [-0.934, 0.529], mean_best_reward: --\n",
      " 66866/100000: episode: 1983, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.767, 1.310], mean_best_reward: --\n",
      " 66892/100000: episode: 1984, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.059 [-0.591, 1.234], mean_best_reward: --\n",
      " 67044/100000: episode: 1985, duration: 0.702s, episode steps: 152, steps per second: 216, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.315 [-2.750, 1.009], mean_best_reward: --\n",
      " 67078/100000: episode: 1986, duration: 0.160s, episode steps: 34, steps per second: 213, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.052 [-0.868, 1.174], mean_best_reward: --\n",
      " 67089/100000: episode: 1987, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.120 [-1.871, 1.172], mean_best_reward: --\n",
      " 67162/100000: episode: 1988, duration: 0.340s, episode steps: 73, steps per second: 215, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.103 [-0.687, 1.085], mean_best_reward: --\n",
      " 67198/100000: episode: 1989, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.064 [-0.988, 0.415], mean_best_reward: --\n",
      " 67272/100000: episode: 1990, duration: 0.352s, episode steps: 74, steps per second: 210, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.748, 1.216], mean_best_reward: --\n",
      " 67325/100000: episode: 1991, duration: 0.250s, episode steps: 53, steps per second: 212, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.020 [-0.739, 1.309], mean_best_reward: --\n",
      " 67345/100000: episode: 1992, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.072 [-0.819, 1.237], mean_best_reward: --\n",
      " 67383/100000: episode: 1993, duration: 0.232s, episode steps: 38, steps per second: 164, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.096 [-0.988, 0.607], mean_best_reward: --\n",
      " 67406/100000: episode: 1994, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.080 [-1.288, 0.599], mean_best_reward: --\n",
      " 67447/100000: episode: 1995, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.415 [0.000, 1.000], mean observation: 0.011 [-1.412, 2.297], mean_best_reward: --\n",
      " 67485/100000: episode: 1996, duration: 0.182s, episode steps: 38, steps per second: 209, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.133 [-1.184, 0.730], mean_best_reward: --\n",
      " 67525/100000: episode: 1997, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.129 [-0.710, 0.439], mean_best_reward: --\n",
      " 67546/100000: episode: 1998, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.101 [-0.883, 0.371], mean_best_reward: --\n",
      " 67598/100000: episode: 1999, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.100 [-0.814, 0.594], mean_best_reward: --\n",
      " 67637/100000: episode: 2000, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.002 [-1.119, 1.362], mean_best_reward: --\n",
      " 67678/100000: episode: 2001, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.148 [-0.266, 0.784], mean_best_reward: 105.000000\n",
      " 67727/100000: episode: 2002, duration: 0.260s, episode steps: 49, steps per second: 189, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: 0.034 [-1.354, 2.144], mean_best_reward: --\n",
      " 67844/100000: episode: 2003, duration: 0.579s, episode steps: 117, steps per second: 202, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.084 [-1.032, 1.022], mean_best_reward: --\n",
      " 67923/100000: episode: 2004, duration: 0.394s, episode steps: 79, steps per second: 200, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.097 [-0.985, 1.059], mean_best_reward: --\n",
      " 67944/100000: episode: 2005, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.115 [-0.600, 0.969], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67973/100000: episode: 2006, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.091 [-0.617, 1.310], mean_best_reward: --\n",
      " 68026/100000: episode: 2007, duration: 0.263s, episode steps: 53, steps per second: 202, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.115 [-0.745, 0.346], mean_best_reward: --\n",
      " 68116/100000: episode: 2008, duration: 0.476s, episode steps: 90, steps per second: 189, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.020 [-1.253, 0.973], mean_best_reward: --\n",
      " 68188/100000: episode: 2009, duration: 0.356s, episode steps: 72, steps per second: 202, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.059 [-0.761, 1.220], mean_best_reward: --\n",
      " 68334/100000: episode: 2010, duration: 0.758s, episode steps: 146, steps per second: 193, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.021 [-0.930, 1.147], mean_best_reward: --\n",
      " 68384/100000: episode: 2011, duration: 0.274s, episode steps: 50, steps per second: 183, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.066 [-1.684, 0.474], mean_best_reward: --\n",
      " 68411/100000: episode: 2012, duration: 0.128s, episode steps: 27, steps per second: 210, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.118 [-0.791, 1.752], mean_best_reward: --\n",
      " 68424/100000: episode: 2013, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.101 [-1.016, 1.596], mean_best_reward: --\n",
      " 68480/100000: episode: 2014, duration: 0.278s, episode steps: 56, steps per second: 201, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.131 [-0.983, 0.981], mean_best_reward: --\n",
      " 68543/100000: episode: 2015, duration: 0.315s, episode steps: 63, steps per second: 200, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: -0.087 [-2.133, 1.148], mean_best_reward: --\n",
      " 68572/100000: episode: 2016, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.096 [-1.272, 0.402], mean_best_reward: --\n",
      " 68611/100000: episode: 2017, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.085 [-1.196, 0.817], mean_best_reward: --\n",
      " 68755/100000: episode: 2018, duration: 0.707s, episode steps: 144, steps per second: 204, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.093 [-1.154, 0.785], mean_best_reward: --\n",
      " 68776/100000: episode: 2019, duration: 0.102s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.061 [-1.925, 1.039], mean_best_reward: --\n",
      " 68825/100000: episode: 2020, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.012 [-1.204, 1.609], mean_best_reward: --\n",
      " 68845/100000: episode: 2021, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.271, 0.806], mean_best_reward: --\n",
      " 68898/100000: episode: 2022, duration: 0.248s, episode steps: 53, steps per second: 213, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.112 [-0.909, 0.472], mean_best_reward: --\n",
      " 68917/100000: episode: 2023, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.122 [-1.307, 0.802], mean_best_reward: --\n",
      " 68928/100000: episode: 2024, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.134 [-1.800, 0.958], mean_best_reward: --\n",
      " 68966/100000: episode: 2025, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.113 [-0.352, 0.877], mean_best_reward: --\n",
      " 69006/100000: episode: 2026, duration: 0.205s, episode steps: 40, steps per second: 196, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.132 [-0.541, 0.920], mean_best_reward: --\n",
      " 69030/100000: episode: 2027, duration: 0.163s, episode steps: 24, steps per second: 147, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.551, 1.012], mean_best_reward: --\n",
      " 69091/100000: episode: 2028, duration: 0.322s, episode steps: 61, steps per second: 190, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.187 [-0.826, 1.328], mean_best_reward: --\n",
      " 69115/100000: episode: 2029, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.087 [-1.384, 0.584], mean_best_reward: --\n",
      " 69202/100000: episode: 2030, duration: 0.451s, episode steps: 87, steps per second: 193, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.133 [-1.504, 0.959], mean_best_reward: --\n",
      " 69313/100000: episode: 2031, duration: 0.543s, episode steps: 111, steps per second: 204, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.079 [-0.593, 0.954], mean_best_reward: --\n",
      " 69337/100000: episode: 2032, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.655, 1.267], mean_best_reward: --\n",
      " 69409/100000: episode: 2033, duration: 0.365s, episode steps: 72, steps per second: 197, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-1.353, 1.017], mean_best_reward: --\n",
      " 69456/100000: episode: 2034, duration: 0.251s, episode steps: 47, steps per second: 188, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.010 [-1.418, 1.153], mean_best_reward: --\n",
      " 69517/100000: episode: 2035, duration: 0.318s, episode steps: 61, steps per second: 192, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.020 [-1.121, 0.788], mean_best_reward: --\n",
      " 69626/100000: episode: 2036, duration: 0.562s, episode steps: 109, steps per second: 194, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.011 [-1.139, 1.024], mean_best_reward: --\n",
      " 69666/100000: episode: 2037, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.140 [-1.256, 0.333], mean_best_reward: --\n",
      " 69697/100000: episode: 2038, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.103 [-0.430, 0.924], mean_best_reward: --\n",
      " 69728/100000: episode: 2039, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.087 [-0.994, 0.549], mean_best_reward: --\n",
      " 69766/100000: episode: 2040, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.078 [-0.935, 0.589], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69824/100000: episode: 2041, duration: 0.306s, episode steps: 58, steps per second: 189, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.072 [-0.864, 0.685], mean_best_reward: --\n",
      " 69843/100000: episode: 2042, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.083 [-0.943, 0.599], mean_best_reward: --\n",
      " 69897/100000: episode: 2043, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.014 [-0.960, 1.232], mean_best_reward: --\n",
      " 69950/100000: episode: 2044, duration: 0.269s, episode steps: 53, steps per second: 197, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.042 [-0.792, 1.242], mean_best_reward: --\n",
      " 69980/100000: episode: 2045, duration: 0.148s, episode steps: 30, steps per second: 202, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-0.984, 0.427], mean_best_reward: --\n",
      " 70016/100000: episode: 2046, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.953, 1.355], mean_best_reward: --\n",
      " 70075/100000: episode: 2047, duration: 0.278s, episode steps: 59, steps per second: 212, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.140 [-0.409, 0.900], mean_best_reward: --\n",
      " 70092/100000: episode: 2048, duration: 0.084s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.128 [-0.559, 1.105], mean_best_reward: --\n",
      " 70154/100000: episode: 2049, duration: 0.293s, episode steps: 62, steps per second: 212, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.047 [-0.831, 0.598], mean_best_reward: --\n",
      " 70239/100000: episode: 2050, duration: 0.407s, episode steps: 85, steps per second: 209, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.003 [-1.467, 1.134], mean_best_reward: --\n",
      " 70302/100000: episode: 2051, duration: 0.294s, episode steps: 63, steps per second: 214, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.103 [-1.113, 0.770], mean_best_reward: 89.000000\n",
      " 70422/100000: episode: 2052, duration: 0.577s, episode steps: 120, steps per second: 208, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.141 [-1.043, 1.125], mean_best_reward: --\n",
      " 70514/100000: episode: 2053, duration: 0.476s, episode steps: 92, steps per second: 193, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.068 [-0.822, 1.015], mean_best_reward: --\n",
      " 70531/100000: episode: 2054, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.098 [-0.767, 1.324], mean_best_reward: --\n",
      " 70571/100000: episode: 2055, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.130 [-0.591, 0.942], mean_best_reward: --\n",
      " 70629/100000: episode: 2056, duration: 0.299s, episode steps: 58, steps per second: 194, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.098 [-0.739, 1.041], mean_best_reward: --\n",
      " 70674/100000: episode: 2057, duration: 0.222s, episode steps: 45, steps per second: 202, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.152 [-0.585, 1.118], mean_best_reward: --\n",
      " 70726/100000: episode: 2058, duration: 0.276s, episode steps: 52, steps per second: 188, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.118 [-0.434, 0.903], mean_best_reward: --\n",
      " 70773/100000: episode: 2059, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.127 [-1.269, 1.057], mean_best_reward: --\n",
      " 70795/100000: episode: 2060, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.069 [-1.660, 1.004], mean_best_reward: --\n",
      " 70829/100000: episode: 2061, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.001 [-2.090, 1.537], mean_best_reward: --\n",
      " 70871/100000: episode: 2062, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.050 [-1.236, 0.762], mean_best_reward: --\n",
      " 70960/100000: episode: 2063, duration: 0.447s, episode steps: 89, steps per second: 199, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.161 [-1.331, 0.592], mean_best_reward: --\n",
      " 71006/100000: episode: 2064, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.043 [-1.634, 0.637], mean_best_reward: --\n",
      " 71103/100000: episode: 2065, duration: 0.511s, episode steps: 97, steps per second: 190, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.303 [-2.250, 1.338], mean_best_reward: --\n",
      " 71158/100000: episode: 2066, duration: 0.278s, episode steps: 55, steps per second: 198, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.048 [-0.556, 0.894], mean_best_reward: --\n",
      " 71196/100000: episode: 2067, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.152 [-1.070, 0.403], mean_best_reward: --\n",
      " 71230/100000: episode: 2068, duration: 0.163s, episode steps: 34, steps per second: 209, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.111, 0.749], mean_best_reward: --\n",
      " 71276/100000: episode: 2069, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.162 [-1.564, 0.776], mean_best_reward: --\n",
      " 71335/100000: episode: 2070, duration: 0.311s, episode steps: 59, steps per second: 190, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.005 [-1.529, 1.024], mean_best_reward: --\n",
      " 71421/100000: episode: 2071, duration: 0.437s, episode steps: 86, steps per second: 197, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.046 [-0.999, 0.713], mean_best_reward: --\n",
      " 71464/100000: episode: 2072, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.031 [-1.131, 0.619], mean_best_reward: --\n",
      " 71526/100000: episode: 2073, duration: 0.332s, episode steps: 62, steps per second: 187, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-1.315, 1.131], mean_best_reward: --\n",
      " 71547/100000: episode: 2074, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.086 [-1.256, 0.762], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71608/100000: episode: 2075, duration: 0.304s, episode steps: 61, steps per second: 201, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.049 [-0.552, 0.844], mean_best_reward: --\n",
      " 71660/100000: episode: 2076, duration: 0.250s, episode steps: 52, steps per second: 208, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.114 [-0.401, 1.245], mean_best_reward: --\n",
      " 71722/100000: episode: 2077, duration: 0.309s, episode steps: 62, steps per second: 201, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.149 [-0.822, 1.141], mean_best_reward: --\n",
      " 71749/100000: episode: 2078, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.043 [-1.003, 1.791], mean_best_reward: --\n",
      " 71812/100000: episode: 2079, duration: 0.294s, episode steps: 63, steps per second: 214, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.103 [-0.983, 1.303], mean_best_reward: --\n",
      " 71907/100000: episode: 2080, duration: 0.442s, episode steps: 95, steps per second: 215, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.220 [-2.139, 1.506], mean_best_reward: --\n",
      " 71926/100000: episode: 2081, duration: 0.094s, episode steps: 19, steps per second: 201, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.109 [-1.462, 0.627], mean_best_reward: --\n",
      " 71949/100000: episode: 2082, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.122 [-0.832, 0.362], mean_best_reward: --\n",
      " 71996/100000: episode: 2083, duration: 0.218s, episode steps: 47, steps per second: 216, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.132 [-0.439, 0.888], mean_best_reward: --\n",
      " 72051/100000: episode: 2084, duration: 0.261s, episode steps: 55, steps per second: 210, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.059 [-0.859, 1.121], mean_best_reward: --\n",
      " 72130/100000: episode: 2085, duration: 0.373s, episode steps: 79, steps per second: 212, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.058 [-1.022, 0.742], mean_best_reward: --\n",
      " 72181/100000: episode: 2086, duration: 0.237s, episode steps: 51, steps per second: 215, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.118 [-1.304, 0.724], mean_best_reward: --\n",
      " 72224/100000: episode: 2087, duration: 0.222s, episode steps: 43, steps per second: 194, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.143 [-0.420, 1.254], mean_best_reward: --\n",
      " 72254/100000: episode: 2088, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.080 [-0.601, 1.006], mean_best_reward: --\n",
      " 72278/100000: episode: 2089, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.069 [-0.998, 1.873], mean_best_reward: --\n",
      " 72303/100000: episode: 2090, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.134 [-0.966, 0.555], mean_best_reward: --\n",
      " 72319/100000: episode: 2091, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.097 [-1.630, 0.795], mean_best_reward: --\n",
      " 72369/100000: episode: 2092, duration: 0.234s, episode steps: 50, steps per second: 214, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.570, 1.160], mean_best_reward: --\n",
      " 72435/100000: episode: 2093, duration: 0.308s, episode steps: 66, steps per second: 214, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.167 [-1.487, 0.473], mean_best_reward: --\n",
      " 72484/100000: episode: 2094, duration: 0.228s, episode steps: 49, steps per second: 214, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.091 [-0.692, 1.272], mean_best_reward: --\n",
      " 72506/100000: episode: 2095, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.098 [-0.453, 1.004], mean_best_reward: --\n",
      " 72538/100000: episode: 2096, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.123 [-1.150, 0.618], mean_best_reward: --\n",
      " 72582/100000: episode: 2097, duration: 0.204s, episode steps: 44, steps per second: 215, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.607, 1.052], mean_best_reward: --\n",
      " 72656/100000: episode: 2098, duration: 0.346s, episode steps: 74, steps per second: 214, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.609, 0.558], mean_best_reward: --\n",
      " 72675/100000: episode: 2099, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.073 [-0.948, 1.602], mean_best_reward: --\n",
      " 72706/100000: episode: 2100, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.115 [-1.324, 0.566], mean_best_reward: --\n",
      " 72733/100000: episode: 2101, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.068 [-1.181, 0.628], mean_best_reward: 99.000000\n",
      " 72758/100000: episode: 2102, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.137 [-1.040, 0.562], mean_best_reward: --\n",
      " 72782/100000: episode: 2103, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.991, 0.604], mean_best_reward: --\n",
      " 72819/100000: episode: 2104, duration: 0.175s, episode steps: 37, steps per second: 212, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.055 [-0.584, 1.101], mean_best_reward: --\n",
      " 72852/100000: episode: 2105, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.066 [-0.976, 0.556], mean_best_reward: --\n",
      " 72878/100000: episode: 2106, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.103 [-0.760, 0.401], mean_best_reward: --\n",
      " 72941/100000: episode: 2107, duration: 0.306s, episode steps: 63, steps per second: 206, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.094 [-0.718, 0.817], mean_best_reward: --\n",
      " 72956/100000: episode: 2108, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.086 [-1.502, 0.840], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72985/100000: episode: 2109, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.099 [-1.453, 0.574], mean_best_reward: --\n",
      " 73000/100000: episode: 2110, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.079 [-1.338, 0.824], mean_best_reward: --\n",
      " 73047/100000: episode: 2111, duration: 0.227s, episode steps: 47, steps per second: 207, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.099 [-0.831, 1.256], mean_best_reward: --\n",
      " 73063/100000: episode: 2112, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.123 [-2.154, 1.160], mean_best_reward: --\n",
      " 73166/100000: episode: 2113, duration: 0.487s, episode steps: 103, steps per second: 212, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.262 [-1.756, 1.022], mean_best_reward: --\n",
      " 73207/100000: episode: 2114, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.097 [-0.748, 0.355], mean_best_reward: --\n",
      " 73239/100000: episode: 2115, duration: 0.189s, episode steps: 32, steps per second: 170, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.112 [-1.605, 0.444], mean_best_reward: --\n",
      " 73262/100000: episode: 2116, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.076 [-1.114, 0.774], mean_best_reward: --\n",
      " 73308/100000: episode: 2117, duration: 0.292s, episode steps: 46, steps per second: 158, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.591, 1.023], mean_best_reward: --\n",
      " 73325/100000: episode: 2118, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.113 [-0.543, 0.944], mean_best_reward: --\n",
      " 73381/100000: episode: 2119, duration: 0.305s, episode steps: 56, steps per second: 184, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.102 [-1.245, 0.518], mean_best_reward: --\n",
      " 73421/100000: episode: 2120, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-0.913, 0.570], mean_best_reward: --\n",
      " 73451/100000: episode: 2121, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.081 [-0.625, 1.283], mean_best_reward: --\n",
      " 73515/100000: episode: 2122, duration: 0.303s, episode steps: 64, steps per second: 211, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.145 [-0.475, 0.930], mean_best_reward: --\n",
      " 73574/100000: episode: 2123, duration: 0.283s, episode steps: 59, steps per second: 208, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: -0.224 [-2.669, 2.063], mean_best_reward: --\n",
      " 73607/100000: episode: 2124, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.101 [-1.113, 0.565], mean_best_reward: --\n",
      " 73642/100000: episode: 2125, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.041 [-0.807, 1.160], mean_best_reward: --\n",
      " 73697/100000: episode: 2126, duration: 0.270s, episode steps: 55, steps per second: 204, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.069 [-0.921, 0.398], mean_best_reward: --\n",
      " 73732/100000: episode: 2127, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.151 [-1.068, 0.388], mean_best_reward: --\n",
      " 73795/100000: episode: 2128, duration: 0.332s, episode steps: 63, steps per second: 190, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.127 [-0.576, 1.837], mean_best_reward: --\n",
      " 73810/100000: episode: 2129, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.095 [-1.181, 0.599], mean_best_reward: --\n",
      " 73831/100000: episode: 2130, duration: 0.102s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.033, 0.414], mean_best_reward: --\n",
      " 73857/100000: episode: 2131, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.024, 0.633], mean_best_reward: --\n",
      " 73878/100000: episode: 2132, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.118 [-1.314, 0.756], mean_best_reward: --\n",
      " 73921/100000: episode: 2133, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.125 [-1.545, 0.392], mean_best_reward: --\n",
      " 73977/100000: episode: 2134, duration: 0.279s, episode steps: 56, steps per second: 201, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.101 [-0.759, 0.973], mean_best_reward: --\n",
      " 74005/100000: episode: 2135, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.045, 0.542], mean_best_reward: --\n",
      " 74065/100000: episode: 2136, duration: 0.303s, episode steps: 60, steps per second: 198, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-1.038, 0.376], mean_best_reward: --\n",
      " 74083/100000: episode: 2137, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.098 [-1.844, 0.999], mean_best_reward: --\n",
      " 74142/100000: episode: 2138, duration: 0.294s, episode steps: 59, steps per second: 201, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.143 [-0.922, 1.122], mean_best_reward: --\n",
      " 74169/100000: episode: 2139, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.085 [-1.038, 0.604], mean_best_reward: --\n",
      " 74192/100000: episode: 2140, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.109 [-0.718, 0.440], mean_best_reward: --\n",
      " 74248/100000: episode: 2141, duration: 0.292s, episode steps: 56, steps per second: 192, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.081 [-0.952, 0.533], mean_best_reward: --\n",
      " 74280/100000: episode: 2142, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.066 [-1.646, 0.794], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74345/100000: episode: 2143, duration: 0.323s, episode steps: 65, steps per second: 201, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.067 [-0.939, 0.527], mean_best_reward: --\n",
      " 74370/100000: episode: 2144, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.095 [-0.938, 1.419], mean_best_reward: --\n",
      " 74400/100000: episode: 2145, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.074 [-1.061, 0.552], mean_best_reward: --\n",
      " 74423/100000: episode: 2146, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.050 [-0.807, 1.279], mean_best_reward: --\n",
      " 74440/100000: episode: 2147, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.111 [-1.092, 0.612], mean_best_reward: --\n",
      " 74502/100000: episode: 2148, duration: 0.309s, episode steps: 62, steps per second: 201, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.057 [-1.507, 1.131], mean_best_reward: --\n",
      " 74523/100000: episode: 2149, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.070 [-1.181, 0.800], mean_best_reward: --\n",
      " 74542/100000: episode: 2150, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.107 [-0.545, 0.985], mean_best_reward: --\n",
      " 74592/100000: episode: 2151, duration: 0.254s, episode steps: 50, steps per second: 197, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.174 [-1.043, 0.586], mean_best_reward: 107.500000\n",
      " 74621/100000: episode: 2152, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.134 [-0.930, 0.545], mean_best_reward: --\n",
      " 74641/100000: episode: 2153, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.062 [-0.831, 1.517], mean_best_reward: --\n",
      " 74668/100000: episode: 2154, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.106 [-0.928, 0.262], mean_best_reward: --\n",
      " 74741/100000: episode: 2155, duration: 0.383s, episode steps: 73, steps per second: 191, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.166 [-1.952, 1.172], mean_best_reward: --\n",
      " 74785/100000: episode: 2156, duration: 0.227s, episode steps: 44, steps per second: 193, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.095 [-0.547, 0.974], mean_best_reward: --\n",
      " 74849/100000: episode: 2157, duration: 0.327s, episode steps: 64, steps per second: 196, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.013 [-1.095, 0.595], mean_best_reward: --\n",
      " 74901/100000: episode: 2158, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.038 [-1.383, 1.689], mean_best_reward: --\n",
      " 74929/100000: episode: 2159, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.143 [-1.061, 0.359], mean_best_reward: --\n",
      " 74979/100000: episode: 2160, duration: 0.248s, episode steps: 50, steps per second: 202, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.386, 0.511], mean_best_reward: --\n",
      " 75118/100000: episode: 2161, duration: 0.655s, episode steps: 139, steps per second: 212, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.163 [-1.095, 1.026], mean_best_reward: --\n",
      " 75129/100000: episode: 2162, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.385, 2.346], mean_best_reward: --\n",
      " 75148/100000: episode: 2163, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.089 [-1.603, 0.771], mean_best_reward: --\n",
      " 75260/100000: episode: 2164, duration: 0.532s, episode steps: 112, steps per second: 211, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.077 [-0.943, 0.987], mean_best_reward: --\n",
      " 75290/100000: episode: 2165, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.047 [-1.383, 1.005], mean_best_reward: --\n",
      " 75338/100000: episode: 2166, duration: 0.230s, episode steps: 48, steps per second: 208, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.093 [-0.705, 0.551], mean_best_reward: --\n",
      " 75374/100000: episode: 2167, duration: 0.169s, episode steps: 36, steps per second: 213, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.111 [-1.698, 0.587], mean_best_reward: --\n",
      " 75444/100000: episode: 2168, duration: 0.331s, episode steps: 70, steps per second: 212, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.598, 1.170], mean_best_reward: --\n",
      " 75514/100000: episode: 2169, duration: 0.331s, episode steps: 70, steps per second: 212, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.079 [-1.845, 0.600], mean_best_reward: --\n",
      " 75545/100000: episode: 2170, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.115 [-0.600, 0.993], mean_best_reward: --\n",
      " 75579/100000: episode: 2171, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.133 [-1.325, 0.566], mean_best_reward: --\n",
      " 75663/100000: episode: 2172, duration: 0.390s, episode steps: 84, steps per second: 215, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.113 [-1.070, 0.710], mean_best_reward: --\n",
      " 75709/100000: episode: 2173, duration: 0.215s, episode steps: 46, steps per second: 214, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.141 [-0.411, 0.855], mean_best_reward: --\n",
      " 75763/100000: episode: 2174, duration: 0.256s, episode steps: 54, steps per second: 211, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.098 [-2.323, 2.729], mean_best_reward: --\n",
      " 75776/100000: episode: 2175, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.097 [-1.477, 0.972], mean_best_reward: --\n",
      " 75829/100000: episode: 2176, duration: 0.247s, episode steps: 53, steps per second: 214, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.118 [-1.940, 1.845], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75902/100000: episode: 2177, duration: 0.363s, episode steps: 73, steps per second: 201, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.084 [-0.506, 1.123], mean_best_reward: --\n",
      " 75944/100000: episode: 2178, duration: 0.218s, episode steps: 42, steps per second: 193, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-1.263, 0.741], mean_best_reward: --\n",
      " 75988/100000: episode: 2179, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.121 [-1.057, 0.346], mean_best_reward: --\n",
      " 76028/100000: episode: 2180, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.145 [-0.373, 0.790], mean_best_reward: --\n",
      " 76055/100000: episode: 2181, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.129 [-0.951, 0.570], mean_best_reward: --\n",
      " 76090/100000: episode: 2182, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.141 [-0.908, 0.212], mean_best_reward: --\n",
      " 76140/100000: episode: 2183, duration: 0.247s, episode steps: 50, steps per second: 203, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.097 [-0.626, 0.983], mean_best_reward: --\n",
      " 76181/100000: episode: 2184, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.128 [-0.591, 1.045], mean_best_reward: --\n",
      " 76217/100000: episode: 2185, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.499, 1.355], mean_best_reward: --\n",
      " 76251/100000: episode: 2186, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.108 [-0.644, 0.409], mean_best_reward: --\n",
      " 76307/100000: episode: 2187, duration: 0.297s, episode steps: 56, steps per second: 189, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.375, 0.551], mean_best_reward: --\n",
      " 76355/100000: episode: 2188, duration: 0.272s, episode steps: 48, steps per second: 177, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.121 [-0.505, 0.972], mean_best_reward: --\n",
      " 76400/100000: episode: 2189, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.100 [-0.526, 0.825], mean_best_reward: --\n",
      " 76437/100000: episode: 2190, duration: 0.183s, episode steps: 37, steps per second: 202, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.062 [-0.764, 1.084], mean_best_reward: --\n",
      " 76496/100000: episode: 2191, duration: 0.300s, episode steps: 59, steps per second: 197, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.077 [-0.866, 1.434], mean_best_reward: --\n",
      " 76565/100000: episode: 2192, duration: 0.350s, episode steps: 69, steps per second: 197, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.106 [-0.624, 0.969], mean_best_reward: --\n",
      " 76594/100000: episode: 2193, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.081 [-0.778, 1.691], mean_best_reward: --\n",
      " 76633/100000: episode: 2194, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.084 [-1.402, 0.436], mean_best_reward: --\n",
      " 76658/100000: episode: 2195, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.052 [-1.595, 1.001], mean_best_reward: --\n",
      " 76678/100000: episode: 2196, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.095 [-0.767, 1.462], mean_best_reward: --\n",
      " 76696/100000: episode: 2197, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.086 [-1.003, 1.723], mean_best_reward: --\n",
      " 76757/100000: episode: 2198, duration: 0.364s, episode steps: 61, steps per second: 168, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.119 [-0.749, 0.998], mean_best_reward: --\n",
      " 76829/100000: episode: 2199, duration: 0.389s, episode steps: 72, steps per second: 185, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.166 [-0.958, 1.074], mean_best_reward: --\n",
      " 76856/100000: episode: 2200, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.094 [-1.290, 0.608], mean_best_reward: --\n",
      " 76901/100000: episode: 2201, duration: 0.218s, episode steps: 45, steps per second: 206, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.146 [-1.336, 0.646], mean_best_reward: 118.000000\n",
      " 76976/100000: episode: 2202, duration: 0.338s, episode steps: 75, steps per second: 222, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.276 [-1.171, 0.664], mean_best_reward: --\n",
      " 76999/100000: episode: 2203, duration: 0.107s, episode steps: 23, steps per second: 214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.052 [-0.779, 1.264], mean_best_reward: --\n",
      " 77031/100000: episode: 2204, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.606, 1.055], mean_best_reward: --\n",
      " 77084/100000: episode: 2205, duration: 0.263s, episode steps: 53, steps per second: 202, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.126 [-0.487, 0.811], mean_best_reward: --\n",
      " 77128/100000: episode: 2206, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.139 [-0.960, 1.100], mean_best_reward: --\n",
      " 77141/100000: episode: 2207, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.101 [-1.395, 2.273], mean_best_reward: --\n",
      " 77166/100000: episode: 2208, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.050 [-1.667, 1.118], mean_best_reward: --\n",
      " 77210/100000: episode: 2209, duration: 0.201s, episode steps: 44, steps per second: 219, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.544, 0.923], mean_best_reward: --\n",
      " 77253/100000: episode: 2210, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.108 [-1.118, 0.617], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77310/100000: episode: 2211, duration: 0.262s, episode steps: 57, steps per second: 217, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.034 [-0.955, 0.433], mean_best_reward: --\n",
      " 77350/100000: episode: 2212, duration: 0.193s, episode steps: 40, steps per second: 208, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.058 [-0.619, 0.962], mean_best_reward: --\n",
      " 77388/100000: episode: 2213, duration: 0.174s, episode steps: 38, steps per second: 218, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.087 [-0.538, 1.166], mean_best_reward: --\n",
      " 77465/100000: episode: 2214, duration: 0.366s, episode steps: 77, steps per second: 210, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.016 [-1.084, 0.745], mean_best_reward: --\n",
      " 77564/100000: episode: 2215, duration: 0.465s, episode steps: 99, steps per second: 213, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.057 [-0.877, 0.632], mean_best_reward: --\n",
      " 77583/100000: episode: 2216, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.064 [-0.629, 0.969], mean_best_reward: --\n",
      " 77623/100000: episode: 2217, duration: 0.192s, episode steps: 40, steps per second: 208, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.121 [-0.463, 1.030], mean_best_reward: --\n",
      " 77674/100000: episode: 2218, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.142 [-0.856, 0.381], mean_best_reward: --\n",
      " 77687/100000: episode: 2219, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-1.035, 1.752], mean_best_reward: --\n",
      " 77709/100000: episode: 2220, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.101 [-0.408, 0.807], mean_best_reward: --\n",
      " 77743/100000: episode: 2221, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.125 [-0.799, 0.260], mean_best_reward: --\n",
      " 77803/100000: episode: 2222, duration: 0.301s, episode steps: 60, steps per second: 200, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.092 [-1.175, 0.658], mean_best_reward: --\n",
      " 77838/100000: episode: 2223, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.121 [-0.453, 0.835], mean_best_reward: --\n",
      " 77856/100000: episode: 2224, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.077 [-1.124, 1.799], mean_best_reward: --\n",
      " 77888/100000: episode: 2225, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.443, 0.764], mean_best_reward: --\n",
      " 77917/100000: episode: 2226, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.079 [-0.756, 1.154], mean_best_reward: --\n",
      " 77944/100000: episode: 2227, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.097 [-0.612, 0.996], mean_best_reward: --\n",
      " 78035/100000: episode: 2228, duration: 0.440s, episode steps: 91, steps per second: 207, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.209 [-1.295, 1.558], mean_best_reward: --\n",
      " 78064/100000: episode: 2229, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.027 [-1.182, 1.767], mean_best_reward: --\n",
      " 78095/100000: episode: 2230, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.063 [-0.606, 0.944], mean_best_reward: --\n",
      " 78131/100000: episode: 2231, duration: 0.165s, episode steps: 36, steps per second: 218, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.143 [-0.905, 0.432], mean_best_reward: --\n",
      " 78215/100000: episode: 2232, duration: 0.425s, episode steps: 84, steps per second: 198, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.204 [-1.346, 0.922], mean_best_reward: --\n",
      " 78244/100000: episode: 2233, duration: 0.137s, episode steps: 29, steps per second: 212, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.075 [-0.602, 1.112], mean_best_reward: --\n",
      " 78273/100000: episode: 2234, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.074 [-1.109, 0.443], mean_best_reward: --\n",
      " 78330/100000: episode: 2235, duration: 0.277s, episode steps: 57, steps per second: 206, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.000 [-1.141, 1.700], mean_best_reward: --\n",
      " 78377/100000: episode: 2236, duration: 0.227s, episode steps: 47, steps per second: 207, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.158 [-0.372, 0.798], mean_best_reward: --\n",
      " 78409/100000: episode: 2237, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.114 [-0.428, 0.700], mean_best_reward: --\n",
      " 78457/100000: episode: 2238, duration: 0.228s, episode steps: 48, steps per second: 211, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-0.996, 0.576], mean_best_reward: --\n",
      " 78473/100000: episode: 2239, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.058 [-1.013, 1.532], mean_best_reward: --\n",
      " 78498/100000: episode: 2240, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.119 [-0.181, 0.795], mean_best_reward: --\n",
      " 78524/100000: episode: 2241, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.484, 1.137], mean_best_reward: --\n",
      " 78547/100000: episode: 2242, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.107 [-0.976, 1.437], mean_best_reward: --\n",
      " 78584/100000: episode: 2243, duration: 0.179s, episode steps: 37, steps per second: 206, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.070 [-1.493, 0.989], mean_best_reward: --\n",
      " 78629/100000: episode: 2244, duration: 0.207s, episode steps: 45, steps per second: 217, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.119 [-0.394, 0.935], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78717/100000: episode: 2245, duration: 0.405s, episode steps: 88, steps per second: 217, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.118 [-0.595, 0.970], mean_best_reward: --\n",
      " 78763/100000: episode: 2246, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.596, 1.133], mean_best_reward: --\n",
      " 78813/100000: episode: 2247, duration: 0.238s, episode steps: 50, steps per second: 210, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.078 [-0.488, 0.910], mean_best_reward: --\n",
      " 78831/100000: episode: 2248, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.074 [-0.775, 1.505], mean_best_reward: --\n",
      " 78845/100000: episode: 2249, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.558, 0.982], mean_best_reward: --\n",
      " 78885/100000: episode: 2250, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.629, 1.030], mean_best_reward: --\n",
      " 78930/100000: episode: 2251, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.121 [-0.870, 0.545], mean_best_reward: 91.000000\n",
      " 78954/100000: episode: 2252, duration: 0.112s, episode steps: 24, steps per second: 215, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.068 [-0.963, 1.571], mean_best_reward: --\n",
      " 79064/100000: episode: 2253, duration: 0.519s, episode steps: 110, steps per second: 212, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.227 [-1.873, 1.001], mean_best_reward: --\n",
      " 79087/100000: episode: 2254, duration: 0.141s, episode steps: 23, steps per second: 164, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.108 [-0.567, 0.996], mean_best_reward: --\n",
      " 79140/100000: episode: 2255, duration: 0.248s, episode steps: 53, steps per second: 214, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.154 [-1.118, 0.548], mean_best_reward: --\n",
      " 79206/100000: episode: 2256, duration: 0.338s, episode steps: 66, steps per second: 195, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.077 [-1.477, 0.783], mean_best_reward: --\n",
      " 79255/100000: episode: 2257, duration: 0.225s, episode steps: 49, steps per second: 217, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.124 [-0.764, 1.546], mean_best_reward: --\n",
      " 79382/100000: episode: 2258, duration: 0.597s, episode steps: 127, steps per second: 213, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.322 [-1.534, 0.950], mean_best_reward: --\n",
      " 79440/100000: episode: 2259, duration: 0.261s, episode steps: 58, steps per second: 222, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.123 [-0.609, 1.839], mean_best_reward: --\n",
      " 79478/100000: episode: 2260, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.092 [-0.975, 0.390], mean_best_reward: --\n",
      " 79523/100000: episode: 2261, duration: 0.208s, episode steps: 45, steps per second: 217, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.175 [-0.946, 0.570], mean_best_reward: --\n",
      " 79567/100000: episode: 2262, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.087 [-1.010, 0.359], mean_best_reward: --\n",
      " 79666/100000: episode: 2263, duration: 0.482s, episode steps: 99, steps per second: 206, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.513 [-2.572, 1.116], mean_best_reward: --\n",
      " 79687/100000: episode: 2264, duration: 0.099s, episode steps: 21, steps per second: 213, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.054 [-1.014, 1.596], mean_best_reward: --\n",
      " 79709/100000: episode: 2265, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-1.191, 1.775], mean_best_reward: --\n",
      " 79766/100000: episode: 2266, duration: 0.281s, episode steps: 57, steps per second: 203, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.175 [-0.617, 0.960], mean_best_reward: --\n",
      " 79804/100000: episode: 2267, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.070 [-2.362, 1.184], mean_best_reward: --\n",
      " 79844/100000: episode: 2268, duration: 0.185s, episode steps: 40, steps per second: 216, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.002 [-1.264, 0.651], mean_best_reward: --\n",
      " 79882/100000: episode: 2269, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.113 [-0.546, 1.450], mean_best_reward: --\n",
      " 79923/100000: episode: 2270, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.088 [-0.743, 1.220], mean_best_reward: --\n",
      " 79970/100000: episode: 2271, duration: 0.244s, episode steps: 47, steps per second: 192, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: 0.026 [-0.796, 1.482], mean_best_reward: --\n",
      " 80044/100000: episode: 2272, duration: 0.370s, episode steps: 74, steps per second: 200, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.210 [-1.216, 0.486], mean_best_reward: --\n",
      " 80096/100000: episode: 2273, duration: 0.252s, episode steps: 52, steps per second: 206, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.089 [-0.581, 0.966], mean_best_reward: --\n",
      " 80145/100000: episode: 2274, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.186 [-1.144, 0.540], mean_best_reward: --\n",
      " 80208/100000: episode: 2275, duration: 0.305s, episode steps: 63, steps per second: 206, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.079 [-0.460, 1.358], mean_best_reward: --\n",
      " 80231/100000: episode: 2276, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.126 [-1.227, 0.750], mean_best_reward: --\n",
      " 80288/100000: episode: 2277, duration: 0.292s, episode steps: 57, steps per second: 195, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.119 [-1.173, 0.798], mean_best_reward: --\n",
      " 80354/100000: episode: 2278, duration: 0.307s, episode steps: 66, steps per second: 215, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.107 [-0.759, 0.936], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80398/100000: episode: 2279, duration: 0.225s, episode steps: 44, steps per second: 195, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.560, 1.055], mean_best_reward: --\n",
      " 80419/100000: episode: 2280, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.039 [-1.508, 1.003], mean_best_reward: --\n",
      " 80481/100000: episode: 2281, duration: 0.297s, episode steps: 62, steps per second: 209, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.436, 0.551], mean_best_reward: --\n",
      " 80681/100000: episode: 2282, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.757, 0.914], mean_best_reward: --\n",
      " 80699/100000: episode: 2283, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.086 [-1.199, 0.756], mean_best_reward: --\n",
      " 80727/100000: episode: 2284, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.116 [-0.628, 1.006], mean_best_reward: --\n",
      " 80760/100000: episode: 2285, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.073 [-1.986, 0.993], mean_best_reward: --\n",
      " 80882/100000: episode: 2286, duration: 0.590s, episode steps: 122, steps per second: 207, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.305 [-2.620, 1.685], mean_best_reward: --\n",
      " 80969/100000: episode: 2287, duration: 0.435s, episode steps: 87, steps per second: 200, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.079 [-0.851, 1.666], mean_best_reward: --\n",
      " 81033/100000: episode: 2288, duration: 0.316s, episode steps: 64, steps per second: 203, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-1.058, 0.732], mean_best_reward: --\n",
      " 81049/100000: episode: 2289, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.106 [-1.811, 0.974], mean_best_reward: --\n",
      " 81078/100000: episode: 2290, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.074 [-1.296, 0.786], mean_best_reward: --\n",
      " 81095/100000: episode: 2291, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.105 [-0.804, 1.307], mean_best_reward: --\n",
      " 81134/100000: episode: 2292, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.144 [-0.744, 1.107], mean_best_reward: --\n",
      " 81190/100000: episode: 2293, duration: 0.333s, episode steps: 56, steps per second: 168, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.042 [-1.019, 0.776], mean_best_reward: --\n",
      " 81229/100000: episode: 2294, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.053 [-0.556, 1.120], mean_best_reward: --\n",
      " 81267/100000: episode: 2295, duration: 0.241s, episode steps: 38, steps per second: 158, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.097 [-1.093, 0.391], mean_best_reward: --\n",
      " 81333/100000: episode: 2296, duration: 0.364s, episode steps: 66, steps per second: 181, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.227 [-1.693, 0.750], mean_best_reward: --\n",
      " 81391/100000: episode: 2297, duration: 0.301s, episode steps: 58, steps per second: 193, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.189 [-1.285, 0.354], mean_best_reward: --\n",
      " 81467/100000: episode: 2298, duration: 0.344s, episode steps: 76, steps per second: 221, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.040 [-0.975, 1.122], mean_best_reward: --\n",
      " 81521/100000: episode: 2299, duration: 0.262s, episode steps: 54, steps per second: 206, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.155 [-1.233, 0.421], mean_best_reward: --\n",
      " 81551/100000: episode: 2300, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.042 [-1.015, 0.747], mean_best_reward: --\n",
      " 81574/100000: episode: 2301, duration: 0.112s, episode steps: 23, steps per second: 206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.115 [-1.103, 0.582], mean_best_reward: 169.000000\n",
      " 81592/100000: episode: 2302, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.095 [-1.005, 1.904], mean_best_reward: --\n",
      " 81618/100000: episode: 2303, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.373, 1.143], mean_best_reward: --\n",
      " 81647/100000: episode: 2304, duration: 0.173s, episode steps: 29, steps per second: 167, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.057 [-0.819, 1.213], mean_best_reward: --\n",
      " 81808/100000: episode: 2305, duration: 0.775s, episode steps: 161, steps per second: 208, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.228 [-1.851, 0.869], mean_best_reward: --\n",
      " 81982/100000: episode: 2306, duration: 0.835s, episode steps: 174, steps per second: 208, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.296 [-2.200, 1.215], mean_best_reward: --\n",
      " 82032/100000: episode: 2307, duration: 0.298s, episode steps: 50, steps per second: 168, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.063 [-1.483, 1.123], mean_best_reward: --\n",
      " 82069/100000: episode: 2308, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.106 [-1.178, 0.580], mean_best_reward: --\n",
      " 82086/100000: episode: 2309, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.098 [-1.710, 0.973], mean_best_reward: --\n",
      " 82132/100000: episode: 2310, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.063 [-0.572, 0.896], mean_best_reward: --\n",
      " 82198/100000: episode: 2311, duration: 0.297s, episode steps: 66, steps per second: 222, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.154 [-0.612, 0.901], mean_best_reward: --\n",
      " 82277/100000: episode: 2312, duration: 0.395s, episode steps: 79, steps per second: 200, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.093 [-1.183, 1.134], mean_best_reward: --\n",
      " 82302/100000: episode: 2313, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.069 [-0.575, 1.318], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82339/100000: episode: 2314, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.033 [-0.630, 1.147], mean_best_reward: --\n",
      " 82358/100000: episode: 2315, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.031 [-2.231, 1.574], mean_best_reward: --\n",
      " 82385/100000: episode: 2316, duration: 0.126s, episode steps: 27, steps per second: 214, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.073 [-0.638, 1.155], mean_best_reward: --\n",
      " 82468/100000: episode: 2317, duration: 0.389s, episode steps: 83, steps per second: 213, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.166 [-1.366, 0.802], mean_best_reward: --\n",
      " 82481/100000: episode: 2318, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.114 [-0.991, 1.770], mean_best_reward: --\n",
      " 82503/100000: episode: 2319, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.092 [-0.581, 1.072], mean_best_reward: --\n",
      " 82576/100000: episode: 2320, duration: 0.349s, episode steps: 73, steps per second: 209, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.216 [-1.610, 0.838], mean_best_reward: --\n",
      " 82679/100000: episode: 2321, duration: 0.490s, episode steps: 103, steps per second: 210, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.263 [-3.234, 2.237], mean_best_reward: --\n",
      " 82691/100000: episode: 2322, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.549, 1.114], mean_best_reward: --\n",
      " 82712/100000: episode: 2323, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.042 [-1.317, 1.913], mean_best_reward: --\n",
      " 82738/100000: episode: 2324, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.040 [-1.138, 1.585], mean_best_reward: --\n",
      " 82778/100000: episode: 2325, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-0.821, 0.580], mean_best_reward: --\n",
      " 82814/100000: episode: 2326, duration: 0.163s, episode steps: 36, steps per second: 221, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.048 [-0.588, 1.214], mean_best_reward: --\n",
      " 82847/100000: episode: 2327, duration: 0.151s, episode steps: 33, steps per second: 219, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.102 [-0.382, 1.391], mean_best_reward: --\n",
      " 82877/100000: episode: 2328, duration: 0.138s, episode steps: 30, steps per second: 217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.048 [-0.823, 1.719], mean_best_reward: --\n",
      " 82921/100000: episode: 2329, duration: 0.202s, episode steps: 44, steps per second: 218, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.586, 1.173], mean_best_reward: --\n",
      " 82977/100000: episode: 2330, duration: 0.254s, episode steps: 56, steps per second: 221, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.065 [-0.551, 0.833], mean_best_reward: --\n",
      " 82990/100000: episode: 2331, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.096 [-2.181, 1.401], mean_best_reward: --\n",
      " 83022/100000: episode: 2332, duration: 0.150s, episode steps: 32, steps per second: 213, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.039 [-1.144, 0.631], mean_best_reward: --\n",
      " 83089/100000: episode: 2333, duration: 0.302s, episode steps: 67, steps per second: 222, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.133 [-0.452, 0.927], mean_best_reward: --\n",
      " 83113/100000: episode: 2334, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.046 [-0.801, 1.453], mean_best_reward: --\n",
      " 83182/100000: episode: 2335, duration: 0.331s, episode steps: 69, steps per second: 208, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.284 [-0.781, 1.475], mean_best_reward: --\n",
      " 83215/100000: episode: 2336, duration: 0.151s, episode steps: 33, steps per second: 219, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.039 [-0.753, 1.296], mean_best_reward: --\n",
      " 83232/100000: episode: 2337, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.064 [-1.228, 1.926], mean_best_reward: --\n",
      " 83244/100000: episode: 2338, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.770, 1.170], mean_best_reward: --\n",
      " 83282/100000: episode: 2339, duration: 0.176s, episode steps: 38, steps per second: 216, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.001 [-1.460, 0.962], mean_best_reward: --\n",
      " 83332/100000: episode: 2340, duration: 0.227s, episode steps: 50, steps per second: 220, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.130 [-0.587, 0.915], mean_best_reward: --\n",
      " 83425/100000: episode: 2341, duration: 0.432s, episode steps: 93, steps per second: 215, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.064 [-1.094, 0.941], mean_best_reward: --\n",
      " 83448/100000: episode: 2342, duration: 0.109s, episode steps: 23, steps per second: 212, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.099 [-0.363, 0.985], mean_best_reward: --\n",
      " 83515/100000: episode: 2343, duration: 0.315s, episode steps: 67, steps per second: 213, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.063 [-1.368, 0.788], mean_best_reward: --\n",
      " 83534/100000: episode: 2344, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.111 [-0.806, 1.232], mean_best_reward: --\n",
      " 83569/100000: episode: 2345, duration: 0.166s, episode steps: 35, steps per second: 211, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.046 [-0.616, 1.035], mean_best_reward: --\n",
      " 83613/100000: episode: 2346, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.069 [-0.569, 1.550], mean_best_reward: --\n",
      " 83631/100000: episode: 2347, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.089 [-1.682, 0.966], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83659/100000: episode: 2348, duration: 0.175s, episode steps: 28, steps per second: 160, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.067 [-0.981, 1.780], mean_best_reward: --\n",
      " 83680/100000: episode: 2349, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.092 [-1.236, 0.630], mean_best_reward: --\n",
      " 83707/100000: episode: 2350, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.049 [-0.764, 1.181], mean_best_reward: --\n",
      " 83766/100000: episode: 2351, duration: 0.337s, episode steps: 59, steps per second: 175, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.102 [-0.403, 0.745], mean_best_reward: 96.500000\n",
      " 83797/100000: episode: 2352, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.087 [-0.607, 1.576], mean_best_reward: --\n",
      " 83816/100000: episode: 2353, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.103 [-0.975, 1.989], mean_best_reward: --\n",
      " 83854/100000: episode: 2354, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.134 [-0.592, 1.211], mean_best_reward: --\n",
      " 83890/100000: episode: 2355, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.139 [-0.791, 0.412], mean_best_reward: --\n",
      " 83923/100000: episode: 2356, duration: 0.161s, episode steps: 33, steps per second: 204, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.072 [-0.973, 0.621], mean_best_reward: --\n",
      " 83949/100000: episode: 2357, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.644, 1.249], mean_best_reward: --\n",
      " 83971/100000: episode: 2358, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-0.957, 0.355], mean_best_reward: --\n",
      " 84015/100000: episode: 2359, duration: 0.206s, episode steps: 44, steps per second: 213, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.715, 1.110], mean_best_reward: --\n",
      " 84050/100000: episode: 2360, duration: 0.164s, episode steps: 35, steps per second: 214, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.111 [-0.423, 1.114], mean_best_reward: --\n",
      " 84114/100000: episode: 2361, duration: 0.309s, episode steps: 64, steps per second: 207, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.084 [-0.960, 0.508], mean_best_reward: --\n",
      " 84165/100000: episode: 2362, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.099 [-0.631, 0.947], mean_best_reward: --\n",
      " 84332/100000: episode: 2363, duration: 0.887s, episode steps: 167, steps per second: 188, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.033 [-1.045, 0.701], mean_best_reward: --\n",
      " 84351/100000: episode: 2364, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.114 [-1.047, 0.543], mean_best_reward: --\n",
      " 84363/100000: episode: 2365, duration: 0.102s, episode steps: 12, steps per second: 118, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.112 [-1.741, 1.007], mean_best_reward: --\n",
      " 84383/100000: episode: 2366, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.545, 0.947], mean_best_reward: --\n",
      " 84423/100000: episode: 2367, duration: 0.250s, episode steps: 40, steps per second: 160, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.062 [-1.003, 0.621], mean_best_reward: --\n",
      " 84471/100000: episode: 2368, duration: 0.290s, episode steps: 48, steps per second: 165, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-0.285, 1.421], mean_best_reward: --\n",
      " 84484/100000: episode: 2369, duration: 0.098s, episode steps: 13, steps per second: 133, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.105 [-1.481, 0.813], mean_best_reward: --\n",
      " 84526/100000: episode: 2370, duration: 0.270s, episode steps: 42, steps per second: 155, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.296, 0.756], mean_best_reward: --\n",
      " 84575/100000: episode: 2371, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.095 [-1.026, 0.598], mean_best_reward: --\n",
      " 84622/100000: episode: 2372, duration: 0.326s, episode steps: 47, steps per second: 144, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.128 [-1.026, 0.588], mean_best_reward: --\n",
      " 84649/100000: episode: 2373, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.059 [-1.008, 1.597], mean_best_reward: --\n",
      " 84668/100000: episode: 2374, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.049 [-0.998, 1.447], mean_best_reward: --\n",
      " 84733/100000: episode: 2375, duration: 0.361s, episode steps: 65, steps per second: 180, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.188 [-0.643, 0.980], mean_best_reward: --\n",
      " 84848/100000: episode: 2376, duration: 0.554s, episode steps: 115, steps per second: 208, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.234 [-0.624, 0.979], mean_best_reward: --\n",
      " 84895/100000: episode: 2377, duration: 0.215s, episode steps: 47, steps per second: 218, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.124 [-0.387, 0.849], mean_best_reward: --\n",
      " 84954/100000: episode: 2378, duration: 0.301s, episode steps: 59, steps per second: 196, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.049 [-1.326, 0.612], mean_best_reward: --\n",
      " 85006/100000: episode: 2379, duration: 0.251s, episode steps: 52, steps per second: 207, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.123 [-1.352, 0.650], mean_best_reward: --\n",
      " 85048/100000: episode: 2380, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.146 [-0.939, 0.441], mean_best_reward: --\n",
      " 85081/100000: episode: 2381, duration: 0.152s, episode steps: 33, steps per second: 217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.068 [-1.279, 0.566], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85107/100000: episode: 2382, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-0.899, 0.375], mean_best_reward: --\n",
      " 85136/100000: episode: 2383, duration: 0.145s, episode steps: 29, steps per second: 199, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.067 [-0.634, 1.059], mean_best_reward: --\n",
      " 85192/100000: episode: 2384, duration: 0.252s, episode steps: 56, steps per second: 222, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.101 [-1.149, 0.923], mean_best_reward: --\n",
      " 85211/100000: episode: 2385, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.042 [-1.715, 1.191], mean_best_reward: --\n",
      " 85297/100000: episode: 2386, duration: 0.411s, episode steps: 86, steps per second: 209, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.171 [-0.924, 1.124], mean_best_reward: --\n",
      " 85347/100000: episode: 2387, duration: 0.228s, episode steps: 50, steps per second: 219, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.059 [-1.305, 0.629], mean_best_reward: --\n",
      " 85386/100000: episode: 2388, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.133 [-0.599, 0.929], mean_best_reward: --\n",
      " 85423/100000: episode: 2389, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.059 [-1.355, 0.600], mean_best_reward: --\n",
      " 85459/100000: episode: 2390, duration: 0.168s, episode steps: 36, steps per second: 214, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.095 [-0.869, 0.615], mean_best_reward: --\n",
      " 85498/100000: episode: 2391, duration: 0.189s, episode steps: 39, steps per second: 207, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.154 [-1.221, 0.344], mean_best_reward: --\n",
      " 85538/100000: episode: 2392, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.101 [-0.565, 1.484], mean_best_reward: --\n",
      " 85564/100000: episode: 2393, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.103 [-0.770, 1.217], mean_best_reward: --\n",
      " 85578/100000: episode: 2394, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.114 [-1.985, 1.146], mean_best_reward: --\n",
      " 85620/100000: episode: 2395, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.122 [-0.417, 0.926], mean_best_reward: --\n",
      " 85647/100000: episode: 2396, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.081 [-0.900, 0.608], mean_best_reward: --\n",
      " 85677/100000: episode: 2397, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.107 [-0.847, 0.419], mean_best_reward: --\n",
      " 85711/100000: episode: 2398, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.082 [-1.296, 0.525], mean_best_reward: --\n",
      " 85749/100000: episode: 2399, duration: 0.183s, episode steps: 38, steps per second: 208, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.128 [-0.587, 0.989], mean_best_reward: --\n",
      " 85779/100000: episode: 2400, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.105 [-0.924, 0.410], mean_best_reward: --\n",
      " 85849/100000: episode: 2401, duration: 0.322s, episode steps: 70, steps per second: 217, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.380, 1.020], mean_best_reward: 130.500000\n",
      " 85878/100000: episode: 2402, duration: 0.138s, episode steps: 29, steps per second: 211, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.127 [-1.039, 0.545], mean_best_reward: --\n",
      " 85894/100000: episode: 2403, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.091 [-1.331, 2.166], mean_best_reward: --\n",
      " 85941/100000: episode: 2404, duration: 0.215s, episode steps: 47, steps per second: 219, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.141 [-0.488, 1.210], mean_best_reward: --\n",
      " 86052/100000: episode: 2405, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.153 [-1.300, 1.095], mean_best_reward: --\n",
      " 86078/100000: episode: 2406, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.060 [-1.392, 0.807], mean_best_reward: --\n",
      " 86107/100000: episode: 2407, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.106 [-1.231, 0.798], mean_best_reward: --\n",
      " 86157/100000: episode: 2408, duration: 0.224s, episode steps: 50, steps per second: 223, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.193 [-0.447, 1.065], mean_best_reward: --\n",
      " 86204/100000: episode: 2409, duration: 0.232s, episode steps: 47, steps per second: 202, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.070 [-0.752, 1.394], mean_best_reward: --\n",
      " 86251/100000: episode: 2410, duration: 0.231s, episode steps: 47, steps per second: 204, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.053 [-1.224, 0.806], mean_best_reward: --\n",
      " 86324/100000: episode: 2411, duration: 0.339s, episode steps: 73, steps per second: 215, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.008 [-1.413, 0.664], mean_best_reward: --\n",
      " 86362/100000: episode: 2412, duration: 0.176s, episode steps: 38, steps per second: 216, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.145 [-0.958, 0.670], mean_best_reward: --\n",
      " 86403/100000: episode: 2413, duration: 0.192s, episode steps: 41, steps per second: 214, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.072 [-0.461, 0.924], mean_best_reward: --\n",
      " 86480/100000: episode: 2414, duration: 0.349s, episode steps: 77, steps per second: 221, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.072 [-1.112, 0.940], mean_best_reward: --\n",
      " 86511/100000: episode: 2415, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.102 [-0.596, 1.017], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86531/100000: episode: 2416, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.085 [-1.382, 0.800], mean_best_reward: --\n",
      " 86566/100000: episode: 2417, duration: 0.161s, episode steps: 35, steps per second: 217, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.128 [-0.432, 0.904], mean_best_reward: --\n",
      " 86614/100000: episode: 2418, duration: 0.222s, episode steps: 48, steps per second: 216, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.091 [-0.905, 0.518], mean_best_reward: --\n",
      " 86649/100000: episode: 2419, duration: 0.160s, episode steps: 35, steps per second: 219, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.090 [-1.140, 0.394], mean_best_reward: --\n",
      " 86693/100000: episode: 2420, duration: 0.201s, episode steps: 44, steps per second: 219, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.129 [-0.548, 1.011], mean_best_reward: --\n",
      " 86723/100000: episode: 2421, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.100 [-0.452, 1.097], mean_best_reward: --\n",
      " 86768/100000: episode: 2422, duration: 0.205s, episode steps: 45, steps per second: 219, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.094 [-1.790, 0.782], mean_best_reward: --\n",
      " 86786/100000: episode: 2423, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.047 [-1.680, 1.179], mean_best_reward: --\n",
      " 86823/100000: episode: 2424, duration: 0.171s, episode steps: 37, steps per second: 217, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.126 [-1.163, 0.744], mean_best_reward: --\n",
      " 86883/100000: episode: 2425, duration: 0.270s, episode steps: 60, steps per second: 222, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.079 [-1.329, 0.738], mean_best_reward: --\n",
      " 86896/100000: episode: 2426, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.110 [-1.314, 0.743], mean_best_reward: --\n",
      " 86948/100000: episode: 2427, duration: 0.239s, episode steps: 52, steps per second: 218, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.098 [-0.621, 1.092], mean_best_reward: --\n",
      " 86990/100000: episode: 2428, duration: 0.191s, episode steps: 42, steps per second: 220, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.151 [-1.186, 0.334], mean_best_reward: --\n",
      " 87009/100000: episode: 2429, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.114 [-1.041, 0.573], mean_best_reward: --\n",
      " 87038/100000: episode: 2430, duration: 0.134s, episode steps: 29, steps per second: 217, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.062 [-1.999, 1.132], mean_best_reward: --\n",
      " 87071/100000: episode: 2431, duration: 0.155s, episode steps: 33, steps per second: 212, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.118 [-0.857, 1.423], mean_best_reward: --\n",
      " 87124/100000: episode: 2432, duration: 0.238s, episode steps: 53, steps per second: 223, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.054 [-0.447, 1.101], mean_best_reward: --\n",
      " 87160/100000: episode: 2433, duration: 0.170s, episode steps: 36, steps per second: 212, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.073 [-0.450, 0.945], mean_best_reward: --\n",
      " 87208/100000: episode: 2434, duration: 0.270s, episode steps: 48, steps per second: 178, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.001, 0.381], mean_best_reward: --\n",
      " 87253/100000: episode: 2435, duration: 0.290s, episode steps: 45, steps per second: 155, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.175 [-0.382, 0.934], mean_best_reward: --\n",
      " 87295/100000: episode: 2436, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.017 [-0.772, 1.124], mean_best_reward: --\n",
      " 87326/100000: episode: 2437, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.068 [-1.901, 0.970], mean_best_reward: --\n",
      " 87379/100000: episode: 2438, duration: 0.255s, episode steps: 53, steps per second: 208, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.076 [-0.880, 0.367], mean_best_reward: --\n",
      " 87444/100000: episode: 2439, duration: 0.339s, episode steps: 65, steps per second: 192, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.131 [-1.102, 0.763], mean_best_reward: --\n",
      " 87471/100000: episode: 2440, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.114 [-1.179, 0.401], mean_best_reward: --\n",
      " 87504/100000: episode: 2441, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.084 [-0.554, 1.097], mean_best_reward: --\n",
      " 87529/100000: episode: 2442, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.026 [-1.400, 1.019], mean_best_reward: --\n",
      " 87593/100000: episode: 2443, duration: 0.367s, episode steps: 64, steps per second: 174, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.086 [-2.013, 0.788], mean_best_reward: --\n",
      " 87620/100000: episode: 2444, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.036 [-0.639, 1.131], mean_best_reward: --\n",
      " 87695/100000: episode: 2445, duration: 0.389s, episode steps: 75, steps per second: 193, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.015 [-1.140, 1.343], mean_best_reward: --\n",
      " 87784/100000: episode: 2446, duration: 0.448s, episode steps: 89, steps per second: 199, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.149 [-0.773, 1.571], mean_best_reward: --\n",
      " 87825/100000: episode: 2447, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.084 [-0.545, 1.239], mean_best_reward: --\n",
      " 87851/100000: episode: 2448, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.083 [-0.571, 1.421], mean_best_reward: --\n",
      " 87899/100000: episode: 2449, duration: 0.227s, episode steps: 48, steps per second: 212, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.034 [-0.770, 1.045], mean_best_reward: --\n",
      " 87938/100000: episode: 2450, duration: 0.180s, episode steps: 39, steps per second: 217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.129 [-0.404, 0.846], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87973/100000: episode: 2451, duration: 0.164s, episode steps: 35, steps per second: 214, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.007 [-1.566, 1.198], mean_best_reward: 99.000000\n",
      " 88007/100000: episode: 2452, duration: 0.155s, episode steps: 34, steps per second: 220, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.135 [-0.385, 0.780], mean_best_reward: --\n",
      " 88064/100000: episode: 2453, duration: 0.291s, episode steps: 57, steps per second: 196, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.138 [-0.421, 0.902], mean_best_reward: --\n",
      " 88100/100000: episode: 2454, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.419, 1.044], mean_best_reward: --\n",
      " 88126/100000: episode: 2455, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.564, 1.113], mean_best_reward: --\n",
      " 88192/100000: episode: 2456, duration: 0.309s, episode steps: 66, steps per second: 214, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.145 [-1.077, 0.573], mean_best_reward: --\n",
      " 88234/100000: episode: 2457, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.136 [-1.081, 0.472], mean_best_reward: --\n",
      " 88262/100000: episode: 2458, duration: 0.132s, episode steps: 28, steps per second: 213, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.108 [-0.365, 1.011], mean_best_reward: --\n",
      " 88287/100000: episode: 2459, duration: 0.119s, episode steps: 25, steps per second: 211, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.100 [-0.955, 0.418], mean_best_reward: --\n",
      " 88362/100000: episode: 2460, duration: 0.345s, episode steps: 75, steps per second: 218, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.065 [-0.619, 0.860], mean_best_reward: --\n",
      " 88447/100000: episode: 2461, duration: 0.382s, episode steps: 85, steps per second: 223, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.101 [-1.355, 0.750], mean_best_reward: --\n",
      " 88484/100000: episode: 2462, duration: 0.171s, episode steps: 37, steps per second: 216, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.622 [0.000, 1.000], mean observation: -0.022 [-2.791, 1.924], mean_best_reward: --\n",
      " 88501/100000: episode: 2463, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.077 [-0.810, 1.196], mean_best_reward: --\n",
      " 88534/100000: episode: 2464, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.106 [-1.137, 0.808], mean_best_reward: --\n",
      " 88565/100000: episode: 2465, duration: 0.145s, episode steps: 31, steps per second: 213, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.111 [-0.388, 1.029], mean_best_reward: --\n",
      " 88590/100000: episode: 2466, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.090 [-0.791, 1.174], mean_best_reward: --\n",
      " 88708/100000: episode: 2467, duration: 0.526s, episode steps: 118, steps per second: 225, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.059 [-1.775, 1.097], mean_best_reward: --\n",
      " 88742/100000: episode: 2468, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.092 [-0.574, 1.141], mean_best_reward: --\n",
      " 88878/100000: episode: 2469, duration: 0.613s, episode steps: 136, steps per second: 222, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.210 [-1.254, 1.373], mean_best_reward: --\n",
      " 88903/100000: episode: 2470, duration: 0.116s, episode steps: 25, steps per second: 215, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.057 [-0.987, 1.639], mean_best_reward: --\n",
      " 88963/100000: episode: 2471, duration: 0.275s, episode steps: 60, steps per second: 219, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.096 [-0.707, 1.187], mean_best_reward: --\n",
      " 88977/100000: episode: 2472, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.125 [-1.632, 0.805], mean_best_reward: --\n",
      " 89041/100000: episode: 2473, duration: 0.285s, episode steps: 64, steps per second: 224, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.118 [-0.862, 0.816], mean_best_reward: --\n",
      " 89078/100000: episode: 2474, duration: 0.168s, episode steps: 37, steps per second: 221, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.064 [-1.622, 0.788], mean_best_reward: --\n",
      " 89108/100000: episode: 2475, duration: 0.140s, episode steps: 30, steps per second: 215, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.074 [-0.618, 1.484], mean_best_reward: --\n",
      " 89139/100000: episode: 2476, duration: 0.141s, episode steps: 31, steps per second: 220, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.103 [-1.586, 0.579], mean_best_reward: --\n",
      " 89188/100000: episode: 2477, duration: 0.226s, episode steps: 49, steps per second: 217, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.057 [-0.423, 1.163], mean_best_reward: --\n",
      " 89234/100000: episode: 2478, duration: 0.208s, episode steps: 46, steps per second: 221, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.148 [-0.890, 0.277], mean_best_reward: --\n",
      " 89316/100000: episode: 2479, duration: 0.365s, episode steps: 82, steps per second: 225, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.120 [-1.003, 1.103], mean_best_reward: --\n",
      " 89363/100000: episode: 2480, duration: 0.214s, episode steps: 47, steps per second: 220, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.048 [-0.872, 0.624], mean_best_reward: --\n",
      " 89383/100000: episode: 2481, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.042 [-2.393, 1.571], mean_best_reward: --\n",
      " 89413/100000: episode: 2482, duration: 0.143s, episode steps: 30, steps per second: 209, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.118 [-0.977, 0.425], mean_best_reward: --\n",
      " 89575/100000: episode: 2483, duration: 0.711s, episode steps: 162, steps per second: 228, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.341 [-1.006, 2.401], mean_best_reward: --\n",
      " 89648/100000: episode: 2484, duration: 0.330s, episode steps: 73, steps per second: 221, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.058 [-0.849, 0.948], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89694/100000: episode: 2485, duration: 0.205s, episode steps: 46, steps per second: 224, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.090 [-0.898, 0.611], mean_best_reward: --\n",
      " 89761/100000: episode: 2486, duration: 0.300s, episode steps: 67, steps per second: 223, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.080 [-0.594, 1.075], mean_best_reward: --\n",
      " 89777/100000: episode: 2487, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.107 [-1.982, 1.133], mean_best_reward: --\n",
      " 89894/100000: episode: 2488, duration: 0.526s, episode steps: 117, steps per second: 223, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.020 [-1.395, 0.862], mean_best_reward: --\n",
      " 89918/100000: episode: 2489, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.032, 0.595], mean_best_reward: --\n",
      " 89974/100000: episode: 2490, duration: 0.256s, episode steps: 56, steps per second: 219, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.087 [-0.715, 0.868], mean_best_reward: --\n",
      " 90007/100000: episode: 2491, duration: 0.151s, episode steps: 33, steps per second: 218, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.089 [-0.426, 0.889], mean_best_reward: --\n",
      " 90058/100000: episode: 2492, duration: 0.239s, episode steps: 51, steps per second: 213, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.046 [-1.501, 1.204], mean_best_reward: --\n",
      " 90087/100000: episode: 2493, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.083 [-1.500, 0.985], mean_best_reward: --\n",
      " 90143/100000: episode: 2494, duration: 0.256s, episode steps: 56, steps per second: 219, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.951, 0.588], mean_best_reward: --\n",
      " 90191/100000: episode: 2495, duration: 0.223s, episode steps: 48, steps per second: 215, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.083 [-0.859, 0.383], mean_best_reward: --\n",
      " 90239/100000: episode: 2496, duration: 0.221s, episode steps: 48, steps per second: 217, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.067 [-0.949, 0.608], mean_best_reward: --\n",
      " 90294/100000: episode: 2497, duration: 0.255s, episode steps: 55, steps per second: 216, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.089 [-0.784, 0.875], mean_best_reward: --\n",
      " 90314/100000: episode: 2498, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.072 [-0.809, 1.567], mean_best_reward: --\n",
      " 90359/100000: episode: 2499, duration: 0.215s, episode steps: 45, steps per second: 209, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.096 [-0.610, 0.868], mean_best_reward: --\n",
      " 90405/100000: episode: 2500, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.098 [-0.795, 0.304], mean_best_reward: --\n",
      " 90500/100000: episode: 2501, duration: 0.431s, episode steps: 95, steps per second: 220, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.177 [-1.326, 0.669], mean_best_reward: 136.500000\n",
      " 90542/100000: episode: 2502, duration: 0.189s, episode steps: 42, steps per second: 222, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.001 [-0.787, 1.083], mean_best_reward: --\n",
      " 90599/100000: episode: 2503, duration: 0.255s, episode steps: 57, steps per second: 224, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.141 [-1.324, 0.547], mean_best_reward: --\n",
      " 90623/100000: episode: 2504, duration: 0.110s, episode steps: 24, steps per second: 218, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.115 [-1.501, 0.582], mean_best_reward: --\n",
      " 90677/100000: episode: 2505, duration: 0.242s, episode steps: 54, steps per second: 223, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.101 [-1.212, 0.590], mean_best_reward: --\n",
      " 90702/100000: episode: 2506, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.083 [-1.596, 0.635], mean_best_reward: --\n",
      " 90722/100000: episode: 2507, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.088 [-1.304, 0.573], mean_best_reward: --\n",
      " 90758/100000: episode: 2508, duration: 0.163s, episode steps: 36, steps per second: 221, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.112 [-0.774, 0.359], mean_best_reward: --\n",
      " 90778/100000: episode: 2509, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-0.995, 0.637], mean_best_reward: --\n",
      " 90817/100000: episode: 2510, duration: 0.175s, episode steps: 39, steps per second: 223, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.065 [-0.927, 1.321], mean_best_reward: --\n",
      " 90851/100000: episode: 2511, duration: 0.156s, episode steps: 34, steps per second: 218, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.047 [-1.600, 0.768], mean_best_reward: --\n",
      " 90899/100000: episode: 2512, duration: 0.216s, episode steps: 48, steps per second: 222, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.058, 0.580], mean_best_reward: --\n",
      " 90925/100000: episode: 2513, duration: 0.125s, episode steps: 26, steps per second: 209, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.121 [-1.224, 0.764], mean_best_reward: --\n",
      " 90988/100000: episode: 2514, duration: 0.296s, episode steps: 63, steps per second: 213, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.081 [-1.025, 0.445], mean_best_reward: --\n",
      " 91024/100000: episode: 2515, duration: 0.162s, episode steps: 36, steps per second: 223, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-1.011, 0.638], mean_best_reward: --\n",
      " 91086/100000: episode: 2516, duration: 0.276s, episode steps: 62, steps per second: 225, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.155 [-0.888, 0.947], mean_best_reward: --\n",
      " 91151/100000: episode: 2517, duration: 0.295s, episode steps: 65, steps per second: 220, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.144 [-0.904, 0.602], mean_best_reward: --\n",
      " 91198/100000: episode: 2518, duration: 0.214s, episode steps: 47, steps per second: 220, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.122 [-1.578, 0.712], mean_best_reward: --\n",
      " 91214/100000: episode: 2519, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.092 [-1.582, 0.937], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91241/100000: episode: 2520, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.069 [-0.414, 0.948], mean_best_reward: --\n",
      " 91284/100000: episode: 2521, duration: 0.193s, episode steps: 43, steps per second: 223, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.052 [-0.986, 1.756], mean_best_reward: --\n",
      " 91323/100000: episode: 2522, duration: 0.177s, episode steps: 39, steps per second: 220, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.114 [-0.349, 1.058], mean_best_reward: --\n",
      " 91385/100000: episode: 2523, duration: 0.282s, episode steps: 62, steps per second: 220, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.126 [-1.104, 0.585], mean_best_reward: --\n",
      " 91399/100000: episode: 2524, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.098 [-0.796, 1.265], mean_best_reward: --\n",
      " 91484/100000: episode: 2525, duration: 0.373s, episode steps: 85, steps per second: 228, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.091 [-1.092, 0.962], mean_best_reward: --\n",
      " 91519/100000: episode: 2526, duration: 0.161s, episode steps: 35, steps per second: 217, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.115 [-0.866, 0.524], mean_best_reward: --\n",
      " 91532/100000: episode: 2527, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.093 [-1.987, 1.220], mean_best_reward: --\n",
      " 91570/100000: episode: 2528, duration: 0.177s, episode steps: 38, steps per second: 215, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.140 [-0.593, 0.957], mean_best_reward: --\n",
      " 91611/100000: episode: 2529, duration: 0.188s, episode steps: 41, steps per second: 218, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.052 [-0.994, 0.564], mean_best_reward: --\n",
      " 91649/100000: episode: 2530, duration: 0.174s, episode steps: 38, steps per second: 218, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.047 [-1.811, 1.035], mean_best_reward: --\n",
      " 91674/100000: episode: 2531, duration: 0.385s, episode steps: 25, steps per second: 65, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.082 [-1.462, 0.763], mean_best_reward: --\n",
      " 91830/100000: episode: 2532, duration: 1.860s, episode steps: 156, steps per second: 84, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.092 [-0.877, 1.127], mean_best_reward: --\n",
      " 91912/100000: episode: 2533, duration: 0.577s, episode steps: 82, steps per second: 142, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.154 [-0.956, 0.516], mean_best_reward: --\n",
      " 91929/100000: episode: 2534, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.089 [-0.426, 1.049], mean_best_reward: --\n",
      " 91967/100000: episode: 2535, duration: 0.274s, episode steps: 38, steps per second: 139, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.162 [-0.937, 0.382], mean_best_reward: --\n",
      " 92015/100000: episode: 2536, duration: 0.309s, episode steps: 48, steps per second: 155, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.135 [-0.753, 0.380], mean_best_reward: --\n",
      " 92071/100000: episode: 2537, duration: 0.364s, episode steps: 56, steps per second: 154, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.214 [-1.293, 0.775], mean_best_reward: --\n",
      " 92204/100000: episode: 2538, duration: 0.855s, episode steps: 133, steps per second: 156, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.043 [-3.226, 3.661], mean_best_reward: --\n",
      " 92221/100000: episode: 2539, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.094 [-1.073, 0.612], mean_best_reward: --\n",
      " 92282/100000: episode: 2540, duration: 0.317s, episode steps: 61, steps per second: 193, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.008 [-1.152, 1.331], mean_best_reward: --\n",
      " 92359/100000: episode: 2541, duration: 0.463s, episode steps: 77, steps per second: 166, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.164 [-0.898, 1.486], mean_best_reward: --\n",
      " 92419/100000: episode: 2542, duration: 0.347s, episode steps: 60, steps per second: 173, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.058 [-1.181, 1.147], mean_best_reward: --\n",
      " 92434/100000: episode: 2543, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.089 [-1.232, 0.783], mean_best_reward: --\n",
      " 92476/100000: episode: 2544, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-1.386, 0.581], mean_best_reward: --\n",
      " 92526/100000: episode: 2545, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.049 [-0.608, 1.394], mean_best_reward: --\n",
      " 92565/100000: episode: 2546, duration: 0.186s, episode steps: 39, steps per second: 210, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.092 [-1.042, 0.545], mean_best_reward: --\n",
      " 92604/100000: episode: 2547, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.131 [-0.576, 1.243], mean_best_reward: --\n",
      " 92645/100000: episode: 2548, duration: 0.186s, episode steps: 41, steps per second: 220, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.061 [-0.781, 1.362], mean_best_reward: --\n",
      " 92694/100000: episode: 2549, duration: 0.242s, episode steps: 49, steps per second: 203, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.078 [-0.611, 0.903], mean_best_reward: --\n",
      " 92722/100000: episode: 2550, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.792, 1.259], mean_best_reward: --\n",
      " 92755/100000: episode: 2551, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.107 [-0.442, 0.917], mean_best_reward: 101.500000\n",
      " 92805/100000: episode: 2552, duration: 0.253s, episode steps: 50, steps per second: 197, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.999, 0.560], mean_best_reward: --\n",
      " 92842/100000: episode: 2553, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.043 [-1.000, 1.757], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92867/100000: episode: 2554, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.066 [-1.030, 1.903], mean_best_reward: --\n",
      " 92915/100000: episode: 2555, duration: 0.282s, episode steps: 48, steps per second: 170, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.416, 1.221], mean_best_reward: --\n",
      " 92964/100000: episode: 2556, duration: 0.298s, episode steps: 49, steps per second: 165, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.408 [0.000, 1.000], mean observation: 0.026 [-1.905, 2.724], mean_best_reward: --\n",
      " 93000/100000: episode: 2557, duration: 0.166s, episode steps: 36, steps per second: 217, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.097 [-0.318, 1.152], mean_best_reward: --\n",
      " 93050/100000: episode: 2558, duration: 0.227s, episode steps: 50, steps per second: 220, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-1.269, 0.429], mean_best_reward: --\n",
      " 93087/100000: episode: 2559, duration: 0.170s, episode steps: 37, steps per second: 218, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: 0.061 [-1.363, 2.303], mean_best_reward: --\n",
      " 93141/100000: episode: 2560, duration: 0.248s, episode steps: 54, steps per second: 218, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.118 [-0.949, 0.415], mean_best_reward: --\n",
      " 93167/100000: episode: 2561, duration: 0.120s, episode steps: 26, steps per second: 216, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.141 [-0.540, 1.136], mean_best_reward: --\n",
      " 93189/100000: episode: 2562, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.611, 1.091], mean_best_reward: --\n",
      " 93245/100000: episode: 2563, duration: 0.250s, episode steps: 56, steps per second: 224, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.143 [-0.523, 1.257], mean_best_reward: --\n",
      " 93262/100000: episode: 2564, duration: 0.080s, episode steps: 17, steps per second: 211, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.094 [-1.365, 0.621], mean_best_reward: --\n",
      " 93279/100000: episode: 2565, duration: 0.080s, episode steps: 17, steps per second: 212, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.113 [-0.800, 1.237], mean_best_reward: --\n",
      " 93306/100000: episode: 2566, duration: 0.124s, episode steps: 27, steps per second: 218, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.105 [-0.572, 0.923], mean_best_reward: --\n",
      " 93368/100000: episode: 2567, duration: 0.280s, episode steps: 62, steps per second: 221, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.117 [-0.767, 1.156], mean_best_reward: --\n",
      " 93410/100000: episode: 2568, duration: 0.189s, episode steps: 42, steps per second: 223, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.049 [-0.600, 1.387], mean_best_reward: --\n",
      " 93432/100000: episode: 2569, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.086 [-1.740, 0.850], mean_best_reward: --\n",
      " 93458/100000: episode: 2570, duration: 0.119s, episode steps: 26, steps per second: 218, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.114 [-0.636, 1.031], mean_best_reward: --\n",
      " 93473/100000: episode: 2571, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.950, 1.801], mean_best_reward: --\n",
      " 93506/100000: episode: 2572, duration: 0.151s, episode steps: 33, steps per second: 219, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.098 [-1.133, 0.574], mean_best_reward: --\n",
      " 93543/100000: episode: 2573, duration: 0.167s, episode steps: 37, steps per second: 222, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.094 [-0.506, 0.774], mean_best_reward: --\n",
      " 93614/100000: episode: 2574, duration: 0.320s, episode steps: 71, steps per second: 222, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.051 [-0.971, 1.233], mean_best_reward: --\n",
      " 93687/100000: episode: 2575, duration: 0.324s, episode steps: 73, steps per second: 225, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.095 [-1.367, 1.310], mean_best_reward: --\n",
      " 93707/100000: episode: 2576, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.085 [-1.083, 0.545], mean_best_reward: --\n",
      " 93768/100000: episode: 2577, duration: 0.279s, episode steps: 61, steps per second: 219, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.164 [-1.548, 1.510], mean_best_reward: --\n",
      " 93821/100000: episode: 2578, duration: 0.296s, episode steps: 53, steps per second: 179, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.103 [-0.584, 1.446], mean_best_reward: --\n",
      " 93912/100000: episode: 2579, duration: 0.468s, episode steps: 91, steps per second: 194, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.057 [-1.102, 1.229], mean_best_reward: --\n",
      " 93964/100000: episode: 2580, duration: 0.280s, episode steps: 52, steps per second: 186, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.177 [-0.222, 1.113], mean_best_reward: --\n",
      " 94018/100000: episode: 2581, duration: 0.276s, episode steps: 54, steps per second: 196, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.566, 0.642], mean_best_reward: --\n",
      " 94060/100000: episode: 2582, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.118 [-0.983, 0.543], mean_best_reward: --\n",
      " 94183/100000: episode: 2583, duration: 0.643s, episode steps: 123, steps per second: 191, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.056 [-0.967, 0.693], mean_best_reward: --\n",
      " 94216/100000: episode: 2584, duration: 0.155s, episode steps: 33, steps per second: 213, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.101 [-0.593, 0.936], mean_best_reward: --\n",
      " 94289/100000: episode: 2585, duration: 0.380s, episode steps: 73, steps per second: 192, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.095 [-2.455, 2.627], mean_best_reward: --\n",
      " 94345/100000: episode: 2586, duration: 0.299s, episode steps: 56, steps per second: 187, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.156 [-0.362, 0.969], mean_best_reward: --\n",
      " 94358/100000: episode: 2587, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.126 [-1.321, 0.769], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94494/100000: episode: 2588, duration: 0.681s, episode steps: 136, steps per second: 200, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.190 [-1.475, 1.095], mean_best_reward: --\n",
      " 94530/100000: episode: 2589, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.602, 0.962], mean_best_reward: --\n",
      " 94554/100000: episode: 2590, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.117 [-0.372, 0.865], mean_best_reward: --\n",
      " 94594/100000: episode: 2591, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: -0.040 [-2.217, 1.196], mean_best_reward: --\n",
      " 94629/100000: episode: 2592, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.106 [-0.191, 0.844], mean_best_reward: --\n",
      " 94659/100000: episode: 2593, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.050 [-1.716, 1.015], mean_best_reward: --\n",
      " 94695/100000: episode: 2594, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.845, 1.204], mean_best_reward: --\n",
      " 94773/100000: episode: 2595, duration: 0.379s, episode steps: 78, steps per second: 206, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.030 [-1.013, 1.357], mean_best_reward: --\n",
      " 94828/100000: episode: 2596, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.007 [-0.988, 1.760], mean_best_reward: --\n",
      " 94912/100000: episode: 2597, duration: 0.426s, episode steps: 84, steps per second: 197, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.117 [-0.751, 0.759], mean_best_reward: --\n",
      " 94960/100000: episode: 2598, duration: 0.221s, episode steps: 48, steps per second: 217, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.132 [-0.560, 0.893], mean_best_reward: --\n",
      " 95002/100000: episode: 2599, duration: 0.187s, episode steps: 42, steps per second: 225, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.833, 1.331], mean_best_reward: --\n",
      " 95039/100000: episode: 2600, duration: 0.168s, episode steps: 37, steps per second: 220, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.060 [-1.174, 0.837], mean_best_reward: --\n",
      " 95066/100000: episode: 2601, duration: 0.123s, episode steps: 27, steps per second: 220, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.080 [-0.972, 1.938], mean_best_reward: 161.000000\n",
      " 95122/100000: episode: 2602, duration: 0.257s, episode steps: 56, steps per second: 218, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.132 [-0.651, 0.910], mean_best_reward: --\n",
      " 95275/100000: episode: 2603, duration: 0.671s, episode steps: 153, steps per second: 228, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.403 [-2.410, 1.087], mean_best_reward: --\n",
      " 95352/100000: episode: 2604, duration: 0.355s, episode steps: 77, steps per second: 217, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.100 [-0.521, 0.920], mean_best_reward: --\n",
      " 95400/100000: episode: 2605, duration: 0.221s, episode steps: 48, steps per second: 218, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.369, 0.403], mean_best_reward: --\n",
      " 95447/100000: episode: 2606, duration: 0.211s, episode steps: 47, steps per second: 223, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.096 [-0.730, 1.074], mean_best_reward: --\n",
      " 95492/100000: episode: 2607, duration: 0.200s, episode steps: 45, steps per second: 225, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.120 [-0.532, 0.821], mean_best_reward: --\n",
      " 95554/100000: episode: 2608, duration: 0.280s, episode steps: 62, steps per second: 221, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.073 [-1.073, 0.585], mean_best_reward: --\n",
      " 95593/100000: episode: 2609, duration: 0.175s, episode steps: 39, steps per second: 223, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.004 [-1.280, 1.001], mean_best_reward: --\n",
      " 95626/100000: episode: 2610, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.043 [-1.143, 0.624], mean_best_reward: --\n",
      " 95666/100000: episode: 2611, duration: 0.179s, episode steps: 40, steps per second: 223, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.355, 0.680], mean_best_reward: --\n",
      " 95696/100000: episode: 2612, duration: 0.138s, episode steps: 30, steps per second: 217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.125 [-0.568, 1.013], mean_best_reward: --\n",
      " 95871/100000: episode: 2613, duration: 0.776s, episode steps: 175, steps per second: 226, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.363 [-2.402, 1.534], mean_best_reward: --\n",
      " 95927/100000: episode: 2614, duration: 0.254s, episode steps: 56, steps per second: 221, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.037 [-0.656, 1.658], mean_best_reward: --\n",
      " 95977/100000: episode: 2615, duration: 0.227s, episode steps: 50, steps per second: 220, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.083 [-0.564, 1.564], mean_best_reward: --\n",
      " 96020/100000: episode: 2616, duration: 0.194s, episode steps: 43, steps per second: 222, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.099 [-0.456, 1.081], mean_best_reward: --\n",
      " 96042/100000: episode: 2617, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.124, 0.451], mean_best_reward: --\n",
      " 96143/100000: episode: 2618, duration: 0.463s, episode steps: 101, steps per second: 218, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.057 [-0.621, 1.035], mean_best_reward: --\n",
      " 96179/100000: episode: 2619, duration: 0.163s, episode steps: 36, steps per second: 221, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.522, 0.626], mean_best_reward: --\n",
      " 96233/100000: episode: 2620, duration: 0.263s, episode steps: 54, steps per second: 205, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.076 [-1.296, 0.420], mean_best_reward: --\n",
      " 96262/100000: episode: 2621, duration: 0.133s, episode steps: 29, steps per second: 218, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.134 [-0.771, 1.209], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96290/100000: episode: 2622, duration: 0.136s, episode steps: 28, steps per second: 205, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-1.178, 0.831], mean_best_reward: --\n",
      " 96331/100000: episode: 2623, duration: 0.187s, episode steps: 41, steps per second: 220, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.167 [-0.937, 0.554], mean_best_reward: --\n",
      " 96416/100000: episode: 2624, duration: 0.378s, episode steps: 85, steps per second: 225, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.097 [-0.930, 1.520], mean_best_reward: --\n",
      " 96445/100000: episode: 2625, duration: 0.132s, episode steps: 29, steps per second: 220, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.071 [-0.934, 1.612], mean_best_reward: --\n",
      " 96458/100000: episode: 2626, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-0.961, 1.628], mean_best_reward: --\n",
      " 96513/100000: episode: 2627, duration: 0.251s, episode steps: 55, steps per second: 219, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.019 [-1.321, 0.942], mean_best_reward: --\n",
      " 96599/100000: episode: 2628, duration: 0.376s, episode steps: 86, steps per second: 228, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.143 [-0.992, 1.855], mean_best_reward: --\n",
      " 96681/100000: episode: 2629, duration: 0.366s, episode steps: 82, steps per second: 224, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.098 [-0.814, 1.126], mean_best_reward: --\n",
      " 96724/100000: episode: 2630, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.130 [-0.309, 0.775], mean_best_reward: --\n",
      " 96786/100000: episode: 2631, duration: 0.289s, episode steps: 62, steps per second: 215, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.063 [-1.097, 0.672], mean_best_reward: --\n",
      " 96816/100000: episode: 2632, duration: 0.137s, episode steps: 30, steps per second: 219, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.007, 0.397], mean_best_reward: --\n",
      " 96834/100000: episode: 2633, duration: 0.086s, episode steps: 18, steps per second: 211, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.115 [-0.811, 1.272], mean_best_reward: --\n",
      " 96926/100000: episode: 2634, duration: 0.403s, episode steps: 92, steps per second: 228, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.002 [-1.204, 1.114], mean_best_reward: --\n",
      " 96997/100000: episode: 2635, duration: 0.319s, episode steps: 71, steps per second: 223, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.060 [-0.633, 0.781], mean_best_reward: --\n",
      " 97038/100000: episode: 2636, duration: 0.183s, episode steps: 41, steps per second: 224, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.111 [-1.529, 0.561], mean_best_reward: --\n",
      " 97106/100000: episode: 2637, duration: 0.305s, episode steps: 68, steps per second: 223, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.035 [-1.007, 0.621], mean_best_reward: --\n",
      " 97120/100000: episode: 2638, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.116, 0.629], mean_best_reward: --\n",
      " 97166/100000: episode: 2639, duration: 0.217s, episode steps: 46, steps per second: 212, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.201 [-0.827, 1.512], mean_best_reward: --\n",
      " 97208/100000: episode: 2640, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.094 [-1.360, 1.270], mean_best_reward: --\n",
      " 97244/100000: episode: 2641, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.152 [-0.577, 0.988], mean_best_reward: --\n",
      " 97359/100000: episode: 2642, duration: 0.627s, episode steps: 115, steps per second: 183, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.144 [-0.795, 1.418], mean_best_reward: --\n",
      " 97412/100000: episode: 2643, duration: 0.267s, episode steps: 53, steps per second: 198, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.098 [-0.582, 1.457], mean_best_reward: --\n",
      " 97490/100000: episode: 2644, duration: 0.363s, episode steps: 78, steps per second: 215, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.660, 1.633], mean_best_reward: --\n",
      " 97525/100000: episode: 2645, duration: 0.161s, episode steps: 35, steps per second: 218, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.149 [-1.086, 0.440], mean_best_reward: --\n",
      " 97575/100000: episode: 2646, duration: 0.235s, episode steps: 50, steps per second: 213, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.172 [-1.095, 0.589], mean_best_reward: --\n",
      " 97643/100000: episode: 2647, duration: 0.319s, episode steps: 68, steps per second: 213, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.051 [-0.615, 1.202], mean_best_reward: --\n",
      " 97768/100000: episode: 2648, duration: 0.566s, episode steps: 125, steps per second: 221, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.186 [-1.735, 0.788], mean_best_reward: --\n",
      " 97813/100000: episode: 2649, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.024 [-0.870, 1.056], mean_best_reward: --\n",
      " 97910/100000: episode: 2650, duration: 0.467s, episode steps: 97, steps per second: 208, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.434 [-3.198, 1.936], mean_best_reward: --\n",
      " 97928/100000: episode: 2651, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.079 [-1.167, 0.782], mean_best_reward: 116.000000\n",
      " 97997/100000: episode: 2652, duration: 0.345s, episode steps: 69, steps per second: 200, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.113 [-1.462, 0.600], mean_best_reward: --\n",
      " 98028/100000: episode: 2653, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.094 [-1.250, 0.455], mean_best_reward: --\n",
      " 98041/100000: episode: 2654, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.085 [-0.795, 1.334], mean_best_reward: --\n",
      " 98084/100000: episode: 2655, duration: 0.219s, episode steps: 43, steps per second: 196, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.106 [-0.740, 1.159], mean_best_reward: --\n",
      " 98115/100000: episode: 2656, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.049 [-1.083, 0.764], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98163/100000: episode: 2657, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.130, 0.708], mean_best_reward: --\n",
      " 98193/100000: episode: 2658, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.101 [-1.105, 0.622], mean_best_reward: --\n",
      " 98251/100000: episode: 2659, duration: 0.300s, episode steps: 58, steps per second: 194, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.133 [-0.974, 0.537], mean_best_reward: --\n",
      " 98322/100000: episode: 2660, duration: 0.341s, episode steps: 71, steps per second: 208, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.158 [-0.955, 0.642], mean_best_reward: --\n",
      " 98429/100000: episode: 2661, duration: 0.533s, episode steps: 107, steps per second: 201, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.114 [-0.955, 0.781], mean_best_reward: --\n",
      " 98479/100000: episode: 2662, duration: 0.233s, episode steps: 50, steps per second: 214, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.101 [-1.310, 0.452], mean_best_reward: --\n",
      " 98539/100000: episode: 2663, duration: 0.279s, episode steps: 60, steps per second: 215, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.153 [-1.235, 0.625], mean_best_reward: --\n",
      " 98593/100000: episode: 2664, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.539, 1.335], mean_best_reward: --\n",
      " 98699/100000: episode: 2665, duration: 0.500s, episode steps: 106, steps per second: 212, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.069 [-0.833, 1.022], mean_best_reward: --\n",
      " 98790/100000: episode: 2666, duration: 0.431s, episode steps: 91, steps per second: 211, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.091 [-0.773, 0.806], mean_best_reward: --\n",
      " 98882/100000: episode: 2667, duration: 0.440s, episode steps: 92, steps per second: 209, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.141 [-1.459, 0.847], mean_best_reward: --\n",
      " 98925/100000: episode: 2668, duration: 0.205s, episode steps: 43, steps per second: 210, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.119 [-0.523, 0.969], mean_best_reward: --\n",
      " 99016/100000: episode: 2669, duration: 0.430s, episode steps: 91, steps per second: 211, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.058 [-1.776, 1.057], mean_best_reward: --\n",
      " 99047/100000: episode: 2670, duration: 0.142s, episode steps: 31, steps per second: 218, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.102 [-0.924, 0.400], mean_best_reward: --\n",
      " 99110/100000: episode: 2671, duration: 0.292s, episode steps: 63, steps per second: 216, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.013 [-0.629, 1.346], mean_best_reward: --\n",
      " 99169/100000: episode: 2672, duration: 0.269s, episode steps: 59, steps per second: 219, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.080 [-1.226, 0.668], mean_best_reward: --\n",
      " 99183/100000: episode: 2673, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.088 [-2.159, 1.379], mean_best_reward: --\n",
      " 99258/100000: episode: 2674, duration: 0.356s, episode steps: 75, steps per second: 210, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.060 [-0.613, 1.041], mean_best_reward: --\n",
      " 99278/100000: episode: 2675, duration: 0.129s, episode steps: 20, steps per second: 155, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.057 [-1.542, 0.971], mean_best_reward: --\n",
      " 99293/100000: episode: 2676, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.111 [-0.596, 1.262], mean_best_reward: --\n",
      " 99323/100000: episode: 2677, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.136 [-0.387, 1.615], mean_best_reward: --\n",
      " 99345/100000: episode: 2678, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.092 [-0.746, 1.545], mean_best_reward: --\n",
      " 99403/100000: episode: 2679, duration: 0.281s, episode steps: 58, steps per second: 207, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.152 [-0.833, 1.482], mean_best_reward: --\n",
      " 99415/100000: episode: 2680, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.773, 1.460], mean_best_reward: --\n",
      " 99447/100000: episode: 2681, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.863, 1.680], mean_best_reward: --\n",
      " 99500/100000: episode: 2682, duration: 0.246s, episode steps: 53, steps per second: 216, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.103 [-0.496, 1.545], mean_best_reward: --\n",
      " 99538/100000: episode: 2683, duration: 0.175s, episode steps: 38, steps per second: 217, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.095 [-1.239, 0.714], mean_best_reward: --\n",
      " 99597/100000: episode: 2684, duration: 0.266s, episode steps: 59, steps per second: 222, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.121 [-0.817, 1.358], mean_best_reward: --\n",
      " 99643/100000: episode: 2685, duration: 0.232s, episode steps: 46, steps per second: 199, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.132 [-0.708, 1.170], mean_best_reward: --\n",
      " 99655/100000: episode: 2686, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-1.773, 0.956], mean_best_reward: --\n",
      " 99717/100000: episode: 2687, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.081 [-0.993, 0.568], mean_best_reward: --\n",
      " 99758/100000: episode: 2688, duration: 0.259s, episode steps: 41, steps per second: 158, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.085 [-0.470, 1.293], mean_best_reward: --\n",
      " 99775/100000: episode: 2689, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.094 [-0.603, 1.097], mean_best_reward: --\n",
      " 99786/100000: episode: 2690, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.132 [-1.991, 1.133], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99807/100000: episode: 2691, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.098 [-1.001, 1.888], mean_best_reward: --\n",
      " 99860/100000: episode: 2692, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.107 [-0.944, 0.544], mean_best_reward: --\n",
      " 99931/100000: episode: 2693, duration: 0.339s, episode steps: 71, steps per second: 209, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.146 [-1.229, 0.689], mean_best_reward: --\n",
      " 99988/100000: episode: 2694, duration: 0.289s, episode steps: 57, steps per second: 197, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.145 [-0.720, 1.394], mean_best_reward: --\n",
      "done, took 500.790 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb3f4839550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 8.000, steps: 8\n",
      "Episode 2: reward: 11.000, steps: 11\n",
      "Episode 3: reward: 9.000, steps: 9\n",
      "Episode 4: reward: 9.000, steps: 9\n",
      "Episode 5: reward: 11.000, steps: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee8392e460>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
